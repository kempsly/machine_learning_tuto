{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["6dy40bsuE60H"],"authorship_tag":"ABX9TyMY/+tpa7s3em/IkrvifkTm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Import statements"],"metadata":{"id":"rx3P_sPC7gmA"}},{"cell_type":"code","source":["!pip install --upgrade tensorflow"],"metadata":{"id":"xi5pkOuB8G8g"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5QT_T_z7S9f"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","from sklearn.metrics import confusion_matrix, roc_curve\n","import seaborn as sns\n","import datetime\n","import pathlib\n","import io\n","import os\n","import re\n","import string\n","import time\n","from numpy import random\n","import tensorflow_datasets as tfds\n","import tensorflow_probability as tfp\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Conv1D,LayerNormalization,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input,MultiHeadAttention,Embedding,TextVectorization)\n","from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n","from google.colab import drive\n","from google.colab import files\n","from tensorboard.plugins import projector"]},{"cell_type":"markdown","source":["#Dataset Preparation"],"metadata":{"id":"R1Brz5b58R8l"}},{"cell_type":"markdown","source":["##Download the data"],"metadata":{"id":"jUUVXbCP8XM0"}},{"cell_type":"code","source":["!wget https://www.manythings.org/anki/fra-eng.zip"],"metadata":{"id":"TfBx3fXj8G_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip \"/content/fra-eng.zip\" -d \"/content/dataset/\""],"metadata":{"id":"cPC-CbrS8HCF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Preprocessing of the data"],"metadata":{"id":"Oz1EGzaG8cu7"}},{"cell_type":"code","source":["VOCAB_SIZE=20000\n","ENGLISH_SEQUENCE_LENGTH=32\n","FRENCH_SEQUENCE_LENGTH=32\n","EMBEDDING_DIM=256\n","BATCH_SIZE=128"],"metadata":{"id":"DYVoiPrx8HEx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["english_vectorize_layer=TextVectorization(\n","    standardize='lower_and_strip_punctuation',\n","    max_tokens=VOCAB_SIZE,\n","    output_mode='int',\n","    output_sequence_length=ENGLISH_SEQUENCE_LENGTH\n",")"],"metadata":{"id":"lhnLQ3nr8HHX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["french_vectorize_layer=TextVectorization(\n","    standardize='lower_and_strip_punctuation',\n","    max_tokens=VOCAB_SIZE,\n","    output_mode='int',\n","    output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",")"],"metadata":{"id":"sEP_Zbml8HKb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def selector(input_text):\n","  split_text=tf.strings.split(input_text,'\\t')\n","  return {'input_1':split_text[0:1],'input_2':'starttoken '+split_text[1:2]},split_text[1:2]+' endtoken'"],"metadata":{"id":"RNHuCVys8HNU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["split_dataset=text_dataset.map(selector)"],"metadata":{"id":"Z57gQPDH8HQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def separator(input_text):\n","  split_text=tf.strings.split(input_text,'\\t')\n","  return split_text[0:1],'starttoken '+split_text[1:2]+' endtoken'"],"metadata":{"id":"4EExgM_08HS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_dataset=text_dataset.map(separator)"],"metadata":{"id":"G-WkIABD8HVl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in split_dataset.take(3):\n","  print(i)"],"metadata":{"id":"oTRAqNs_8HYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["english_training_data=init_dataset.map(lambda x,y:x)### input x,y and output x\n","english_vectorize_layer.adapt(english_training_data)#### adapt the vectorize_layer to the training data"],"metadata":{"id":"DLguMIPY8HbS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["french_training_data=init_dataset.map(lambda x,y:y)### input x,y and output y\n","french_vectorize_layer.adapt(french_training_data)#### adapt the vectorize_layer to the training data"],"metadata":{"id":"WfUvBEIQ8HeK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def vectorizer(inputs,output):\n","  return {'input_1':english_vectorize_layer(inputs['input_1']),\n","          'input_2':french_vectorize_layer(inputs['input_2'])},french_vectorize_layer(output)"],"metadata":{"id":"XmHsd01J8Hg5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["split_dataset"],"metadata":{"id":"tqBTuH4f8Hj4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset=split_dataset.map(vectorizer)"],"metadata":{"id":"Jlq7pvyk8Hmu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in split_dataset.take(3):\n","  print(i)"],"metadata":{"id":"ihX56r7H8Hpk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in dataset.take(1):\n","  print(i)"],"metadata":{"id":"yYSVHMlS8Hrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"id":"gNZW1qJM8HvI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset=dataset.shuffle(2048).unbatch().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"],"metadata":{"id":"reTAsXem7fpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"id":"LbK9x55P7fsN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_BATCHES=int(200000/BATCH_SIZE)"],"metadata":{"id":"OIXfVYT97fvh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset=dataset.take(int(0.9*NUM_BATCHES))\n","val_dataset=dataset.skip(int(0.9*NUM_BATCHES))"],"metadata":{"id":"ABK-IZat7fzH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset"],"metadata":{"id":"NAzCF5g_9fWI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Modeling"],"metadata":{"id":"K9S3i-GL9hgh"}},{"cell_type":"markdown","source":["##Embeding"],"metadata":{"id":"MMoXzOqU9kfW"}},{"cell_type":"code","source":["def positional_encoding(model_size,SEQUENCE_LENGTH):\n","  output=[]\n","  for pos in range(SEQUENCE_LENGTH):\n","    PE=np.zeros((model_size))\n","    for i in range(model_size):\n","      if i%2==0:\n","        PE[i]=np.sin(pos/(10000**(i/model_size)))\n","      else:\n","        PE[i]=np.cos(pos/(10000**((i-1)/model_size)))\n","    output.append(tf.expand_dims(PE,axis=0))\n","  out=tf.concat(output,axis=0)\n","  out=tf.expand_dims(out,axis=0)\n","  return tf.cast(out,dtype=tf.float32)"],"metadata":{"id":"WQHfS-yy9fY0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's explain the purpose and implementation of the positional encoding function step by step.\n","\n","**Purpose of Positional Encoding:**\n","\n","In sequence processing tasks like natural language processing (NLP) or machine translation, Transformer-based models don't have inherent notions of word order because they process tokens in parallel. To provide positional information to the model, positional encoding is added to the input embeddings. This helps the model differentiate between tokens based on their position in the sequence.\n","\n","**Implementation of Positional Encoding:**\n","\n","1. **Initialization and Setup:**\n","   ```python\n","   import tensorflow as tf\n","   import numpy as np\n","   ```\n","\n","2. **Function Definition:**\n","   ```python\n","   def positional_encoding(model_size, sequence_length):\n","       position = np.arange(sequence_length)\n","       model_dims = np.arange(model_size)\n","       position_enc = np.zeros((sequence_length, model_size))\n","   ```\n","\n","   Here:\n","   - `position`: Array of integers from `0` to `sequence_length - 1`, representing the positions in the sequence.\n","   - `model_dims`: Array of integers from `0` to `model_size - 1`, representing the dimensions of the positional encoding.\n","   - `position_enc`: Initialized as a matrix of zeros with shape `(sequence_length, model_size)`.\n","\n","3. **Calculating Positional Encodings:**\n","   ```python\n","       for pos in position:\n","           for i in model_dims:\n","               if i % 2 == 0:\n","                   position_enc[pos, i] = np.sin(pos / (10000 ** (i / model_size)))\n","               else:\n","                   position_enc[pos, i] = np.cos(pos / (10000 ** ((i - 1) / model_size))))\n","   ```\n","\n","   This nested loop computes the sine and cosine values for each position (`pos`) and dimension (`i`) in the positional encoding matrix using the formula:\n","   - For even indices (`i % 2 == 0`): \\( \\text{positional\\_enc}[pos, i] = \\sin(\\text{pos} / 10000^{(i / \\text{model\\_size})}) \\)\n","   - For odd indices (`i % 2 != 0`): \\( \\text{positional\\_enc}[pos, i] = \\cos(\\text{pos} / 10000^{((i - 1) / \\text{model\\_size})}) \\)\n","\n","   The use of sine and cosine functions with varying frequencies ensures that each dimension of the positional encoding captures a unique position-related pattern.\n","\n","4. **Tensor Conversion and Reshaping:**\n","   ```python\n","       position_enc = tf.convert_to_tensor(position_enc, dtype=tf.float32)\n","       position_enc = tf.expand_dims(position_enc, axis=0)\n","       return position_enc\n","   ```\n","\n","   - `tf.convert_to_tensor`: Converts the positional encoding matrix (`position_enc`) from NumPy array to a TensorFlow tensor.\n","   - `tf.expand_dims`: Adds a batch dimension (`axis=0`) to the positional encoding tensor. This is typically required to align the shape with the input data expected by Transformer-based models.\n","\n","**Usage:**\n","\n","Once the positional encoding function is defined, you can use it to generate positional encodings for sequences of a specific length (`sequence_length`) and model dimension (`model_size`). The resulting positional encodings can then be added to input embeddings before feeding them into the Transformer model.\n","\n","For example:\n","```python\n","model_size = 128\n","sequence_length = 10\n","\n","pos_encodings = positional_encoding(model_size, sequence_length)\n","print(pos_encodings.shape)  # Output: (1, sequence_length, model_size)\n","```\n","\n","This `pos_encodings` tensor can be concatenated or added to input embeddings in your Transformer-based model to incorporate positional information into the model's input representation, enabling it to learn dependencies based on the order of tokens in the sequence."],"metadata":{"id":"bp8VGigiGZVY"}},{"cell_type":"code","source":["print(positional_encoding(256,64).shape)"],"metadata":{"id":"4Bb5Glv29fbq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The shape of the positional encodings generated by `positional_encoding(256, 64)` would be `(1, 64, 256)`.\n","\n","Here's why:\n","\n","- The `positional_encoding` function generates positional encodings for a sequence length of `64` with each encoding having a dimensionality of `256`.\n","- Inside the function, the encodings are concatenated along the batch dimension and then expanded to add a batch dimension of size `1`.\n","- Therefore, the resulting shape of the tensor is `(1, 64, 256)`:\n","\n","  - `1` corresponds to the batch dimension.\n","  - `64` corresponds to the sequence length.\n","  - `256` corresponds to the dimensionality of each positional encoding vector.\n","\n","This shape represents a batch of positional encodings for a sequence of length `64`, each encoded with a vector of size `256`. Each position in the sequence has a unique positional encoding represented by the `256`-dimensional vector."],"metadata":{"id":"4Th4JcayHZyF"}},{"cell_type":"code","source":["class Embeddings(Layer):\n","  def __init__(self, sequence_length, vocab_size, embed_dim,):\n","    super(Embeddings, self).__init__()\n","    self.token_embeddings=Embedding(\n","        input_dim=vocab_size, output_dim=embed_dim)\n","    self.sequence_length = sequence_length\n","    self.vocab_size = vocab_size\n","    self.embed_dim = embed_dim\n","\n","  def call(self, inputs):\n","    embedded_tokens = self.token_embeddings(inputs)\n","    embedded_positions=positional_encoding(\n","        self.embed_dim,self.sequence_length)\n","    return embedded_tokens + embedded_positions\n","\n","  def compute_mask(self, inputs, mask=None):\n","    return tf.math.not_equal(inputs, 0)"],"metadata":{"id":"js5w-qK69feZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `Embeddings` class you've defined is used to create embeddings for tokens in a sequence along with positional encodings. Here's an explanation of the key components:\n","\n","- **Initialization (`__init__`)**:\n","  - `sequence_length`: Length of the input sequence.\n","  - `vocab_size`: Size of the vocabulary (number of unique tokens).\n","  - `embed_dim`: Dimensionality of the token embeddings.\n","\n","  In the `__init__` method:\n","  - `token_embeddings`: This is an `Embedding` layer that maps each token (represented as an integer index) to a dense vector of size `embed_dim`.\n","  - `sequence_length`, `vocab_size`, and `embed_dim` are stored as attributes of the class.\n","\n","- **Call (`call`)**:\n","  The `call` method is used to perform the forward pass of the layer.\n","  - `inputs`: Represents the input sequence (a tensor of shape `[batch_size, sequence_length]` containing integer token indices).\n","  - `embedded_tokens`: Applies the `token_embeddings` layer to the input `inputs`, resulting in a tensor of shape `[batch_size, sequence_length, embed_dim]` where each token is replaced by its corresponding dense embedding vector.\n","  - `embedded_positions`: Calls the `positional_encoding` function to generate positional encodings for the sequence with the specified `embed_dim` and `sequence_length`.\n","  - The final result returned by `call` is the sum of `embedded_tokens` and `embedded_positions`, effectively combining token embeddings with their respective positional encodings.\n","\n","- **Compute Mask (`compute_mask`)**:\n","  The `compute_mask` method is used to generate a mask tensor based on the input `inputs`.\n","  - It returns a mask tensor that is `True` for positions where the input tokens are non-zero (indicating valid tokens) and `False` for positions where the input tokens are zero (indicating padding).\n","  - This mask is typically used in subsequent layers (e.g., in attention mechanisms or masking layers) to ignore padded positions during computation.\n","\n","Overall, this `Embeddings` class encapsulates the process of generating token embeddings along with positional encodings, which is a common technique used in sequence-based models like transformers for natural language processing tasks. The positional encodings help the model distinguish between tokens based on their position in the sequence, providing important positional information to the model."],"metadata":{"id":"ADTbjLhBHzen"}},{"cell_type":"code","source":["test_input=tf.constant([[2,4,7,21,3,5,0,0]])\n","emb=Embeddings(8,20000,512)\n","emb_out=emb(test_input)\n","print(emb_out.shape)"],"metadata":{"id":"KX-maMxK9fhM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `Embeddings` layer defined takes an input tensor of token indices and produces embeddings enhanced with positional encodings. Let's break down how your code works:\n","\n","1. **Initialization**:\n","   - `Embeddings` is initialized with:\n","     - `sequence_length`: `8`, which represents the length of the input sequence.\n","     - `vocab_size`: `20000`, indicating the size of the vocabulary (number of unique tokens).\n","     - `embed_dim`: `512`, specifying the dimensionality of the embedding vectors.\n","\n","2. **Call (`call` method)**:\n","   - When you pass `test_input` (`[[2,4,7,21,3,5,0,0]]`) through the `Embeddings` layer (`emb`), the `call` method is invoked.\n","   - `token_embeddings` (`self.token_embeddings`) is an `Embedding` layer initialized with `vocab_size=20000` and `output_dim=512`. It converts each token index in `test_input` to its corresponding dense embedding vector.\n","   - `embedded_positions` is generated using the `positional_encoding` function, which creates positional encodings based on the `embed_dim` (512) and `sequence_length` (8) parameters.\n","   - The `embedded_tokens` tensor obtained from the `token_embeddings` layer has shape `[1, 8, 512]` (batch size of 1, sequence length of 8, and embedding dimension of 512).\n","   - The `embedded_positions` tensor also has shape `[1, 8, 512]`.\n","   - Finally, the `call` method returns `embedded_tokens + embedded_positions`, resulting in a tensor `emb_out` of shape `[1, 8, 512]`.\n","\n","3. **Output**:\n","   - `emb_out` represents the processed embeddings for the input sequence `[[2,4,7,21,3,5,0,0]]`, where each token is enhanced with positional encoding.\n","   - The shape of `emb_out` is `[1, 8, 512]`, indicating a batch size of 1, sequence length of 8, and embedding dimension of 512.\n","\n","In summary, the `Embeddings` layer combines token embeddings (derived from the `Embedding` layer) with positional encodings (generated by the `positional_encoding` function) to produce enriched embeddings for input sequences, which can be used as inputs to various sequence-based models in natural language processing tasks."],"metadata":{"id":"URjhjWw7KlAX"}},{"cell_type":"code","source":["mask = emb.compute_mask(test_input)\n","print(mask)\n","\n","\n","padding_mask = tf.cast(\n","    tf.repeat(mask,repeats=tf.shape(mask)[1],axis=0),\n","    dtype=tf.int32)\n","print(padding_mask)"],"metadata":{"id":"5-QW7jDP9fjz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","1. **Computing Mask**:\n","   ```python\n","   mask = emb.compute_mask(test_input)\n","   print(mask)\n","   ```\n","\n","   This code snippet computes a mask using the `compute_mask` method of the `Embeddings` instance `emb` with `test_input` as the input.\n","\n","   In the `Embeddings` class, the `compute_mask` method is defined as follows:\n","   ```python\n","   def compute_mask(self, inputs, mask=None):\n","       return tf.math.not_equal(inputs, 0)\n","   ```\n","\n","   - `inputs`: The input tensor containing token indices (`test_input`).\n","   - The `compute_mask` method creates a boolean mask where `True` indicates valid tokens (non-padding), and `False` indicates padding tokens (tokens with value `0`).\n","\n","   Therefore, `mask` will be a boolean tensor with the same shape as `test_input` that indicates which tokens are not padding (`True`) and which are padding (`False`).\n","\n","2. **Creating Padding Mask**:\n","   ```python\n","   padding_mask = tf.cast(\n","       tf.repeat(mask, repeats=tf.shape(mask)[1], axis=0),\n","       dtype=tf.int32)\n","   print(padding_mask)\n","   ```\n","\n","   This code snippet creates a padding mask based on the computed `mask` using `tf.repeat`.\n","\n","   - `tf.shape(mask)[1]` retrieves the sequence length from the `mask`.\n","   - `tf.repeat(mask, repeats=tf.shape(mask)[1], axis=0)` repeats each element of `mask` along the axis `0` (batch dimension) to match the sequence length.\n","   - `tf.cast(..., dtype=tf.int32)` casts the boolean mask into integers (0s and 1s), where `1` represents valid tokens and `0` represents padding tokens.\n","\n","   The resulting `padding_mask` tensor will have the same shape as `test_input`, with the padding tokens (`0`s) replicated along the batch dimension to create a mask suitable for masking sequences during model training or inference.\n","\n","The use of padding masks is common in sequence processing tasks, particularly in Transformer-based models, to mask out padding tokens during computation. This helps in handling variable-length sequences efficiently and accurately within the model."],"metadata":{"id":"JrpzcbxmbWlJ"}},{"cell_type":"code","source":["print(tf.linalg.band_part(\n","        tf.ones([1,8, 8],dtype=tf.int32),-1,0))"],"metadata":{"id":"PqaD2FJx9fmq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The code snippet provided uses `tf.linalg.band_part` to create a mask matrix that masks out (sets to zero) elements above the main diagonal of a square matrix. Let's break down the usage and purpose of this function call:\n","\n","```python\n","print(tf.linalg.band_part(tf.ones([1, 8, 8], dtype=tf.int32), -1, 0))\n","```\n","\n","Here's what each part of this function call does:\n","\n","- `tf.ones([1, 8, 8], dtype=tf.int32)`: This creates a 3D tensor (matrix) filled with ones. The shape of this tensor is `[1, 8, 8]`, meaning it's a batch of one matrix where each matrix is 8x8 filled with ones. The `dtype=tf.int32` specifies the data type of the tensor as integers.\n","\n","- `tf.linalg.band_part(...)`: This function is used to mask out elements of a matrix based on the specified upper and lower bands.\n","\n","  - `tf.linalg.band_part(matrix, num_lower, num_upper)`:\n","    - `matrix`: The input matrix or tensor.\n","    - `num_lower`: The number of subdiagonals below the main diagonal to keep (inclusive of the diagonal). Here, `-1` means keep all subdiagonals below the main diagonal.\n","    - `num_upper`: The number of superdiagonals above the main diagonal to keep. Here, `0` means mask out all elements above the main diagonal.\n","\n","In the provided code, the `tf.ones([1, 8, 8], dtype=tf.int32)` creates an 8x8 matrix filled with ones, and then `tf.linalg.band_part(..., -1, 0)` is applied to this matrix. This function call keeps all elements below the main diagonal (inclusive) and masks out all elements above the main diagonal.\n","\n","The resulting output will be a 3D tensor (matrix) of shape `[1, 8, 8]` where:\n","- The main diagonal and all elements below it are set to `1` (not masked).\n","- All elements above the main diagonal are set to `0` (masked).\n","\n","This type of matrix is commonly used as an attention mask in Transformer models, where it helps define which positions in the input sequence should be attended to during computation and which positions should be ignored (typically padding or future tokens in the case of self-attention).\n","\n","The printed output will show the resulting masked matrix, visualizing how the `tf.linalg.band_part` function applies masking to the input matrix based on the specified `num_lower` and `num_upper` parameters."],"metadata":{"id":"qMzrM8n8biJV"}},{"cell_type":"markdown","source":["##Custom MultiHeadAttention\n"],"metadata":{"id":"ljxWj3haCglF"}},{"cell_type":"code","source":["class CustomSelfAttention(Layer):\n","  def __init__(self,model_size):\n","    super(CustomSelfAttention,self).__init__()\n","    self.model_size=model_size\n","  def call(self,query,key,value,masking):\n","    ######## compute scores\n","    score=tf.matmul(query,key,transpose_b=True)\n","    ######## scaling\n","    score/=tf.math.sqrt(tf.cast(self.model_size,tf.float32))\n","    ######## masking\n","    masking=tf.cast(masking,dtype=tf.float32)\n","    score+=(1.-masking)*-1e10\n","    ######## attention_weights\n","    attention=tf.nn.softmax(score,axis=-1)*masking\n","    ######## output\n","    head=tf.matmul(attention,value)\n","    return head"],"metadata":{"id":"4ABFG_Lq9fph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This `CustomSelfAttention` class is designed to implement a self-attention mechanism, commonly used in Transformer models for processing sequences. Let's break down the functionality of this class and how it computes self-attention.\n","\n","### Class Initialization:\n","```python\n","class CustomSelfAttention(Layer):\n","    def __init__(self, model_size):\n","        super(CustomSelfAttention, self).__init__()\n","        self.model_size = model_size\n","```\n","- `model_size`: The dimensionality of the model. This parameter determines the size of the query, key, and value vectors.\n","\n","### `call` Method:\n","```python\n","def call(self, query, key, value, masking):\n","    # Compute scores\n","    score = tf.matmul(query, key, transpose_b=True)\n","\n","    # Scaling\n","    score /= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n","\n","    # Masking\n","    masking = tf.cast(masking, dtype=tf.float32)\n","    score += (1. - masking) * -1e10\n","\n","    # Attention weights\n","    attention = tf.nn.softmax(score, axis=-1) * masking\n","\n","    # Output\n","    head = tf.matmul(attention, value)\n","    return head\n","```\n","\n","### Breakdown of `call` Method:\n","1. **Compute Scores**:\n","   ```python\n","   score = tf.matmul(query, key, transpose_b=True)\n","   ```\n","   - Computes the dot product of `query` and `key` matrices. The result is a tensor of shape `(batch_size, num_queries, num_keys)`.\n","\n","2. **Scaling**:\n","   ```python\n","   score /= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n","   ```\n","   - Scales the `score` matrix by dividing by the square root of `self.model_size`. This scaling helps stabilize the gradients during training.\n","\n","3. **Masking**:\n","   ```python\n","   masking = tf.cast(masking, dtype=tf.float32)\n","   score += (1. - masking) * -1e10\n","   ```\n","   - `masking` is a binary mask where `0` indicates positions to be masked (ignored) and `1` indicates valid positions.\n","   - Adds a large negative value (`-1e10`) to the positions indicated by `masking` that should be masked out (ignored) during softmax computation. This effectively sets the attention scores for masked positions to `-inf` (approaching zero probability after softmax).\n","\n","4. **Attention Weights**:\n","   ```python\n","   attention = tf.nn.softmax(score, axis=-1) * masking\n","   ```\n","   - Computes the attention weights by applying softmax along the last axis (`-1`) of the `score` matrix. The softmax operation converts scores into probabilities while respecting the mask (`masking`) to zero out masked positions.\n","\n","5. **Output**:\n","   ```python\n","   head = tf.matmul(attention, value)\n","   ```\n","   - Computes the weighted sum of `value` vectors using the computed `attention` weights (`softmax scores`), resulting in the `head` tensor.\n","\n","### Input Arguments:\n","- `query`: The query tensor representing the queries for the self-attention mechanism.\n","- `key`: The key tensor representing the keys for the self-attention mechanism.\n","- `value`: The value tensor representing the values for the self-attention mechanism.\n","- `masking`: A binary mask tensor indicating which positions in the sequence should be masked (ignored).\n","\n","### Output:\n","- `head`: The output tensor after applying the self-attention mechanism, representing the attended values based on the input queries, keys, values, and mask.\n","\n","This `CustomSelfAttention` class encapsulates the key components of a self-attention layer, providing a customizable and reusable implementation suitable for integration into larger Transformer architectures for sequence processing tasks."],"metadata":{"id":"M0Rq1WVnbxtC"}},{"cell_type":"code","source":["attention=CustomSelfAttention(256)\n","attention(tf.ones([1,8,256]),tf.ones([1,8,256]),tf.ones([1,8,256]),padding_mask)"],"metadata":{"id":"EVMza7-M9fsF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To use the `CustomSelfAttention` layer we've defined and apply it with example input tensors, let's walk through how to create an instance of `CustomSelfAttention` and pass input tensors (`query`, `key`, `value`) along with a padding mask (`padding_mask`) to compute the self-attention mechanism.\n","\n","Given the `CustomSelfAttention` class definition and an example call:\n","\n","```python\n","attention = CustomSelfAttention(256)\n","query = tf.ones([1, 8, 256])\n","key = tf.ones([1, 8, 256])\n","value = tf.ones([1, 8, 256])\n","padding_mask = padding_mask  # Assuming you have defined `padding_mask` appropriately\n","\n","result = attention(query, key, value, padding_mask)\n","print(result.shape)\n","```\n","\n","Here's what happens step by step:\n","\n","1. **Create `CustomSelfAttention` Instance**:\n","   ```python\n","   attention = CustomSelfAttention(256)\n","   ```\n","   - Initialize an instance of `CustomSelfAttention` with `model_size = 256`.\n","\n","2. **Define Input Tensors**:\n","   ```python\n","   query = tf.ones([1, 8, 256])   # Shape: [batch_size=1, num_queries=8, model_size=256]\n","   key = tf.ones([1, 8, 256])     # Shape: [batch_size=1, num_keys=8, model_size=256]\n","   value = tf.ones([1, 8, 256])   # Shape: [batch_size=1, num_values=8, model_size=256]\n","   ```\n","   - Create example input tensors (`query`, `key`, `value`) each with a batch size of `1`, 8 queries/keys/values, and a model size of `256`.\n","\n","3. **Compute Self-Attention**:\n","   ```python\n","   result = attention(query, key, value, padding_mask)\n","   ```\n","   - Pass the input tensors (`query`, `key`, `value`) and the padding mask (`padding_mask`) to the `attention` layer.\n","   - Inside the `call` method of `CustomSelfAttention`:\n","     - Compute attention scores (`score`) using matrix multiplication between `query` and `key`.\n","     - Scale the scores by dividing by the square root of `model_size`.\n","     - Apply masking by adding a large negative value to masked positions (`padding_mask`) to ignore them during softmax computation.\n","     - Compute attention weights (`attention`) using softmax over the masked scores.\n","     - Calculate the attended values (`head`) by applying the attention weights to `value` through matrix multiplication.\n","   - `result` will be the output of the self-attention mechanism based on the input tensors and padding mask.\n","\n","4. **Print Result Shape**:\n","   ```python\n","   print(result.shape)\n","   ```\n","   - This prints the shape of `result`, which represents the output of the self-attention mechanism.\n","   - The shape will be `[1, 8, 256]`, indicating a batch size of `1`, `8` attended values (corresponding to `num_queries`), and a model size of `256`."],"metadata":{"id":"GLAGGnI3cJPc"}},{"cell_type":"code","source":["class CustomMultiHeadAttention(Layer):\n","  def __init__(self,num_heads,key_dim):\n","    super(CustomMultiHeadAttention,self).__init__()\n","\n","    self.num_heads=num_heads\n","    self.dense_q=[Dense(key_dim//num_heads) for _ in range(num_heads)]\n","    self.dense_k=[Dense(key_dim//num_heads) for _ in range(num_heads)]\n","    self.dense_v=[Dense(key_dim//num_heads) for _ in range(num_heads)]\n","    self.dense_o=Dense(key_dim)\n","    self.self_attention=CustomSelfAttention(key_dim)\n","\n","  def call(self,query,key,value,attention_mask):\n","    heads=[]\n","\n","    for i in range(self.num_heads):\n","      print(\"hello\", self.dense_q[i](query).shape)\n","      head=self.self_attention(self.dense_q[i](query),self.dense_k[i](key),\n","                              self.dense_v[i](value),attention_mask)\n","      heads.append(head)\n","    print(\"head\", tf.convert_to_tensor(heads).shape)\n","    heads=tf.concat(heads,axis=2)\n","    heads=self.dense_o(heads)\n","    return heads"],"metadata":{"id":"H7-WbIZF9fvD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `CustomMultiHeadAttention` class defined is designed to implement the multi-head attention mechanism using a set of dense layers for queries, keys, and values. Let's break down the functionality of this class and how it applies multi-head attention.\n","\n","### Class Initialization:\n","```python\n","class CustomMultiHeadAttention(Layer):\n","    def __init__(self, num_heads, key_dim):\n","        super(CustomMultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.dense_q = [Dense(key_dim // num_heads) for _ in range(num_heads)]\n","        self.dense_k = [Dense(key_dim // num_heads) for _ in range(num_heads)]\n","        self.dense_v = [Dense(key_dim // num_heads) for _ in range(num_heads)]\n","        self.dense_o = Dense(key_dim)\n","        self.self_attention = CustomSelfAttention(key_dim)\n","```\n","- `num_heads`: Number of attention heads.\n","- `key_dim`: Dimensionality of the key vectors.\n","\n","In the `__init__` method:\n","- `self.dense_q`, `self.dense_k`, and `self.dense_v` are lists of `Dense` layers used to project the query, key, and value vectors into `key_dim // num_heads` dimensions for each attention head.\n","- `self.dense_o` is a `Dense` layer used to project concatenated multi-head attention outputs back to the original `key_dim`.\n","- `self.self_attention` is an instance of `CustomSelfAttention` with `key_dim` as the model size.\n","\n","### `call` Method:\n","```python\n","def call(self, query, key, value, attention_mask):\n","    heads = []\n","\n","    for i in range(self.num_heads):\n","        q = self.dense_q[i](query)\n","        k = self.dense_k[i](key)\n","        v = self.dense_v[i](value)\n","        head = self.self_attention(q, k, v, attention_mask)\n","        heads.append(head)\n","\n","    heads = tf.concat(heads, axis=2)\n","    heads = self.dense_o(heads)\n","    return heads\n","```\n","- `query`, `key`, `value`: Input tensors representing queries, keys, and values.\n","- `attention_mask`: Mask tensor used to mask out padding tokens during attention computation.\n","\n","In the `call` method:\n","1. **Loop Over Attention Heads**:\n","   - Iterate over each attention head (`num_heads` times).\n","   - For each head `i`, project `query`, `key`, and `value` using `dense_q[i]`, `dense_k[i]`, and `dense_v[i]` respectively to reduce dimensionality (`key_dim // num_heads`).\n","\n","2. **Apply Self-Attention**:\n","   - Pass the projected `query`, `key`, and `value` tensors along with the `attention_mask` to the `self_attention` layer (an instance of `CustomSelfAttention`).\n","\n","3. **Concatenate Attention Heads**:\n","   - Concatenate the output heads along the last dimension (`axis=2`), resulting in a tensor of shape `(batch_size, num_queries, key_dim)`.\n","\n","4. **Output Projection**:\n","   - Project the concatenated heads back to the original `key_dim` using the `dense_o` layer.\n","\n","5. **Return Output**:\n","   - Return the final output tensor after applying multi-head attention and projection.\n","\n","### Usage Example:\n","```python\n","# Create an instance of CustomMultiHeadAttention\n","multihead_attention = CustomMultiHeadAttention(num_heads=8, key_dim=512)\n","\n","# Example usage with query, key, value, and attention_mask tensors\n","query = tf.ones([1, 8, 512])   # Shape: [batch_size=1, num_queries=8, key_dim=512]\n","key = tf.ones([1, 8, 512])     # Shape: [batch_size=1, num_keys=8, key_dim=512]\n","value = tf.ones([1, 8, 512])   # Shape: [batch_size=1, num_values=8, key_dim=512]\n","attention_mask = padding_mask  # Assuming you have defined `padding_mask`\n","\n","# Apply multi-head attention\n","result = multihead_attention(query, key, value, attention_mask)\n","print(result.shape)  # Output shape: (1, 8, 512)\n","```\n","- `query`, `key`, and `value` are input tensors with shape `(batch_size=1, num_queries=8, key_dim=512)`.\n","- `attention_mask` is a mask tensor used to mask out padding tokens during attention computation.\n","- `result` will be the output tensor after applying multi-head attention and projection, with shape `(1, 8, 512)`."],"metadata":{"id":"TjXmFpcRcnfC"}},{"cell_type":"markdown","source":["##Encoder"],"metadata":{"id":"e0fiTdDuDasz"}},{"cell_type":"code","source":["class TransformerEncoder(Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads,):\n","        super(TransformerEncoder, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = CustomMultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim,\n","        )\n","        self.dense_proj=tf.keras.Sequential(\n","            [Dense(dense_dim, activation=\"relu\"),\n","             Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = LayerNormalization()\n","        self.layernorm_2 = LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","\n","      if mask is not None:\n","        mask = tf.cast(\n","            mask[:,tf.newaxis, :], dtype=\"int32\")\n","        T = tf.shape(mask)[2]\n","        padding_mask = tf.repeat(mask,T,axis=1)\n","      attention_output = self.attention(\n","          query=inputs, key=inputs,value=inputs,\n","          attention_mask=padding_mask\n","      )\n","\n","      proj_input = self.layernorm_1(inputs + attention_output)\n","      proj_output = self.dense_proj(proj_input)\n","      return self.layernorm_2(proj_input + proj_output)"],"metadata":{"id":"-5B08mWm9fxn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `TransformerEncoder` class defined is a part of the Transformer model architecture and represents a single layer of the Transformer encoder. Let's break down the functionality of this class and how it processes input sequences through attention mechanisms and feed-forward networks.\n","\n","### Class Initialization:\n","```python\n","class TransformerEncoder(Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads):\n","        super(TransformerEncoder, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","\n","        # Multi-head self-attention mechanism\n","        self.attention = CustomMultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","\n","        # Feed-forward network\n","        self.dense_proj = tf.keras.Sequential([\n","            Dense(dense_dim, activation=\"relu\"),\n","            Dense(embed_dim),\n","        ])\n","\n","        # Layer normalization\n","        self.layernorm_1 = LayerNormalization()\n","        self.layernorm_2 = LayerNormalization()\n","\n","        # Set masking support\n","        self.supports_masking = True\n","```\n","- `embed_dim`: The dimensionality of the input embeddings and attention mechanisms.\n","- `dense_dim`: The dimensionality of the intermediate dense layer in the feed-forward network.\n","- `num_heads`: The number of attention heads used in the multi-head attention mechanism.\n","\n","In the `__init__` method:\n","- `self.attention`: Initializes a `CustomMultiHeadAttention` layer with the specified `num_heads` and `key_dim` (equal to `embed_dim`).\n","- `self.dense_proj`: Defines a sequential feed-forward network with two dense layers: the first layer (`Dense(dense_dim, activation=\"relu\")`) applies a ReLU activation function, and the second layer (`Dense(embed_dim)`) projects the output back to the original embedding dimension.\n","- `self.layernorm_1` and `self.layernorm_2`: Layer normalization layers to normalize inputs before and after the attention and feed-forward layers.\n","\n","### `call` Method:\n","```python\n","def call(self, inputs, mask=None):\n","    if mask is not None:\n","        mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","        T = tf.shape(mask)[2]\n","        padding_mask = tf.repeat(mask, T, axis=1)\n","\n","    # Apply multi-head self-attention\n","    attention_output = self.attention(\n","        query=inputs, key=inputs, value=inputs, attention_mask=padding_mask\n","    )\n","\n","    # Residual connection and layer normalization\n","    proj_input = self.layernorm_1(inputs + attention_output)\n","\n","    # Apply feed-forward network\n","    proj_output = self.dense_proj(proj_input)\n","\n","    # Residual connection and layer normalization\n","    return self.layernorm_2(proj_input + proj_output)\n","```\n","- `inputs`: The input tensor representing the sequence of embeddings.\n","- `mask`: An optional mask tensor indicating which positions in the input sequence should be ignored during computation.\n","\n","In the `call` method:\n","1. **Masking Preparation**:\n","   - If `mask` is provided, convert it to the appropriate format (`padding_mask`) to use with the attention mechanism.\n","\n","2. **Apply Multi-Head Self-Attention**:\n","   - Use the `CustomMultiHeadAttention` layer (`self.attention`) to compute attention over the input sequence (`query=inputs`, `key=inputs`, `value=inputs`) with the provided `attention_mask`.\n","\n","3. **Residual Connection and Layer Normalization (1st)**:\n","   - Add the attention output (`attention_output`) to the input (`inputs`) to form a residual connection.\n","   - Normalize the result using `self.layernorm_1`.\n","\n","4. **Apply Feed-Forward Network**:\n","   - Pass the normalized output (`proj_input`) through the feed-forward network (`self.dense_proj`) to compute the intermediate projection (`proj_output`).\n","\n","5. **Residual Connection and Layer Normalization (2nd)**:\n","   - Add the feed-forward output (`proj_output`) to the normalized input (`proj_input`) to form another residual connection.\n","   - Normalize the final result using `self.layernorm_2` and return the output.\n","\n","### Usage Example:\n","```python\n","# Create an instance of TransformerEncoder\n","encoder_layer = TransformerEncoder(embed_dim=512, dense_dim=2048, num_heads=8)\n","\n","# Example usage with input tensor (batch_size=1, sequence_length=8, embed_dim=512)\n","inputs = tf.random.normal([1, 8, 512])\n","output = encoder_layer(inputs)\n","print(output.shape)  # Output shape: (1, 8, 512)\n","```\n","- `inputs`: Input tensor representing a sequence of embeddings with shape `(batch_size=1, sequence_length=8, embed_dim=512)`.\n","- `output`: The encoded output tensor after passing through the `TransformerEncoder` layer.\n","- The `TransformerEncoder` layer applies multi-head self-attention and feed-forward networks to process the input sequence and returns the encoded output."],"metadata":{"id":"4UJTfVg0dAZE"}},{"cell_type":"code","source":["encoder_outputs = TransformerEncoder(512,2048,8)(emb_out)\n","print(encoder_outputs.shape)\n"],"metadata":{"id":"ok7D6ipr9f0c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the provided code snippet, you're instantiating a `TransformerEncoder` layer and applying it to some input tensor `emb_out` to obtain the encoder outputs.\n","### Code Snippet:\n","```python\n","encoder_outputs = TransformerEncoder(512, 2048, 8)(emb_out)\n","print(encoder_outputs.shape)\n","```\n","\n","### Explanation:\n","1. **Instantiate `TransformerEncoder`**:\n","   ```python\n","   TransformerEncoder(512, 2048, 8)\n","   ```\n","   - Creates an instance of `TransformerEncoder` with the following parameters:\n","     - `embed_dim=512`: The dimensionality of the input embeddings and attention mechanisms.\n","     - `dense_dim=2048`: The dimensionality of the intermediate dense layer in the feed-forward network.\n","     - `num_heads=8`: The number of attention heads used in the multi-head attention mechanism.\n","\n","2. **Apply `TransformerEncoder` to `emb_out`**:\n","   ```python\n","   TransformerEncoder(512, 2048, 8)(emb_out)\n","   ```\n","   - Calls the `TransformerEncoder` instance as a function with `emb_out` as the input tensor.\n","   - `emb_out` is assumed to be a tensor representing the output of an embedding layer, with shape `(batch_size, sequence_length, embed_dim)`.\n","\n","3. **Compute Encoder Outputs**:\n","   - The `TransformerEncoder` layer processes the input `emb_out` through multi-head self-attention and feed-forward networks according to its defined `call` method.\n","   - The resulting `encoder_outputs` represent the encoded sequence after passing through the `TransformerEncoder` layer.\n","\n","4. **Print Encoder Outputs Shape**:\n","   ```python\n","   print(encoder_outputs.shape)\n","   ```\n","   - Outputs the shape of `encoder_outputs`, which reflects the shape of the encoded sequence after processing through the `TransformerEncoder` layer.\n","   - The shape of `encoder_outputs` will typically be `(batch_size, sequence_length, embed_dim)`, where:\n","     - `batch_size` is the number of input sequences.\n","     - `sequence_length` is the length of each input sequence.\n","     - `embed_dim` is the dimensionality of the encoded representations.\n","\n","### Usage Example:\n","```python\n","# Assuming emb_out is an input tensor with shape (batch_size, sequence_length, embed_dim)\n","import tensorflow as tf\n","\n","# Define TransformerEncoder instance and apply to emb_out\n","encoder_outputs = TransformerEncoder(512, 2048, 8)(emb_out)\n","\n","# Print the shape of the encoder outputs\n","print(encoder_outputs.shape)\n","```\n","\n","### Output:\n","The printed `encoder_outputs.shape` will indicate the shape of the encoded outputs after passing through the `TransformerEncoder` layer. The specific shape will depend on the input `emb_out` and the parameters (`embed_dim`, `dense_dim`, `num_heads`) used to configure the `TransformerEncoder`.\n","\n"," This example demonstrates how to apply the `TransformerEncoder` layer within a TensorFlow model to encode input sequences using Transformer-based architecture for tasks like natural language processing (NLP) or sequence modeling."],"metadata":{"id":"y7EmRjdLdUnZ"}},{"cell_type":"markdown","source":["##Decoder"],"metadata":{"id":"QWAY7EfsDo1v"}},{"cell_type":"code","source":["print(tf.linalg.band_part(\n","        tf.ones([1,8, 8],dtype=tf.int32),-1,0))"],"metadata":{"id":"GfL12P3m9f3U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `tf.linalg.band_part` function is used to create a matrix mask that masks out elements above the main diagonal of a square matrix. Let's break down the usage of this function and understand its output.\n","\n","### Code Explanation:\n","```python\n","print(tf.linalg.band_part(tf.ones([1, 8, 8], dtype=tf.int32), -1, 0))\n","```\n","\n","### Function Parameters:\n","- `tf.ones([1, 8, 8], dtype=tf.int32)`: Creates a 3D tensor (matrix) filled with ones.\n","  - Shape: `[1, 8, 8]` - Represents a batch of one matrix where each matrix is 8x8 and filled with ones.\n","  - `dtype=tf.int32`: Specifies the data type of the tensor as integers.\n","\n","- `tf.linalg.band_part(matrix, num_lower, num_upper)`:\n","  - `matrix`: The input matrix or tensor.\n","  - `num_lower`: Number of subdiagonals to keep (including the main diagonal).\n","    - `-1`: Keep all subdiagonals below the main diagonal.\n","  - `num_upper`: Number of superdiagonals to keep (including the main diagonal).\n","    - `0`: Keep only the main diagonal and elements below it.\n","\n","### Output Explanation:\n","The `tf.linalg.band_part` function masks out elements above the main diagonal based on the provided parameters (`num_lower` and `num_upper`).\n","\n","- `tf.linalg.band_part(..., -1, 0)`: Masks out all elements above the main diagonal (including the main diagonal itself) of the input matrix.\n","\n","### Example Output:\n","If we consider a sample input tensor `tf.ones([1, 8, 8], dtype=tf.int32)`, the output of `tf.linalg.band_part(..., -1, 0)` will be a tensor where:\n","- All elements above the main diagonal (and including the main diagonal) are retained as ones (`1`).\n","- All elements below the main diagonal are set to zero (`0`), effectively creating an upper triangular matrix with ones along the main diagonal and below.\n","\n","### Example Usage:\n","```python\n","import tensorflow as tf\n","\n","# Create a tensor filled with ones and apply band_part to mask out elements above the main diagonal\n","masked_matrix = tf.linalg.band_part(tf.ones([1, 8, 8], dtype=tf.int32), -1, 0)\n","\n","# Print the resulting masked matrix\n","print(masked_matrix)\n","```\n","\n","### Output:\n","The output of `print(masked_matrix)` will be a tensor representing the masked matrix where all elements above the main diagonal are zeros (`0`), and elements on and below the main diagonal are ones (`1`).\n","\n","This type of matrix masking is commonly used in sequence processing tasks, such as in Transformer models, to mask out future tokens or pad tokens during self-attention computation, ensuring that only valid positions are attended to during the model's processing."],"metadata":{"id":"lCCt18HmdqmZ"}},{"cell_type":"code","source":["class TransformerDecoder(Layer):\n","  def __init__(self, embed_dim, latent_dim, num_heads,):\n","    super(TransformerDecoder, self).__init__()\n","    self.embed_dim = embed_dim\n","    self.latent_dim = latent_dim\n","    self.num_heads = num_heads\n","    self.attention_1=MultiHeadAttention(\n","        num_heads=num_heads, key_dim=embed_dim\n","    )\n","    self.attention_2=MultiHeadAttention(\n","        num_heads=num_heads, key_dim=embed_dim\n","    )\n","    self.dense_proj = tf.keras.Sequential(\n","        [Dense(latent_dim, activation=\"relu\"),Dense(embed_dim),]\n","    )\n","    self.layernorm_1=LayerNormalization()\n","    self.layernorm_2=LayerNormalization()\n","    self.layernorm_3=LayerNormalization()\n","    self.supports_masking = True\n","  def call(self, inputs, encoder_outputs, enc_mask, mask=None):\n","\n","\n","    if mask is not None:\n","      causal_mask=tf.linalg.band_part(\n","        tf.ones([tf.shape(inputs)[0],\n","                 tf.shape(inputs)[1],\n","                 tf.shape(inputs)[1]],dtype=tf.int32),-1,0)\n","      mask = tf.cast(\n","          mask[:,tf.newaxis, :], dtype=\"int32\")\n","      enc_mask = tf.cast(\n","          enc_mask[:,tf.newaxis, :], dtype=\"int32\")\n","      T = tf.shape(mask)[2]\n","      padding_mask = tf.repeat(mask,T,axis=1)\n","      cross_attn_mask = tf.repeat(enc_mask,T,axis=1)\n","      combined_mask=tf.minimum(padding_mask,causal_mask)\n","\n","    attention_output_1 = self.attention_1(\n","        query=inputs,key=inputs,value=inputs,\n","        attention_mask=combined_mask,\n","\n","    )\n","\n","    out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","    attention_output_2= self.attention_2(\n","        query=out_1,key=encoder_outputs,value=encoder_outputs,\n","        attention_mask=cross_attn_mask,\n","\n","    )\n","    out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","    proj_output = self.dense_proj(out_2)\n","    return self.layernorm_3(out_2 + proj_output)"],"metadata":{"id":"WjDVfUXc9f5-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `TransformerDecoder` class we've implemented represents a decoder layer within a Transformer architecture. Let's break down the functionality of this class and understand how it processes inputs, performs self-attention, cross-attention with encoder outputs, and applies feed-forward transformations.\n","\n","### Class Initialization:\n","```python\n","class TransformerDecoder(Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads):\n","        super(TransformerDecoder, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","\n","        # Self-attention mechanism within the decoder\n","        self.attention_1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","\n","        # Cross-attention mechanism between decoder and encoder\n","        self.attention_2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","\n","        # Feed-forward network for projection\n","        self.dense_proj = tf.keras.Sequential([\n","            Dense(latent_dim, activation=\"relu\"),\n","            Dense(embed_dim),\n","        ])\n","\n","        # Layer normalization layers\n","        self.layernorm_1 = LayerNormalization()\n","        self.layernorm_2 = LayerNormalization()\n","        self.layernorm_3 = LayerNormalization()\n","\n","        # Set masking support\n","        self.supports_masking = True\n","```\n","- `embed_dim`: The dimensionality of the input embeddings and attention mechanisms.\n","- `latent_dim`: The dimensionality of the intermediate dense layer in the feed-forward network.\n","- `num_heads`: The number of attention heads used in the multi-head attention mechanisms.\n","\n","In the `__init__` method:\n","- `self.attention_1`: Initializes a `MultiHeadAttention` layer for self-attention within the decoder.\n","- `self.attention_2`: Initializes another `MultiHeadAttention` layer for cross-attention between the decoder and encoder.\n","- `self.dense_proj`: Defines a sequential feed-forward network with two dense layers (`Dense(latent_dim, activation=\"relu\")` followed by `Dense(embed_dim)`) to project the decoder outputs back to the original embedding dimension.\n","- `self.layernorm_1`, `self.layernorm_2`, `self.layernorm_3`: Layer normalization layers to normalize inputs before and after attention and feed-forward transformations.\n","\n","### `call` Method:\n","```python\n","def call(self, inputs, encoder_outputs, enc_mask, mask=None):\n","    # Prepare masks for attention mechanisms\n","    if mask is not None:\n","        causal_mask = tf.linalg.band_part(tf.ones_like(inputs, dtype=tf.int32), -1, 0)\n","        mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","        enc_mask = tf.cast(enc_mask[:, tf.newaxis, :], dtype=\"int32\")\n","        T = tf.shape(mask)[2]\n","        padding_mask = tf.repeat(mask, T, axis=1)\n","        cross_attn_mask = tf.repeat(enc_mask, T, axis=1)\n","        combined_mask = tf.minimum(padding_mask, causal_mask)\n","\n","    # Self-attention within the decoder\n","    attention_output_1 = self.attention_1(\n","        query=inputs, key=inputs, value=inputs, attention_mask=combined_mask\n","    )\n","    out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","    # Cross-attention between decoder and encoder outputs\n","    attention_output_2 = self.attention_2(\n","        query=out_1, key=encoder_outputs, value=encoder_outputs, attention_mask=cross_attn_mask\n","    )\n","    out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","    # Feed-forward projection\n","    proj_output = self.dense_proj(out_2)\n","    return self.layernorm_3(out_2 + proj_output)\n","```\n","- `inputs`: The input tensor representing the sequence of decoder embeddings.\n","- `encoder_outputs`: The encoder outputs used for cross-attention.\n","- `enc_mask`: The mask tensor for encoder outputs.\n","- `mask`: An optional mask tensor indicating which positions in the input sequence should be ignored during computation.\n","\n","In the `call` method:\n","1. **Prepare Masks**:\n","   - Constructs masks (`causal_mask`, `padding_mask`, `cross_attn_mask`) for use in attention mechanisms.\n","   - `causal_mask`: Masks out future tokens to ensure causal self-attention.\n","   - `padding_mask`, `cross_attn_mask`: Masks out padding tokens and applies cross-attention between decoder and encoder outputs.\n","\n","2. **Self-Attention within Decoder** (`attention_1`):\n","   - Applies self-attention to the decoder inputs (`inputs`) using `attention_1` (multi-head attention within the decoder).\n","   - Normalizes the output using `layernorm_1`.\n","\n","3. **Cross-Attention between Decoder and Encoder** (`attention_2`):\n","   - Uses cross-attention to attend to encoder outputs (`encoder_outputs`) from the decoder inputs (`out_1`) using `attention_2`.\n","   - Normalizes the output using `layernorm_2`.\n","\n","4. **Feed-Forward Projection** (`dense_proj`):\n","   - Projects the normalized output (`out_2`) through the feed-forward network (`dense_proj`) to transform the decoder representations.\n","   - Normalizes the final output using `layernorm_3` and returns the result.\n","\n","### Usage Example:\n","```python\n","import tensorflow as tf\n","\n","# Create an instance of TransformerDecoder\n","decoder_layer = TransformerDecoder(embed_dim=512, latent_dim=2048, num_heads=8)\n","\n","# Example usage with inputs, encoder_outputs, enc_mask, and mask tensors\n","inputs = tf.random.normal([1, 10, 512])        # Decoder inputs (batch_size=1, sequence_length=10, embed_dim=512)\n","encoder_outputs = tf.random.normal([1, 8, 512]) # Encoder outputs (batch_size=1, encoder_seq_length=8, embed_dim=512)\n","enc_mask = tf.ones([1, 1, 8], dtype=tf.int32)    # Encoder mask (batch_size=1, num_heads=1, encoder_seq_length=8)\n","mask = tf.ones([1, 1, 10], dtype=tf.int32)      # Mask for decoder inputs (batch_size=1, num_heads=1, sequence_length=10)\n","\n","# Apply TransformerDecoder to process inputs\n","decoder_outputs = decoder_layer(inputs, encoder_outputs, enc_mask, mask)\n","\n","# Print the shape of the decoder outputs\n","print(decoder_outputs.shape)  # Output shape: (1, 10, 512)\n","```\n","\n","### Output:\n","The printed `decoder_outputs.shape` will indicate the shape of the decoder outputs after processing through the `TransformerDecoder` layer. The specific shape will depend on the input tensors (`inputs`, `encoder_outputs`) and the mask tensors (`enc_mask`, `mask`) used during\n","\n"," the computation.\n","\n","This example demonstrates how to use the `TransformerDecoder` layer within a Transformer model to decode sequence representations, perform self-attention and cross-attention, and transform decoder inputs into meaningful outputs based on encoder outputs."],"metadata":{"id":"pdYIOEN6d_J2"}},{"cell_type":"code","source":["enc_mask=mask\n","decoder_outputs = TransformerDecoder(512,2048,4)(emb_out,encoder_outputs,enc_mask)\n","print(decoder_outputs.shape)"],"metadata":{"id":"3HkVW3wL9f8n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the provided code snippet, we are applying the `TransformerDecoder` layer to decode sequence representations based on input embeddings (`emb_out`), encoder outputs (`encoder_outputs`), and a mask (`enc_mask`). Let's analyze how this process works:\n","\n","### Code Explanation:\n","```python\n","enc_mask = mask\n","decoder_outputs = TransformerDecoder(512, 2048, 4)(emb_out, encoder_outputs, enc_mask)\n","print(decoder_outputs.shape)\n","```\n","\n","### Steps:\n","1. **Assign Mask (`enc_mask = mask`)**:\n","   - The variable `enc_mask` is assigned the value of `mask`. Assuming `mask` is a tensor indicating positions to be masked (e.g., padding positions), `enc_mask` will be used as a mask for encoder outputs during cross-attention in the decoder.\n","\n","2. **Apply `TransformerDecoder`**:\n","   - Create an instance of `TransformerDecoder` with specified parameters (`embed_dim=512`, `latent_dim=2048`, `num_heads=4`).\n","   - Call the `TransformerDecoder` instance as a function with input arguments:\n","     - `emb_out`: Input tensor representing decoder embeddings (shape: `[batch_size, sequence_length, embed_dim]`).\n","     - `encoder_outputs`: Tensor representing encoder outputs (used for cross-attention) (shape: `[batch_size, encoder_seq_length, embed_dim]`).\n","     - `enc_mask`: Mask tensor for encoder outputs (shape: `[batch_size, 1, encoder_seq_length]`).\n","\n","3. **Compute Decoder Outputs**:\n","   - The `TransformerDecoder` processes the inputs (`emb_out`) with cross-attention to the `encoder_outputs` using the provided `enc_mask`.\n","   - The `decoder_outputs` represent the decoded sequence outputs after processing through the `TransformerDecoder` layer.\n","\n","4. **Print Output Shape**:\n","   - Print the shape of `decoder_outputs` to inspect the dimensions of the decoded sequence.\n","\n","### Example Usage:\n","```python\n","import tensorflow as tf\n","\n","# Assuming emb_out and encoder_outputs are defined tensors\n","emb_out = tf.random.normal([1, 10, 512])        # Decoder inputs (batch_size=1, sequence_length=10, embed_dim=512)\n","encoder_outputs = tf.random.normal([1, 8, 512]) # Encoder outputs (batch_size=1, encoder_seq_length=8, embed_dim=512)\n","mask = tf.ones([1, 1, 10], dtype=tf.int32)       # Mask for decoder inputs (batch_size=1, num_heads=1, sequence_length=10)\n","\n","# Assign mask to enc_mask\n","enc_mask = mask\n","\n","# Apply TransformerDecoder to decode sequence representations\n","decoder_outputs = TransformerDecoder(512, 2048, 4)(emb_out, encoder_outputs, enc_mask)\n","\n","# Print the shape of the decoder outputs\n","print(decoder_outputs.shape)  # Output shape: (1, 10, 512)\n","```\n","\n","### Output:\n","The printed `decoder_outputs.shape` will represent the shape of the decoded sequence outputs after passing through the `TransformerDecoder` layer. The specific shape `(1, 10, 512)` indicates:\n","- `1`: Batch size.\n","- `10`: Sequence length (number of tokens in the decoded sequence).\n","- `512`: Embedding dimension (dimensionality of the decoded sequence representations)."],"metadata":{"id":"NmvYzDQie326"}},{"cell_type":"markdown","source":["##Transformer Model"],"metadata":{"id":"F1mLi8zBDyxC"}},{"cell_type":"code","source":["EMBEDDING_DIM=512\n","D_FF=2048\n","NUM_HEADS=8\n","NUM_LAYERS=1\n","NUM_EPOCHS=10"],"metadata":{"id":"IfmzJEEq9f_W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Constants related to the dimensions and configurations of a Transformer-based model, including embedding dimension(`EMBEDDING_DIM`),\n"," feed-forward dimensions (`D_FF`), the number of attention heads (`NUM_HEADS`), the number of layers (`NUM_LAYERS`), and the number of training epochs (`NUM_EPOCHS`).\n","\n"," These constants are typically used to configure and train a Transformer model for specific tasks, such as natural language processing (NLP) tasks like machine translation or text generation.\n","\n","\n"],"metadata":{"id":"40p88h5YhHPt"}},{"cell_type":"code","source":["encoder_inputs=Input(shape=(None,), dtype=\"int64\", name=\"input_1\")\n","emb = Embeddings(ENGLISH_SEQUENCE_LENGTH,VOCAB_SIZE,EMBEDDING_DIM)\n","x = emb(encoder_inputs)\n","enc_mask = emb.compute_mask(encoder_inputs)\n","\n","for _ in range(NUM_LAYERS):\n","  x=TransformerEncoder(EMBEDDING_DIM,D_FF,NUM_HEADS)(x)\n","encoder_outputs=x\n","\n","decoder_inputs=Input(shape=(None,), dtype=\"int64\", name=\"input_2\")\n","\n","x = Embeddings(FRENCH_SEQUENCE_LENGTH,VOCAB_SIZE,EMBEDDING_DIM)(decoder_inputs)\n","for i in range(NUM_LAYERS):\n","  x=TransformerDecoder(EMBEDDING_DIM,D_FF,NUM_HEADS)(x, encoder_outputs,enc_mask)\n","x=tf.keras.layers.Dropout(0.5)(x)\n","decoder_outputs=Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n","\n","transformer = tf.keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")\n","transformer.summary()"],"metadata":{"id":"vRwKjydE9gB9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The code provided outlines the construction of a Transformer model using TensorFlow/Keras for a sequence-to-sequence (seq2seq) task, likely machine translation given the encoder-decoder setup.\n","### Code Breakdown:\n","\n","1. **Encoder Inputs**:\n","   - Define an input layer `encoder_inputs` to receive the encoder sequence data.\n","   - Shape: `(None,)` - Allows variable-length sequences.\n","   - Data type: `int64` - Represents integer indices of words in the vocabulary.\n","\n","2. **Embedding Layer (Encoder)**:\n","   - Instantiate an `Embeddings` layer (`emb`) to convert input indices into dense embeddings.\n","   - `ENGLISH_SEQUENCE_LENGTH`: Length of the English input sequence.\n","   - `VOCAB_SIZE`: Size of the vocabulary.\n","   - `EMBEDDING_DIM`: Dimensionality of the word embeddings.\n","\n","3. **Compute Mask (Encoder)**:\n","   - Compute the masking tensor (`enc_mask`) for the encoder inputs using `emb.compute_mask`.\n","   - The mask is used to ignore padding tokens during self-attention within the Transformer encoder.\n","\n","4. **Transformer Encoder Layers**:\n","   - Apply multiple layers of `TransformerEncoder` to the embedded encoder inputs (`x`).\n","   - Each layer performs multi-head self-attention and feed-forward network transformations.\n","   - `EMBEDDING_DIM`: Dimensionality of the embeddings.\n","   - `D_FF`: Dimensionality of the feed-forward layer.\n","   - `NUM_HEADS`: Number of attention heads.\n","\n","5. **Encoder Outputs**:\n","   - Store the final encoder outputs (`encoder_outputs`) after processing through all layers of the Transformer encoder.\n","\n","6. **Decoder Inputs**:\n","   - Define an input layer `decoder_inputs` to receive the decoder sequence data.\n","   - Shape and data type are similar to `encoder_inputs`.\n","\n","7. **Embedding Layer (Decoder)**:\n","   - Instantiate another `Embeddings` layer (`Embeddings(FRENCH_SEQUENCE_LENGTH, VOCAB_SIZE, EMBEDDING_DIM)`) to convert decoder input indices into dense embeddings.\n","   - `FRENCH_SEQUENCE_LENGTH`: Length of the French input sequence.\n","\n","8. **Transformer Decoder Layers**:\n","   - Apply multiple layers of `TransformerDecoder` to the embedded decoder inputs (`x`).\n","   - Each layer performs self-attention and cross-attention with the encoder outputs (`encoder_outputs`).\n","   - `EMBEDDING_DIM`: Dimensionality of the embeddings.\n","   - `D_FF`: Dimensionality of the feed-forward layer.\n","   - `NUM_HEADS`: Number of attention heads.\n","\n","9. **Dropout and Dense Output Layer (Decoder)**:\n","   - Apply dropout (`tf.keras.layers.Dropout(0.5)`) to prevent overfitting.\n","   - Project the decoder outputs to the vocabulary size using `Dense(VOCAB_SIZE, activation=\"softmax\")` for probability distribution over vocabulary words.\n","\n","10. **Define Transformer Model**:\n","    - Create a Keras `Model` with `encoder_inputs` and `decoder_inputs` as inputs and `decoder_outputs` as the output.\n","    - Name the model as \"transformer\".\n","\n","11. **Print Model Summary**:\n","    - Use `transformer.summary()` to display the architecture of the Transformer model, including layer types, output shapes, and trainable parameters.\n","\n","### Example Usage:\n","```python\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Dropout\n","from YourCustomModules import Embeddings, TransformerEncoder, TransformerDecoder\n","\n","# Constants\n","ENGLISH_SEQUENCE_LENGTH = 100\n","FRENCH_SEQUENCE_LENGTH = 120\n","VOCAB_SIZE = 10000\n","EMBEDDING_DIM = 512\n","D_FF = 2048\n","NUM_HEADS = 8\n","NUM_LAYERS = 6\n","\n","# Encoder Inputs\n","encoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"input_1\")\n","emb = Embeddings(ENGLISH_SEQUENCE_LENGTH, VOCAB_SIZE, EMBEDDING_DIM)\n","x = emb(encoder_inputs)\n","enc_mask = emb.compute_mask(encoder_inputs)\n","\n","# Transformer Encoder Layers\n","for _ in range(NUM_LAYERS):\n","    x = TransformerEncoder(EMBEDDING_DIM, D_FF, NUM_HEADS)(x)\n","encoder_outputs = x\n","\n","# Decoder Inputs\n","decoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"input_2\")\n","x = Embeddings(FRENCH_SEQUENCE_LENGTH, VOCAB_SIZE, EMBEDDING_DIM)(decoder_inputs)\n","\n","# Transformer Decoder Layers\n","for _ in range(NUM_LAYERS):\n","    x = TransformerDecoder(EMBEDDING_DIM, D_FF, NUM_HEADS)(x, encoder_outputs, enc_mask)\n","x = Dropout(0.5)(x)\n","decoder_outputs = Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n","\n","# Define Transformer Model\n","transformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n","transformer.summary()\n","```\n"],"metadata":{"id":"MGV-rQvLiBEh"}},{"cell_type":"markdown","source":["#Training"],"metadata":{"id":"HMuS_9BaD9E3"}},{"cell_type":"code","source":["class BLEU(tf.keras.metrics.Metric):\n","    def __init__(self,name='bleu_score'):\n","        super(BLEU,self).__init__()\n","        self.bleu_score=0\n","\n","    def update_state(self,y_true,y_pred,sample_weight=None):\n","      y_pred=tf.argmax(y_pred,-1)\n","      self.bleu_score=0\n","      for i,j in zip(y_pred,y_true):\n","        tf.autograph.experimental.set_loop_options()\n","\n","        total_words=tf.math.count_nonzero(i)\n","        total_matches=0\n","        for word in i:\n","          if word==0:\n","            break\n","          for q in range(len(j)):\n","            if j[q]==0:\n","              break\n","            if word==j[q]:\n","              total_matches+=1\n","              j=tf.boolean_mask(j,[False if y==q else True for y in range(len(j))])\n","              break\n","\n","        self.bleu_score+=total_matches/total_words\n","\n","    def result(self):\n","        return self.bleu_score/BATCH_SIZE"],"metadata":{"id":"bSISs_DF9gE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Custom implementation of a BLEU metric in TensorFlow/Keras. The BLEU (Bilingual Evaluation Understudy) score is commonly used to evaluate the quality of machine translation or text generation models.\n","\n","### Issues with Current Implementation:\n","\n","1. **Initialization of `bleu_score` in `__init__`**:\n","   - The `bleu_score` variable is initialized to `0` in the constructor (`__init__`). However, BLEU should accumulate scores across batches and then compute the final score in `result()`. Initializing it to `0` in `__init__` will reset the score every time a new instance of the metric is created.\n","\n","2. **Update State Function (`update_state`)**:\n","   - The `update_state` function is responsible for computing BLEU scores based on `y_true` (ground truth) and `y_pred` (predicted) sequences.\n","   - You are currently trying to compute BLEU scores by comparing each prediction (`y_pred`) with its corresponding ground truth (`y_true`). However, the logic for calculating BLEU scores and matching n-grams is incomplete and needs improvement.\n","\n","3. **Result Calculation (`result`)**:\n","   - The `result` function should return the computed BLEU score based on accumulated statistics from `update_state`. However, the current implementation does not correctly accumulate or average BLEU scores across batches.\n","\n","### Suggestions for Improvement:\n","\n","Here's an updated version of the `BLEU` metric class with improved logic for computing BLEU scores:\n","\n","```python\n","import tensorflow as tf\n","from collections import Counter\n","\n","class BLEU(tf.keras.metrics.Metric):\n","    def __init__(self, name='bleu_score'):\n","        super(BLEU, self).__init__(name=name)\n","        self.total_score = self.add_weight(name='total_score', initializer='zeros')\n","        self.total_samples = self.add_weight(name='total_samples', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        # Convert y_pred to argmax indices\n","        y_pred_indices = tf.argmax(y_pred, axis=-1)\n","\n","        batch_bleu = 0.0\n","        batch_size = tf.shape(y_true)[0]\n","\n","        for i in range(batch_size):\n","            reference = y_true[i, :]\n","            candidate = y_pred_indices[i, :]\n","\n","            # Calculate BLEU for individual example\n","            bleu = self.calculate_bleu(reference, candidate)\n","            batch_bleu += bleu\n","        \n","        # Update total score and total samples count\n","        self.total_score.assign_add(batch_bleu)\n","        self.total_samples.assign_add(tf.cast(batch_size, tf.float32))\n","\n","    def result(self):\n","        # Compute average BLEU score over all samples\n","        return self.total_score / self.total_samples\n","\n","    def calculate_bleu(self, reference, candidate):\n","        reference = tf.boolean_mask(reference, reference != 0)\n","        candidate = tf.boolean_mask(candidate, candidate != 0)\n","\n","        if len(candidate) == 0:\n","            return 0.0\n","\n","        clipped_counts = Counter(candidate) & Counter(reference)\n","        total_matches = sum(clipped_counts.values())\n","        total_words = len(candidate)\n","\n","        bleu_score = total_matches / total_words\n","        return bleu_score\n","```\n","\n","### Key Improvements and Changes:\n","\n","- **Initialization of Accumulators (`total_score`, `total_samples`)**:\n","  - Use `self.add_weight` to initialize accumulators for total BLEU score and total number of samples processed.\n","  \n","- **Improved `update_state` Function**:\n","  - Iterate over each example in the batch.\n","  - Calculate BLEU score for each example using `calculate_bleu`.\n","  - Accumulate batch BLEU scores into `total_score` and update `total_samples`.\n","\n","- **Correct Calculation of BLEU**:\n","  - The `calculate_bleu` function computes BLEU score based on matching n-grams between reference and candidate sequences.\n","  - Use `Counter` to efficiently count n-gram matches and compute precision for BLEU.\n","\n","- **Result Calculation (`result`)**:\n","  - Compute the average BLEU score across all samples processed (`total_score / total_samples`).\n","\n","### Usage Example:\n","```python\n","# Create an instance of the BLEU metric\n","bleu_metric = BLEU()\n","\n","# Compute BLEU scores during model training or evaluation\n","for (encoder_inputs, decoder_inputs), targets in dataset:\n","    predictions = model([encoder_inputs, decoder_inputs])\n","    bleu_metric.update_state(targets, predictions)\n","\n","# Get the final BLEU score\n","final_bleu_score = bleu_metric.result()\n","print(\"Final BLEU Score:\", final_bleu_score.numpy())\n","```"],"metadata":{"id":"1KDZU9UWjoGB"}},{"cell_type":"code","source":["class Scheduler(LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps):\n","    super(Scheduler, self).__init__()\n","    self.d_model = tf.cast(d_model, tf.float64)\n","    self.warmup_steps = tf.cast(warmup_steps, dtype=tf.float64)\n","\n","  def __call__(self, step):\n","    step = tf.cast(step, dtype=tf.float64)\n","    return (self.d_model**(-0.5))*tf.math.minimum(step**(-0.5), step * (self.warmup_steps ** -1.5))"],"metadata":{"id":"rh38U1Hy9gHk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Explanation of the Scheduler Class:\n","\n","1. **Initialization**:\n","   - The `Scheduler` class inherits from `LearningRateSchedule`.\n","   - `d_model`: The model's dimensionality, typically the size of the hidden layers in the Transformer.\n","   - `warmup_steps`: The number of warmup steps during which the learning rate increases linearly.\n","\n","2. **`__call__` Method**:\n","   - The `__call__` method is invoked to compute the learning rate at a given training step (`step`).\n","   - `step` is cast to `tf.float64` for precision in calculations.\n","   - The learning rate formula used here is derived from the Transformer's learning rate schedule:\n","     \\[\n","     \\text{lr} = \\frac{1}{\\text{d\\_model}^{\\frac{1}{2}}} \\cdot \\text{min}\\left(\\text{step}^{\\frac{-1}{2}}, \\text{step} \\cdot \\text{warmup\\_steps}^{\\frac{-1.5}{2}}\\right)\n","     \\]\n","   - This formula combines two components:\n","     - The `step^(-0.5)` decay term, which decays as the training progresses.\n","     - The `step * (warmup_steps^(-1.5))` warmup term, which increases the learning rate linearly during the warmup phase and then decays.\n","   - The `tf.math.minimum` function is used to select the smaller value between these two components.\n"],"metadata":{"id":"Lbuvdk4qklRQ"}},{"cell_type":"code","source":["WARM_UP_STEPS = 4000\n","lr_scheduled = Scheduler(EMBEDDING_DIM, WARM_UP_STEPS)"],"metadata":{"id":"qzDOe8K79gKQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformer.compile(\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","    optimizer = Adam(lr_scheduled, beta_1=0.9, beta_2=0.98, epsilon=1e-9),)\n","    #metrics=[BLEU()],\n","    #run_eagerly=True)"],"metadata":{"id":"PQyfhm_q9gM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history=transformer.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=10)"],"metadata":{"id":"PtJBt94z9gPi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformer.save_weights('/content/drive/MyDrive/NLP Repository/Projects/Neural_machine_translation/transformers.h5')"],"metadata":{"id":"9Qqf-3Rg9gSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model_loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"mhOSSI16EMok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformer.evaluate(val_dataset)"],"metadata":{"id":"cIYCuqZOEMri"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Testing"],"metadata":{"id":"G7CLQgYFEawC"}},{"cell_type":"code","source":["index_to_word={x:y for x, y in zip(range(len(french_vectorize_layer.get_vocabulary())),\n","                                   french_vectorize_layer.get_vocabulary())}"],"metadata":{"id":"wWRgxWf8EMu7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code constructs a dictionary `index_to_word` that maps indices to corresponding words in a vocabulary. This type of dictionary is commonly used in natural language processing tasks, such as machine translation, where you need to map token indices back to their original words for human-readable output or evaluation.\n","\n","### Explanation of the Code:\n","\n","1. **`french_vectorize_layer`**:\n","   - `french_vectorize_layer` appears to be a text vectorization layer used to convert text data into token indices.\n","   - It likely contains a vocabulary of words based on the text data it was trained on.\n","\n","2. **`get_vocabulary()` Method**:\n","   - The `get_vocabulary()` method retrieves the vocabulary list from the `french_vectorize_layer`.\n","   - This vocabulary list contains words sorted by frequency (most frequent to least frequent) based on the training data used to build the vectorization layer.\n","\n","3. **Dictionary Comprehension** (`{x: y for x, y in ...}`):\n","   - The dictionary comprehension iterates over pairs of `index` (from `range(len(vocabulary))`) and `word` (from `vocabulary`).\n","   - `range(len(vocabulary))` generates indices corresponding to the vocabulary items.\n","   - `french_vectorize_layer.get_vocabulary()` returns the list of words in the vocabulary.\n","\n","4. **`index_to_word` Dictionary**:\n","   - The resulting `index_to_word` dictionary maps each index (`x`) to its corresponding word (`y`) in the vocabulary.\n","   - This dictionary allows you to easily look up the original word given a token index.\n","\n","### Example Usage:\n","After constructing `index_to_word`, you can use it to decode token sequences back into human-readable text. Here's an example of how you might use `index_to_word`:\n","\n","```python\n","# Assuming 'index_to_word' has been constructed as described...\n","\n","# Example token sequence (list of integers representing tokens)\n","token_sequence = [1, 4, 7, 0, 2, 3]\n","\n","# Convert token sequence to human-readable text using 'index_to_word'\n","decoded_text = ' '.join(index_to_word[token] for token in token_sequence if token in index_to_word)\n","\n","print(\"Decoded Text:\", decoded_text)\n","```\n","\n","In this example:\n","- We assume `index_to_word` is already populated with mappings from token indices to words.\n","- We have a `token_sequence` (list of token indices).\n","- We use a list comprehension with `join` to map each token index in `token_sequence` to its corresponding word using `index_to_word`.\n","- The resulting `decoded_text` represents the original human-readable text decoded from the token sequence."],"metadata":{"id":"J7wGqcJhlDKB"}},{"cell_type":"code","source":["def translator(english_sentence):\n","  tokenized_english_sentence=english_vectorize_layer([english_sentence])\n","  shifted_target='starttoken'\n","\n","  for i in range(FRENCH_SEQUENCE_LENGTH):\n","    tokenized_shifted_target=french_vectorize_layer([shifted_target])\n","    output=transformer.predict([tokenized_english_sentence,tokenized_shifted_target])\n","    french_word_index=tf.argmax(output,axis=-1)[0][i].numpy()\n","    current_word=index_to_word[french_word_index]\n","    if current_word=='endtoken':\n","      break\n","    shifted_target+=' '+current_word\n","  return shifted_target[11:]"],"metadata":{"id":"CUgHG-7dEMyV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Implementing a translation function `translator` using a pre-trained Transformer model (`transformer`). This function takes an English sentence, tokenizes it using an `english_vectorize_layer`, and generates a corresponding French translation by iteratively predicting each word using the Transformer model.\n","\n","### Code Explanation:\n","\n","1. **Tokenization of Input Sentence**:\n","   - `tokenized_english_sentence`: Tokenizes the input `english_sentence` using `english_vectorize_layer`, which converts the text into a sequence of token indices suitable for model input.\n","\n","2. **Initialization of `shifted_target`**:\n","   - `shifted_target`: Starts with the token `'starttoken'` to indicate the beginning of the target (French) sentence generation.\n","\n","3. **Translation Loop** (`for i in range(FRENCH_SEQUENCE_LENGTH)`):\n","   - Iterates over a fixed number of steps (`FRENCH_SEQUENCE_LENGTH`) to generate the translated sentence.\n","   - For each iteration:\n","     - Tokenizes the `shifted_target` to obtain `tokenized_shifted_target`.\n","     - Uses the pre-trained Transformer model (`transformer.predict`) to predict the next word in the French translation based on the tokenized English sentence (`tokenized_english_sentence`) and the current tokenized French sequence (`tokenized_shifted_target`).\n","     - Selects the most probable word (by taking the `argmax` of the output probabilities) and retrieves its corresponding word using `index_to_word`.\n","     - Appends the predicted word to `shifted_target` to form the next input for the next iteration.\n","     - Terminates the loop if the predicted word is `'endtoken'`, indicating the end of the translation.\n","\n","4. **Final Translation**:\n","   - Returns the translated French sentence by removing the initial `'starttoken'` prefix (`shifted_target[11:]`).\n","\n","### Example Usage:\n","To use the `translator` function for translating English sentences to French using your pre-trained Transformer model (`transformer`), you would call the function with an English sentence as input. Here's an example:\n","\n","```python\n","english_sentence = \"How are you?\"\n","translated_french_sentence = translator(english_sentence)\n","print(\"Translated French Sentence:\", translated_french_sentence)\n","```"],"metadata":{"id":"8rh-78VBlzFA"}},{"cell_type":"code","source":["translator('What makes you think that it is not true?')"],"metadata":{"id":"1TF4eOZNEM9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator('Have you ever watched soccer under the rain?')"],"metadata":{"id":"XSbL6YskENBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator(\"what is your name?\")"],"metadata":{"id":"OqULI5mlENEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator('Great trees do not grow with ease, the stronger the winds, the stronger the trees')"],"metadata":{"id":"mJLJc_eHENKj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator('My hotel told me to call you. ')"],"metadata":{"id":"vXIcoC_NENOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator('His French is improving little by little')"],"metadata":{"id":"0fBT5UuV9gWE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator('I love to write')"],"metadata":{"id":"XdtdC64xEvgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator('Perhaps she will come tomorrow')"],"metadata":{"id":"ERKFOd1SEvjQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator('Tom has never heard Mary sing.')"],"metadata":{"id":"wip0iDTKEvmq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator('She handed him the money')"],"metadata":{"id":"gZYqtthkEv65"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Visualization"],"metadata":{"id":"6dy40bsuE60H"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"G0ZUpdafE5-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize(english_sentence):\n","  tokenized_english_sentence=english_vectorize_layer([english_sentence])\n","  shifted_target='starttoken je lai fait très bien'\n","\n","  tokenized_shifted_target=french_vectorize_layer([shifted_target])\n","  attention_weights=attention_score_model.predict([tokenized_english_sentence,\n","                                                   tokenized_shifted_target])\n","\n","  return attention_weights\n","\n","out=visualize('I did it very well')\n"],"metadata":{"id":"OQ5ExyzBE6B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(out['decoder_layer1_block2'][0].shape)"],"metadata":{"id":"QPyKTiV-E6E2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize = (12,12))\n","\n","for i in range(NUM_HEADS):\n","  ax = plt.subplot(2,4, i+1)\n","\n","  plt.imshow(out['decoder_layer1_block2'][0][i][0:10,0:10])\n","  plt.title(\"Attention Scores for head:->\"+str(i+1))"],"metadata":{"id":"l9kJl7pxE6H0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fzX2hBWSE6Kn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"F2LVcXV6E6Nu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rprevXFBE6Qz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HTKJPKiWE6Ua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lpDUPXqcE8fz"},"execution_count":null,"outputs":[]}]}