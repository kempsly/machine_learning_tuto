{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Ja7gtaVWwWz-","cGpukTkZ6f0Q","66r2dAkX7CoL","G3qy0mNf7MtP"],"toc_visible":true,"authorship_tag":"ABX9TyNBs6MvVHWpV4rVD5N0on6A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#IMport Statements"],"metadata":{"id":"9BwLgXAqvJnC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdolR2-mmdk_"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","from sklearn.metrics import confusion_matrix, roc_curve\n","import seaborn as sns\n","import datetime\n","import pathlib\n","import io\n","import os\n","import re\n","import string\n","import time\n","from numpy import random\n","import gensim.downloader as api\n","from PIL import Image\n","import tensorflow_datasets as tfds\n","import tensorflow_probability as tfp\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Conv1D,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input,GlobalMaxPooling1D,Embedding,TextVectorization,LayerNormalization,MultiHeadAttention)\n","from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n","from tensorflow.keras.optimizers import Adam\n","from google.colab import drive\n","from google.colab import files\n","from tensorboard.plugins import projector"]},{"cell_type":"code","source":["BATCH_SIZE=64"],"metadata":{"id":"URT2hcfvmi3O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Data Preparation"],"metadata":{"id":"0Swf-lZZvWv4"}},{"cell_type":"markdown","source":["##Load the data"],"metadata":{"id":"oC3rhx4kveOf"}},{"cell_type":"code","source":["train_ds,val_ds,test_ds=tfds.load('imdb_reviews', split=['train', 'test[:50%]', 'test[50%:]'],as_supervised=True)"],"metadata":{"id":"WR08iWUlmi54"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds"],"metadata":{"id":"d-yyeYesmi8l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for review,label in val_ds.take(2):\n","  print(review)\n","  print(label)"],"metadata":{"id":"w_qi4gsTmi_g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Process the Data"],"metadata":{"id":"FbnDz1q4vnkx"}},{"cell_type":"code","source":["def standardization(input_data):\n","    '''\n","    Input: raw reviews\n","    output: standardized reviews\n","    '''\n","    lowercase=tf.strings.lower(input_data)\n","    no_tag=tf.strings.regex_replace(lowercase,\"<[^>]+>\",\"\")\n","    output=tf.strings.regex_replace(no_tag,\"[%s]\"%re.escape(string.punctuation),\"\")\n","\n","    return output"],"metadata":{"id":"k_MAkTlAmjCN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["standardization(tf.constant(\"<u>In the movie?, </u>man called Tévèz, went to a friend’s pl**ce and they had a tensed discussion. I don’t love this movie! would you?<br> <br /><br />T\"))"],"metadata":{"id":"MpWPcvchmjEw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["VOCAB_SIZE=10000\n","SEQUENCE_LENGTH=250\n","EMBEDDING_DIM=300"],"metadata":{"id":"UrP74Y2omjHc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorize_layer=TextVectorization(\n","    standardize=standardization,\n","    max_tokens=VOCAB_SIZE,\n","    output_mode='int',\n","    output_sequence_length=SEQUENCE_LENGTH\n",")"],"metadata":{"id":"L3MQlW9ymjKB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data=train_ds.map(lambda x,y:x)\n","vectorize_layer.adapt(training_data)"],"metadata":{"id":"AZo2qZ11mjMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(vectorize_layer.get_vocabulary())"],"metadata":{"id":"ofhQ4TX9mjPS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def vectorizer(review,label):\n","    return vectorize_layer(review),label"],"metadata":{"id":"D3xAs_QMmjR2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset=train_ds.map(vectorizer)\n","val_dataset=val_ds.map(vectorizer)"],"metadata":{"id":"wL6IjVIxmjU3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorize_layer.get_vocabulary()[411]"],"metadata":{"id":"QVGSn6BcmjXz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for review,label in train_dataset.take(1):\n","  print(review)\n","  print(label)"],"metadata":{"id":"IIQJnXJ3mjaP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset=train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n","val_dataset=val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"],"metadata":{"id":"zoJl3e7RmjdJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Modeling"],"metadata":{"id":"QbTcMtezwOSD"}},{"cell_type":"markdown","source":["##Transformers"],"metadata":{"id":"dQZ-ySMRwQVn"}},{"cell_type":"markdown","source":["###Embeddings"],"metadata":{"id":"Ja7gtaVWwWz-"}},{"cell_type":"code","source":["def positional_encoding(model_size,SEQUENCE_LENGTH):\n","  output=[]\n","  for pos in range(SEQUENCE_LENGTH):\n","    PE=np.zeros((model_size))\n","    for i in range(model_size):\n","      if i%2==0:\n","        PE[i]=np.sin(pos/(10000**(i/model_size)))\n","      else:\n","        PE[i]=np.cos(pos/(10000**((i-1)/model_size)))\n","    output.append(tf.expand_dims(PE,axis=0))\n","  out=tf.concat(output,axis=0)\n","  out=tf.expand_dims(out,axis=0)\n","  return tf.cast(out,dtype=tf.float32)"],"metadata":{"id":"c2ObXsj-mjf_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `positional_encoding` function provided is used to generate positional encodings for Transformer models. Positional encodings are added to the input embeddings to provide positional information to the model, enabling it to understand the order or sequence of tokens within an input sequence.\n","\n","### Function Explanation:\n","\n","1. **Inputs**:\n","   - `model_size`: The dimensionality of the input embeddings or the hidden size of the Transformer model.\n","   - `SEQUENCE_LENGTH`: The maximum sequence length for which positional encodings are generated.\n","\n","2. **Initialization**:\n","   - Initialize an empty list `output` to collect positional encodings for each position in the sequence.\n","\n","3. **Positional Encoding Calculation**:\n","   - Iterate over each position `pos` in the sequence range `[0, SEQUENCE_LENGTH)`:\n","     - Create a numpy array `PE` (Positional Encoding) initialized with zeros and of size `model_size`.\n","     - Iterate over each dimension `i` in the `model_size`:\n","       - If `i` is even (`i % 2 == 0`), compute the sine-based positional encoding using the formula:\n","         \\[\n","         \\text{PE}[i] = \\sin\\left(\\frac{\\text{pos}}{10000^{i / \\text{model\\_size}}}\\right)\n","         \\]\n","       - If `i` is odd (`i % 2 != 0`), compute the cosine-based positional encoding using the formula:\n","         \\[\n","         \\text{PE}[i] = \\cos\\left(\\frac{\\text{pos}}{10000^{(i-1) / \\text{model\\_size}}}\\right)\n","         \\]\n","     - Append the positional encoding `PE` as a new axis (`tf.expand_dims(PE, axis=0)`) to the `output` list.\n","\n","4. **Concatenation and Reshaping**:\n","   - Concatenate all positional encodings in the `output` list along the `axis=0` (sequence axis) to create a tensor `out`.\n","   - Expand the dimensions of `out` to include a batch dimension (`tf.expand_dims(out, axis=0)`).\n","\n","5. **Data Type Casting**:\n","   - Cast the resulting tensor `out` to `tf.float32` data type using `tf.cast`.\n","\n","6. **Return**:\n","   - Return the final positional encoding tensor `out`.\n","\n","### Key Points:\n","\n","- **Positional Encoding Formula**:\n","  - The positional encoding formula incorporates sine and cosine functions with varying frequencies based on the position (`pos`) and dimension (`i`) within the embedding space.\n","  - Using sine and cosine functions ensures that the positional encodings have unique patterns that can convey positional information to the model.\n","\n","- **Dimensional Interpretation**:\n","  - Each dimension (`i`) of the positional encoding tensor corresponds to a different frequency pattern, enabling the model to differentiate between positions effectively.\n","\n","- **Sequence Length Consideration**:\n","  - The positional encoding tensor generated (`out`) will have a shape of `(1, SEQUENCE_LENGTH, model_size)`, suitable for adding to input embeddings of sequences with maximum length `SEQUENCE_LENGTH`.\n","\n"],"metadata":{"id":"CGNZRHAjwwgw"}},{"cell_type":"code","source":["class Embeddings(Layer):\n","  def __init__(self, sequence_length, vocab_size, embed_dim,):\n","    super(Embeddings, self).__init__()\n","    self.token_embeddings=Embedding(\n","        input_dim=vocab_size, output_dim=embed_dim)\n","    self.sequence_length = sequence_length\n","    self.vocab_size = vocab_size\n","    self.embed_dim = embed_dim\n","\n","  def call(self, inputs):\n","    embedded_tokens = self.token_embeddings(inputs)\n","    embedded_positions=positional_encoding(\n","        self.embed_dim,self.sequence_length)\n","    return embedded_tokens + embedded_positions\n","\n","  def compute_mask(self, inputs, mask=None):\n","    return tf.math.not_equal(inputs, 0)\n","\n","  def get_config(self):\n","      config = super().get_config()\n","      config.update({\n","        \"sequence_length\": self.sequence_length,\n","        \"vocab_size\": self.vocab_size,\n","        \"embed_dim\": self.embed_dim,\n","      })\n","      return config\n"],"metadata":{"id":"A0o7jtC7mjiz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `Embeddings` class we provided is a custom layer in TensorFlow/Keras that combines token embeddings with positional encodings, typically used in Transformer architectures for natural language processing tasks. Here's a breakdown of how this layer works:\n","\n","### Class Initialization (`__init__`):\n","- **Parameters**:\n","  - `sequence_length`: Maximum length of input sequences.\n","  - `vocab_size`: Size of the vocabulary (number of unique tokens).\n","  - `embed_dim`: Dimensionality of the token embeddings.\n","\n","- **Attributes**:\n","  - `token_embeddings`: An `Embedding` layer that maps token indices to dense embeddings of size `(vocab_size, embed_dim)`.\n","  - `sequence_length`, `vocab_size`, `embed_dim`: Store input parameters as attributes of the layer.\n","\n","### `call` Method:\n","- **Input**:\n","  - `inputs`: Tensor representing input sequences (token indices).\n","\n","- **Output**:\n","  - Computes token embeddings (`embedded_tokens`) using the `Embedding` layer (`token_embeddings`).\n","  - Generates positional encodings (`embedded_positions`) using a custom `positional_encoding` function.\n","  - Returns the sum of token embeddings and positional encodings as the output.\n","\n","### `compute_mask` Method:\n","- **Input**:\n","  - `inputs`: Tensor representing input sequences.\n","\n","- **Output**:\n","  - Computes a mask that marks non-padding tokens (tokens not equal to 0).\n","\n","### `get_config` Method:\n","- **Output**:\n","  - Generates a dictionary (`config`) containing the layer's configuration, including `sequence_length`, `vocab_size`, and `embed_dim`.\n","  - Updates the base configuration using `super().get_config()` and adds custom attributes.\n","\n","### Usage Example:\n","You can use the `Embeddings` layer within a Transformer model or any sequence processing model in TensorFlow/Keras. Here's how you might use it:\n","\n","```python\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","\n","# Define input sequence tensor\n","input_sequence = Input(shape=(sequence_length,), dtype='int32', name='input_sequence')\n","\n","# Create an instance of the Embeddings layer\n","embeddings_layer = Embeddings(sequence_length=sequence_length, vocab_size=vocab_size, embed_dim=embed_dim)\n","\n","# Apply the embeddings layer to the input sequence\n","embedded_output = embeddings_layer(input_sequence)\n","\n","# Example: Add additional layers to the model\n","dense_layer = Dense(units=128, activation='relu')(embedded_output)\n","output_layer = Dense(units=num_classes, activation='softmax')(dense_layer)\n","\n","# Create the model\n","model = Model(inputs=input_sequence, outputs=output_layer)\n","\n","# Compile the model with appropriate loss and optimizer\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Display model summary\n","model.summary()\n","```\n","\n","In this example:\n","- `input_sequence` represents the input tensor with shape `(None, sequence_length)`, where `None` indicates a variable batch size.\n","- The `Embeddings` layer (`embeddings_layer`) is initialized with specified parameters (`sequence_length`, `vocab_size`, `embed_dim`).\n","- The `embedded_output` is obtained by applying the `embeddings_layer` to `input_sequence`.\n","- Additional layers (e.g., `Dense`) can be added on top of the embeddings for further processing.\n","- Finally, a full model (`model`) is created, compiled, and ready for training or inference."],"metadata":{"id":"zlpHtM831UKy"}},{"cell_type":"code","source":["test_input=tf.constant([[  2, 112,   10,   12,  5,   0,   0,   0,]])\n","\n","emb=Embeddings(8,20000,256)\n","emb_out=emb(test_input)\n","print(emb_out.shape)"],"metadata":{"id":"SO1If0vdmjlt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","### Given Input and Embeddings Layer Configuration:\n","\n","- **Input**:\n","  ```python\n","  test_input = tf.constant([[2, 112, 10, 12, 5, 0, 0, 0]])\n","  ```\n","  This input represents a batch of sequences with a single sequence `[2, 112, 10, 12, 5, 0, 0, 0]`.\n","\n","- **Embeddings Layer Configuration** (`Embeddings(8, 20000, 256)`):\n","  - `sequence_length`: `8`\n","  - `vocab_size`: `20000`\n","  - `embed_dim`: `256`\n","\n","### Steps to Compute `emb_out`:\n","\n","1. **Token Embeddings** (`embedded_tokens`):\n","   - The `Embeddings` layer (`emb`) first computes token embeddings (`embedded_tokens`) for the input sequence using the `token_embeddings` layer (an instance of `tf.keras.layers.Embedding`).\n","   - Each token in the input sequence is replaced with its corresponding embedding vector.\n","   - The shape of `embedded_tokens` will be `(1, sequence_length, embed_dim)`, where `1` represents the batch size (single sequence), `sequence_length` is `8`, and `embed_dim` is `256`.\n","\n","2. **Positional Encodings** (`embedded_positions`):\n","   - The `Embeddings` layer also generates positional encodings (`embedded_positions`) using a function like `positional_encoding(embed_dim, sequence_length)`.\n","   - The shape of `embedded_positions` will be `(1, sequence_length, embed_dim)`, matching the shape of `embedded_tokens`.\n","\n","3. **Combining Embeddings and Positions** (`emb_out`):\n","   - The final output (`emb_out`) is obtained by adding the token embeddings (`embedded_tokens`) and the positional encodings (`embedded_positions`) element-wise.\n","   - This operation is performed using TensorFlow's broadcasting rules, where each element in `embedded_tokens` is added to its corresponding element in `embedded_positions`.\n","   - The resulting shape of `emb_out` will also be `(1, sequence_length, embed_dim)`.\n","\n","### Output Shape of `emb_out`:\n","- After combining token embeddings and positional encodings, the shape of `emb_out` will be `(1, 8, 256)`.\n","```\n"],"metadata":{"id":"W-fKRR_F27J4"}},{"cell_type":"markdown","source":["###Encoder"],"metadata":{"id":"vrdXK0CV3rgW"}},{"cell_type":"code","source":["class TransformerEncoder(Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads,):\n","        super(TransformerEncoder, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim,\n","        )\n","        self.dense_proj=tf.keras.Sequential(\n","            [Dense(dense_dim, activation=\"relu\"),Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = LayerNormalization()\n","        self.layernorm_2 = LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","      if mask is not None:\n","        mask1 = mask[:, :, tf.newaxis]\n","        mask2 = mask[:,tf.newaxis, :]\n","        padding_mask = tf.cast(mask1&mask2, dtype=\"int32\")\n","\n","      attention_output = self.attention(\n","          query=inputs, key=inputs,value=inputs,attention_mask=padding_mask\n","      )\n","\n","      proj_input = self.layernorm_1(inputs + attention_output)\n","      proj_output = self.dense_proj(proj_input)\n","      return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):\n","      config = super().get_config()\n","      config.update({\n","        \"embed_dim\": self.embed_dim,\n","        \"num_heads\": self.num_heads,\n","        \"dense_dim\": self.dense_dim,\n","      })\n","      return config"],"metadata":{"id":"vpXZIq-Wmjoi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `TransformerEncoder` class defined is a custom layer representing the encoder block in a Transformer model. Let's break down the key components and functionality of this layer:\n","\n","### Class Initialization (`__init__`):\n","- **Parameters**:\n","  - `embed_dim`: Dimensionality of the input embeddings.\n","  - `dense_dim`: Dimensionality of the intermediate dense layer within the feed-forward network.\n","  - `num_heads`: Number of attention heads in the multi-head attention mechanism.\n","\n","- **Attributes**:\n","  - `embed_dim`, `dense_dim`, `num_heads`: Store input parameters as attributes of the layer.\n","  - `attention`: Instance of `MultiHeadAttention` layer, configured with the specified number of heads (`num_heads`) and key dimension (`embed_dim`).\n","  - `dense_proj`: Sequential model consisting of two `Dense` layers: the first with `dense_dim` units and ReLU activation, and the second with `embed_dim` units (to match the input embedding dimension).\n","  - `layernorm_1`, `layernorm_2`: Layer normalization layers applied before and after the feed-forward network.\n","  - `supports_masking`: Indicates that the layer supports masking of input sequences.\n","\n","### `call` Method:\n","- **Inputs**:\n","  - `inputs`: Input tensor representing a sequence of embeddings (shape: `[batch_size, sequence_length, embed_dim]`).\n","  - `mask`: Optional input mask tensor to handle padding masks (shape: `[batch_size, sequence_length]`).\n","\n","- **Masking**:\n","  - If a `mask` tensor is provided (`mask is not None`), it is used to create a 2D padding mask (`padding_mask`) that can be applied during the attention computation.\n","  - The padding mask (`padding_mask`) is used to mask out padded elements in the input sequences during attention computation.\n","\n","- **Attention Computation**:\n","  - The input `inputs` tensor is passed through the `attention` layer (multi-head attention mechanism), where the same input is used for `query`, `key`, and `value`.\n","  - The `attention_mask` (if provided) masks out specific elements in the attention computation based on the `padding_mask`.\n","\n","- **Layer Normalization and Feed-Forward Network**:\n","  - The output of the attention mechanism (`attention_output`) is added to the input (`inputs`) and normalized using `layernorm_1`.\n","  - The result (`proj_input`) is passed through the feed-forward network (`dense_proj`), consisting of two `Dense` layers.\n","  - The output of the feed-forward network (`proj_output`) is added back to `proj_input` and normalized using `layernorm_2`.\n","\n","- **Output**:\n","  - Returns the final output tensor (`proj_input + proj_output`) after layer normalization.\n","\n","### `get_config` Method:\n","- **Output**:\n","  - Generates a dictionary (`config`) containing the layer's configuration, including `embed_dim`, `num_heads`, and `dense_dim`.\n","  - Updates the base configuration using `super().get_config()` and adds custom attributes."],"metadata":{"id":"cAg5hnaS4Uhp"}},{"cell_type":"code","source":["encoder_outputs = TransformerEncoder(256,2048,2)(emb_out)\n","print(encoder_outputs.shape)\n"],"metadata":{"id":"OueZnerbmjrj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Given Configuration:\n","\n","- **Input Shape** (`emb_out`):\n","  - `emb_out` has a shape of `(batch_size, sequence_length, embed_dim)`, where:\n","    - `batch_size` is inferred from the input data.\n","    - `sequence_length` is the length of the input sequence.\n","    - `embed_dim` is the dimensionality of the input embeddings.\n","\n","- **TransformerEncoder Configuration**:\n","  - `embed_dim`: `256`\n","  - `dense_dim`: `2048`\n","  - `num_heads`: `2`\n","\n","### Steps to Compute `encoder_outputs`:\n","\n","1. **Instantiation of `TransformerEncoder`**:\n","   - Create an instance of `TransformerEncoder` with the specified parameters (`embed_dim=256`, `dense_dim=2048`, `num_heads=2`).\n","\n","2. **Applying `TransformerEncoder` to `emb_out`**:\n","   - Pass the input tensor `emb_out` through the `TransformerEncoder` layer.\n","   - The `TransformerEncoder` layer will perform the following operations:\n","     - Apply multi-head self-attention using the `MultiHeadAttention` mechanism with `num_heads=2` and `key_dim=256`.\n","     - Normalize the attention output using layer normalization (`layernorm_1`).\n","     - Process the normalized output through a feed-forward neural network (`dense_proj`) with a hidden dimension of `2048`.\n","     - Apply another layer normalization (`layernorm_2`) to the output of the feed-forward network.\n","   \n","3. **Output Shape** (`encoder_outputs`):\n","   - The shape of `encoder_outputs` will match the shape of the input `emb_out`, which is `(batch_size, sequence_length, embed_dim)`.\n","   - Therefore, `encoder_outputs.shape` will be `(batch_size, sequence_length, embed_dim)`.\n"],"metadata":{"id":"OJPuMiJZ4-Q4"}},{"cell_type":"markdown","source":["###Transformer Model"],"metadata":{"id":"HFVDkQGr5bu_"}},{"cell_type":"code","source":["EMBEDDING_DIM=128\n","D_FF=1024\n","NUM_HEADS=8\n","NUM_LAYERS=1\n","NUM_EPOCHS=20"],"metadata":{"id":"f9-g8NcMmjuQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","- **EMBEDDING_DIM**: This parameter defines the dimensionality of the token embeddings. In Transformer models, each token in the input sequence is initially represented as a dense vector of `EMBEDDING_DIM` dimensions.\n","\n","- **D_FF (Feed-Forward Dimension)**: This parameter specifies the dimensionality of the intermediate layer in the feed-forward network used within each Transformer block. Typically, this intermediate dimension is larger than the `EMBEDDING_DIM` to allow for more complex transformations.\n","\n","- **NUM_HEADS**: The number of attention heads used in the multi-head attention mechanism. Each attention head allows the model to focus on different parts of the input sequence, enhancing its ability to capture dependencies and relationships within the data.\n","\n","- **NUM_LAYERS**: The number of stacked Transformer encoder or decoder layers in the model. Increasing the number of layers can improve the model's capacity to learn complex patterns but may also increase computational cost and risk overfitting.\n","\n","- **NUM_EPOCHS**: The number of epochs (full passes through the training data) used during model training. Each epoch involves one forward pass (computing predictions), calculating the loss, and updating the model's parameters (weights) through backpropagation.\n","    \n","   \n","\n"],"metadata":{"id":"FPVxplqM5r3W"}},{"cell_type":"code","source":["encoder_input=Input(shape=(None,), dtype=\"int64\", name=\"input\")\n","x = Embeddings(SEQUENCE_LENGTH,VOCAB_SIZE,EMBEDDING_DIM)(encoder_input)\n","\n","for _ in range(NUM_LAYERS):\n","  x=TransformerEncoder(EMBEDDING_DIM,D_FF,NUM_HEADS)(x)\n","\n","x = Flatten()(x)\n","output=Dense(1, activation=\"sigmoid\")(x)\n","\n","transformer = tf.keras.Model(\n","    encoder_input, output, name=\"transformer\"\n",")\n","transformer.summary()"],"metadata":{"id":"3PEF0C-TmjxC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Components of the Model:\n","\n","1. **Input Layer** (`encoder_input`):\n","   - This layer defines the input shape of your model, where sequences of integers (token indices) are expected. The shape is `(batch_size, sequence_length)`.\n","\n","2. **Embeddings Layer** (`Embeddings`):\n","   - This layer converts input token indices into dense vectors (`EMBEDDING_DIM` dimensions) using trainable embedding weights.\n","   - Parameters:\n","     - `SEQUENCE_LENGTH`: Length of input sequences.\n","     - `VOCAB_SIZE`: Size of the vocabulary (total number of unique tokens).\n","     - `EMBEDDING_DIM`: Dimensionality of the token embeddings.\n","\n","3. **TransformerEncoder Blocks** (`TransformerEncoder`):\n","   - A loop (`for _ in range(NUM_LAYERS)`) that stacks `NUM_LAYERS` of `TransformerEncoder` layers.\n","   - Each `TransformerEncoder` layer processes the input sequence using self-attention and feed-forward networks, capturing dependencies within the sequence.\n","   - Parameters:\n","     - `EMBEDDING_DIM`: Dimensionality of the input embeddings.\n","     - `D_FF`: Dimensionality of the intermediate dense layer in the feed-forward network within each Transformer block.\n","     - `NUM_HEADS`: Number of attention heads in the multi-head attention mechanism.\n","\n","4. **Flatten Layer** (`Flatten`):\n","   - This layer reshapes the output tensor from the last `TransformerEncoder` block into a 1D tensor.\n","   - Necessary for connecting the output to a `Dense` layer for classification.\n","\n","5. **Output Layer** (`Dense` with `sigmoid` activation):\n","   - This `Dense` layer with a single unit and sigmoid activation function is commonly used for binary classification tasks.\n","   - It produces a scalar output (probability) representing the likelihood of the input sequence belonging to the positive class.\n","\n","6. **Model Compilation and Summary**:\n","   - The model is defined using `tf.keras.Model`, specifying the input (`encoder_input`) and output (`output`) layers.\n","   - The model is then compiled, typically with a suitable optimizer (e.g., Adam) and loss function (e.g., binary cross-entropy) for binary classification tasks.\n","   - Finally, the model summary is displayed, showing the architecture and parameter counts."],"metadata":{"id":"noYaDFT-6Ggi"}},{"cell_type":"markdown","source":["###LSH *Attention*"],"metadata":{"id":"z0rR36uV7Wcu"}},{"cell_type":"code","source":["def look_one_back(x):\n","  x_extra=tf.concat([x[:,-1:,...],x[:,:-1,...]],axis=1)\n","  return tf.concat([x,x_extra],axis=2)\n","\n","def sticker_look_one_back(x):\n","  x_extra=tf.concat([x[:-1:],x[:,:-1]],axis=1)\n","  return tf.concat([x,x_extra],axis=-1)\n","\n","def causal_masker(a,b):\n","  a,b=tf.cast(a,dtype=tf.float32)+0.01,tf.cast(b,dtype=tf.float32)+0.01\n","  vals=tf.einsum('ipj,ipk->ipjk',b,1/a)\n","  out=tf.cast(tf.cast(tf.cast(vals,dtype=tf.int32),dtype=tf.bool),dtype=tf.int32)\n","  out=-out+1\n","  return tf.cast(out,dtype=tf.float32)\n","\n","class LSHAttention(tf.keras.layers.Layer):\n","    def __init__(self,bucket_size=8,n_hashes=1):\n","        super(LSHAttention,self).__init__()\n","        self.n_hashes=n_hashes\n","        self.bucket_size=bucket_size\n","\n","    def call(self,query,key,value,causal_masking=False):\n","        R=tf.random.normal((tf.shape(query)[0],tf.shape(query)[-1],self.bucket_size//2))\n","        xR=tf.matmul(query,R)\n","        concat_xR=tf.concat([xR,-xR],axis=-1)\n","        buckets=tf.math.argmax(concat_xR,axis=-1)\n","\n","        sticker=tf.argsort(buckets)\n","        undo_sort=tf.argsort(sticker)\n","        sorted_query=tf.gather(query,sticker,axis=1,batch_dims=1)\n","        sorted_value=tf.gather(value,sticker,axis=1,batch_dims=1)\n","\n","        chunked_query=tf.stack(tf.split(sorted_query,self.bucket_size,1),1)\n","        chunked_value=tf.stack(tf.split(sorted_value,self.bucket_size,1),1)\n","\n","        sticker=tf.stack(tf.split(sticker,self.bucket_size,1),1)\n","        new_sticker=sticker_look_one_back(sticker)\n","\n","        lb_chunked_query=look_one_back(chunked_query)\n","        lb_chunked_value=look_one_back(chunked_value)\n","\n","        score=tf.einsum('bhie,bhje->bhij',chunked_query,lb_chunked_query)\n","        score/=tf.math.sqrt(tf.cast(query.shape[-1],tf.float32))\n","\n","        if causal_masking==True:\n","            causal_mask=causal_masker(sticker,new_sticker)\n","            dots+=causal_mask*-1e-10\n","        score=tf.nn.softmax(score)\n","        output=tf.einsum('buij,buje->buie',score,lb_chunked_value)\n","\n","        sorted_output=tf.reshape(output,(tf.shape(output)[0],tf.shape(query)[i],output.shape[3]))\n","        output=tf.gather(sorted_output,undo_sort,axis=1,batch_dims=1)\n","        return output"],"metadata":{"id":"M5VZ6Drj7mQ6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code defines a custom layer `LSHAttention` which implements Locality Sensitive Hashing (LSH) for attention computation. Let's break down the components and operations within this layer:\n","\n","### Components Explained:\n","\n","1. **Initialization**:\n","   - The `LSHAttention` layer is initialized with parameters `bucket_size` and `n_hashes`.\n","     - `bucket_size`: Number of buckets for hashing.\n","     - `n_hashes`: Number of hashing functions to use.\n","\n","2. **Random Projections and Hashing**:\n","   - Random projections (`R`) are generated to transform the `query` tensor (`query`) into a lower-dimensional space (`(batch_size, sequence_length, bucket_size/2)`).\n","   - The `query` tensor is multiplied with `R` and concatenated with its negative counterpart to create `concat_xR`.\n","   - Bucket assignments (`buckets`) are obtained by finding the index of the maximum value along the last dimension of `concat_xR`.\n","\n","3. **Sorting and Chunking**:\n","   - The bucket assignments (`buckets`) are sorted to rearrange the `query` and `value` tensors accordingly (`sorted_query` and `sorted_value`).\n","   - The sorted tensors are chunked into smaller segments (`chunked_query` and `chunked_value`) based on `bucket_size`.\n","\n","4. **Applying Look-One-Back**:\n","   - The `sticker_look_one_back` function is applied to reorder the bucket assignments (`sticker`) for causal masking purposes.\n","\n","5. **Calculating Scores and Softmax**:\n","   - Dot products (`score`) are computed between the chunked `query` tensors and their look-one-back counterparts.\n","   - Scores are divided by the square root of the query dimension for scaling (`tf.math.sqrt(tf.cast(query.shape[-1], tf.float32))`).\n","   - Softmax is applied along the last dimension of `score` to compute attention weights (`score=tf.nn.softmax(score)`).\n","\n","6. **Masking (Optional)**:\n","   - If `causal_masking` is enabled (`True`), a causal mask is computed using the `causal_masker` function and applied to the attention scores (`score += causal_mask * -1e-10`).\n","\n","7. **Weighted Sum**:\n","   - Weighted sum (`output`) is computed by performing matrix multiplication (`tf.einsum`) between the attention weights (`score`) and the chunked `value` tensors (`lb_chunked_value`).\n","\n","8. **Reordering and Output**:\n","   - Reordering and reshaping are applied to the `output` tensor to restore the original order of elements (`sorted_output`).\n","   - The sorted `output` tensor is restored to its original order using `gather` operations (`output=tf.gather(sorted_output, undo_sort, axis=1, batch_dims=1)`).\n","\n","### Usage Example:\n","\n","To use the `LSHAttention` layer in a Transformer model, you can instantiate an instance of `LSHAttention` and call it with appropriate inputs (`query`, `key`, `value`) along with optional `causal_masking`.\n","\n","```python\n","import tensorflow as tf\n","\n","# Define input tensors (example shapes)\n","batch_size = 32\n","sequence_length = 10\n","embedding_dim = 128\n","\n","query = tf.random.normal((batch_size, sequence_length, embedding_dim))\n","key = tf.random.normal((batch_size, sequence_length, embedding_dim))\n","value = tf.random.normal((batch_size, sequence_length, embedding_dim))\n","\n","# Instantiate LSHAttention layer\n","lsh_attention = LSHAttention(bucket_size=8, n_hashes=1)\n","\n","# Apply LSHAttention layer\n","attention_output = lsh_attention(query, key, value, causal_masking=True)\n","\n","# Display the shape of the output\n","print(attention_output.shape)\n","```\n","\n","In this example:\n","- We generate random input tensors (`query`, `key`, `value`) with specified shapes.\n","- We instantiate an `LSHAttention` layer (`lsh_attention`) with `bucket_size=8` and `n_hashes=1`.\n","- The `LSHAttention` layer is applied to the input tensors (`query`, `key`, `value`) with optional `causal_masking=True`.\n","- The resulting `attention_output` tensor represents the output of the attention mechanism applied by the `LSHAttention` layer."],"metadata":{"id":"7s25-axp8FG3"}},{"cell_type":"code","source":[],"metadata":{"id":"Ln_hP3NZ7mhD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Training"],"metadata":{"id":"cGpukTkZ6f0Q"}},{"cell_type":"code","source":["checkpoint_filepath = '/content/drive/MyDrive/NLP Repository/Projects/Sentiments_analysis/transformer.h5'\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)"],"metadata":{"id":"yN-Fu1LYmjzx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformer.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","              optimizer=tf.keras.optimizers.Adam(1e-4),\n","              metrics=['accuracy'])"],"metadata":{"id":"LgDSzq8amj2Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history=transformer.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=10,\n","    callbacks=[model_checkpoint_callback])"],"metadata":{"id":"4zgIVaQxmj5D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model_loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"BNg0_h3Kmj7j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","\n","plt.title('model_accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"hbG2w4x_mj-Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#EValuation"],"metadata":{"id":"66r2dAkX7CoL"}},{"cell_type":"code","source":["transformer.load_weights(checkpoint_filepath)"],"metadata":{"id":"E9HjL4TPmkBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset=test_ds.map(vectorizer)\n","test_dataset=test_dataset.batch(BATCH_SIZE)\n","transformer.evaluate(test_dataset)"],"metadata":{"id":"TPL5xMdKmkD8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Testing"],"metadata":{"id":"G3qy0mNf7MtP"}},{"cell_type":"code","source":["test_data=tf.data.Dataset.from_tensor_slices([[\"this movie looks very interesting, i love the fact that the actors do a great job in showing how people lived in the 18th century, which wasn't very good at all. But atleast this movie recreates this scenes! \"],\n","                                              [\"very good start, but movie started becoming uninteresting at some point though initially i thought it would have been much more fun. There was too much background noise, so in all i didn't like this movie \"],])\n"],"metadata":{"id":"alQAg1N9mkGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def vectorizer_test(review):\n","    return vectorize_layer(review)\n","test_dataset=test_data.map(vectorizer_test)"],"metadata":{"id":"sn4TMEoZmkJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformer.predict(test_dataset)"],"metadata":{"id":"zrUqYPBymkMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2-j89VBAmkPC"},"execution_count":null,"outputs":[]}]}