{"cells":[{"cell_type":"markdown","metadata":{"id":"yoBrzWs__NIU"},"source":["#Import Statements"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":23954,"status":"ok","timestamp":1713962429806,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"jQxD4o6e-_R8"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","from sklearn.metrics import confusion_matrix, roc_curve\n","import seaborn as sns\n","import datetime\n","import pathlib\n","import io\n","import os\n","import re\n","import string\n","import time\n","from numpy import random\n","import tensorflow_datasets as tfds\n","import tensorflow_probability as tfp\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Conv1D,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input, Embedding,TextVectorization)\n","from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n","from tensorflow.keras.optimizers import Adam\n","from google.colab import drive\n","from google.colab import files\n","from tensorboard.plugins import projector"]},{"cell_type":"markdown","metadata":{"id":"KgbVl-yj_j1F"},"source":["#Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"kjFD7D9Y_pSe"},"source":["##Download the data"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1264,"status":"ok","timestamp":1713962451094,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"CQEJVInL_Kfg","outputId":"092fc2cb-7d07-48ac-ba70-e95c973cd4ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-04-24 12:40:51--  https://www.manythings.org/anki/fra-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n","Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7943074 (7.6M) [application/zip]\n","Saving to: ‘fra-eng.zip’\n","\n","fra-eng.zip         100%[===================\u003e]   7.57M  14.8MB/s    in 0.5s    \n","\n","2024-04-24 12:40:52 (14.8 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n","\n"]}],"source":["!wget https://www.manythings.org/anki/fra-eng.zip"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678,"status":"ok","timestamp":1713962455510,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"vBRrBc5F_Kia","outputId":"ebe6074e-ccd8-4d06-9c29-033df2d83030"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  /content/fra-eng.zip\n","  inflating: /content/dataset/_about.txt  \n","  inflating: /content/dataset/fra.txt  \n"]}],"source":["!unzip \"/content/fra-eng.zip\" -d \"/content/dataset/\""]},{"cell_type":"markdown","metadata":{"id":"ipKLBlfsABi4"},"source":["##Dataset From Kaggle"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40164,"status":"ok","timestamp":1713962500175,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"OMqxNUEy_Kl1","outputId":"a8401e7b-4ee6-4072-b3a3-373b565a61ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading en-fr-translation-dataset.zip to /content\n","100% 2.54G/2.54G [00:29\u003c00:00, 129MB/s]\n","100% 2.54G/2.54G [00:29\u003c00:00, 91.2MB/s]\n"]}],"source":["!pip install -q kaggle\n","!mkdir ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 /root/.kaggle/kaggle.json\n","!kaggle datasets download -d dhruvildave/en-fr-translation-dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157728,"status":"ok","timestamp":1713962665620,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"Of7GTRpj1sN-","outputId":"351b8325-66b1-4bab-8916-0a54a8a56578"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  /content/en-fr-translation-dataset.zip\n","  inflating: /content/dataset/en-fr.csv  \n"]}],"source":["!unzip \"/content/en-fr-translation-dataset.zip\" -d \"/content/dataset/\""]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":335,"status":"ok","timestamp":1713962671239,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"vM823R6P_KsQ"},"outputs":[],"source":["dataset = tf.data.experimental.CsvDataset(\n","  \"/content/dataset/en-fr.csv\",\n","  [\n","    tf.string,\n","    tf.string\n","  ],\n",")"]},{"cell_type":"markdown","metadata":{"id":"vmL4ppSOF-u5"},"source":["The code snippet demonstrates the use of `tf.data.experimental.CsvDataset` in TensorFlow to create a dataset from a CSV file. Let's break down what this code does:\n","\n","```python\n","dataset = tf.data.experimental.CsvDataset(\n","  \"/content/dataset/en-fr.csv\",\n","  [\n","    tf.string,\n","    tf.string\n","  ],\n",")\n","```\n","\n","Explanation:\n","- **`tf.data.experimental.CsvDataset`**: This is a TensorFlow function used to create a dataset from CSV (Comma-Separated Values) files. It reads and parses the contents of the CSV file.\n","  \n","- **`\"/content/dataset/en-fr.csv\"`**: This is the path to the CSV file from which the dataset will be created.\n","\n","- **`[tf.string, tf.string]`**: This specifies the structure of each row in the CSV file. In this case, each row is expected to contain two columns, both of type `tf.string`. This means that each row will be parsed as a tuple of two string tensors.\n","\n","When you use `tf.data.experimental.CsvDataset`, TensorFlow will read the specified CSV file and parse each row according to the provided structure (`[tf.string, tf.string]` in this case). This creates a dataset where each element corresponds to a row in the CSV file, and each element is a tuple containing the parsed values from the respective columns of that row.\n"]},{"cell_type":"markdown","metadata":{"id":"KLOU1h69Gcrp"},"source":["##Data processing"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":503,"status":"ok","timestamp":1713962695901,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"RWDkn1Uh_KvT"},"outputs":[],"source":["text_dataset=tf.data.TextLineDataset(\"/content/dataset/fra.txt\")"]},{"cell_type":"markdown","metadata":{"id":"4Qxo443bG-vu"},"source":["The code snippet uses `tf.data.TextLineDataset` in TensorFlow to create a dataset from a text file where each line of the file becomes an element of the dataset. Let's break down this code:\n","\n","```python\n","text_dataset = tf.data.TextLineDataset(\"/content/dataset/fra.txt\")\n","```\n","\n","Explanation:\n","- **`tf.data.TextLineDataset`**: This is a TensorFlow function used to create a dataset where each element corresponds to a line of text from a text file.\n","\n","- **`\"/content/dataset/fra.txt\"`**: This is the path to the text file from which the dataset will be created. Each line of this text file will be treated as a separate element in the dataset.\n","\n","When you use `tf.data.TextLineDataset`, TensorFlow reads the specified text file (`fra.txt` in this case) and creates a dataset where each element corresponds to a line of text from the file. Each line is treated as a string tensor."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1713962699367,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"efWN_n7w_KyW"},"outputs":[],"source":["VOCAB_SIZE = 20000\n","ENGLISH_SEQUENCE_LENGTH = 64\n","FRENCH_SEQUENCE_LENGTH = 64\n","EMBEDDING_DIM=300\n","BATCH_SIZE=64"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":403,"status":"ok","timestamp":1713962703500,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"kLB1Oi9v_K1Y"},"outputs":[],"source":["english_vectorize_layer = TextVectorization(\n","    standardize='lower_and_strip_punctuation',\n","    max_tokens=VOCAB_SIZE,\n","    output_mode='int',\n","    output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",")"]},{"cell_type":"markdown","metadata":{"id":"59S89X4XIXay"},"source":["Thi code snippet sets up a `TextVectorization` layer in TensorFlow for English text.\n","\n","Explanation:\n","- **`TextVectorization`**: This is a TensorFlow layer used for vectorizing text data. It tokenizes strings (e.g., sentences or documents) into integer sequences (sequences of token indices).\n","\n","- **`standardize='lower_and_strip_punctuation'`**: This parameter specifies the standardization method for text preprocessing. Here, it converts all characters to lowercase and removes punctuation from the input text.\n","\n","- **`max_tokens=VOCAB_SIZE`**: This sets the maximum size of the vocabulary (number of unique tokens) that the vectorization layer will generate. `VOCAB_SIZE` is a predefined constant that determines the vocabulary size.\n","\n","- **`output_mode='int'`**: This parameter specifies the output mode of the vectorization layer. `'int'` indicates that the layer should output sequences of integers (token indices).\n","\n","- **`output_sequence_length=FRENCH_SEQUENCE_LENGTH`**: This parameter sets the length of the output sequences generated by the vectorization layer. `FRENCH_SEQUENCE_LENGTH` determines the fixed length of output sequences. If a sequence is shorter than this length, it will be padded; if longer, it will be truncated.\n","\n","In summary, `english_vectorize_layer` is configured to preprocess English text by converting it to lowercase, removing punctuation, tokenizing it into integer sequences based on a specified vocabulary size (`VOCAB_SIZE`), and ensuring that all output sequences have a fixed length (`FRENCH_SEQUENCE_LENGTH`)."]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":554,"status":"ok","timestamp":1713962722646,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"opzeos8w_K4T"},"outputs":[],"source":["french_vectorize_layer=TextVectorization(\n","    standardize='lower_and_strip_punctuation',\n","    max_tokens=VOCAB_SIZE,\n","    output_mode='int',\n","    output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1713962725095,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"8qqHfw8I_K7b"},"outputs":[],"source":["def selector(input_text):\n","  split_text = tf.strings.split(input_text, '\\t')\n","  return {'input_1':split_text[0:1],'input_2':'starttoken '+split_text[1:2]},split_text[1:2]+' endtoken'"]},{"cell_type":"markdown","metadata":{"id":"zCzfQBn8Jk3x"},"source":["Explanation:\n","- **`tf.strings.split(input_text, '\\t')`**: This line uses TensorFlow's `tf.strings.split` function to split the input text (`input_text`) into segments based on the tab (`'\\t'`) delimiter. This assumes that the input text is formatted with tab-separated values.\n","\n","- **`split_text[0:1]`**: This extracts the first segment of the split text, corresponding to the first part before the tab delimiter.\n","\n","- **`split_text[1:2]`**: This extracts the second segment of the split text, corresponding to the part after the tab delimiter.\n","\n","- **`{'input_1': split_text[0:1], 'input_2': 'starttoken ' + split_text[1:2]}`**: This constructs a dictionary as the first part of the return value. It assigns the first segment (`split_text[0:1]`) to the key `'input_1'` and concatenates the second segment (`split_text[1:2]`) with the string `'starttoken '` to form the value associated with the key `'input_2'`.\n","\n","- **`split_text[1:2] + ' endtoken'`**: This concatenates the second segment (`split_text[1:2]`) with the string `' endtoken'` as the second part of the return value.\n","\n","Overall, the `selector` function processes input text by splitting it into two segments based on the tab delimiter (`'\\t'`). It constructs a dictionary containing these segments (`input_1` and `input_2`) and returns a tuple with this dictionary as the first element and the processed text as the second element. This function is designed for data preprocessing or text manipulation tasks within a TensorFlow environment."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":303,"status":"ok","timestamp":1713962728445,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"rrcCoR1B_K-k"},"outputs":[],"source":["split_dataset=text_dataset.map(selector)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":254,"status":"ok","timestamp":1713962730359,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"WMiGyJcF_LBZ"},"outputs":[],"source":["def separator(input_text):\n","  split_text=tf.strings.split(input_text,'\\t')\n","  return split_text[0:1],'starttoken '+split_text[1:2]+' endtoken'"]},{"cell_type":"markdown","metadata":{"id":"GTTEtOEhK95D"},"source":["The `separator` function defined takes an input text and splits it using the tab (`'\\t'`) delimiter. It then constructs and returns a tuple containing two elements:\n","\n","1. **`split_text[0:1]`**: This extracts the first segment of the split text, which corresponds to the part before the tab delimiter.\n","\n","2. **`'starttoken ' + split_text[1:2] + ' endtoken'`**: Here, `split_text[1:2]` extracts the second segment of the split text (the part after the tab delimiter). This segment is then concatenated with `'starttoken '` at the beginning and `' endtoken'` at the end to form a single string.\n","\n","The function `separator` effectively preprocesses input text, splitting it into segments based on tab delimiters and formatting the output as described. The result is a tuple containing the processed segments suitable for further processing or analysis."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":447,"status":"ok","timestamp":1713962733221,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"QiRfnxKR_LE1"},"outputs":[],"source":["init_dataset=text_dataset.map(separator)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":399,"status":"ok","timestamp":1713962735479,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"rMy8VDP-_LIO","outputId":"b9997a11-2ffb-40c5-e8d5-d8357092ae36"},"outputs":[{"name":"stdout","output_type":"stream","text":["({'input_1': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)\u003e, 'input_2': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va !'], dtype=object)\u003e}, \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! endtoken'], dtype=object)\u003e)\n","({'input_1': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)\u003e, 'input_2': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Marche.'], dtype=object)\u003e}, \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'Marche. endtoken'], dtype=object)\u003e)\n","({'input_1': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)\u003e, 'input_2': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken En route !'], dtype=object)\u003e}, \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'En route ! endtoken'], dtype=object)\u003e)\n"]}],"source":["for i in split_dataset.take(3):\n","  print(i)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":432144,"status":"ok","timestamp":1713963170568,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"bgIQ85U0_LMF"},"outputs":[],"source":["english_training_data=init_dataset.map(lambda x,y:x)\n","english_vectorize_layer.adapt(english_training_data)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":369,"status":"ok","timestamp":1713963200568,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"r3YKYTCD_LOQ","outputId":"580f1808-f688-412e-bb27-74e4b98371de"},"outputs":[{"name":"stdout","output_type":"stream","text":["16952\n","2\n"]}],"source":["print(len(english_vectorize_layer.get_vocabulary()))\n","print(len(french_vectorize_layer.get_vocabulary()))"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":428411,"status":"ok","timestamp":1713963631430,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"nPpG79y-_LRY"},"outputs":[],"source":["# Take x,y as input and output y, where x=english text, y= french translation\n","french_training_data=init_dataset.map(lambda x,y:y)\n","# After extracting the french text from the dataset, we adapt the vectorize_layer to the french training data\n","french_vectorize_layer.adapt(french_training_data)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713963744443,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"OIX7tGjK_LUU"},"outputs":[],"source":["def vectorizer(inputs,output):\n","  return {'input_1':english_vectorize_layer(inputs['input_1']),\n","          'input_2':french_vectorize_layer(inputs['input_2'])},french_vectorize_layer(output)"]},{"cell_type":"markdown","metadata":{"id":"VZe_0AWMQXlp"},"source":["The `vectorizer` function defined is a preprocessing function designed to vectorize input and output data for a machine learning model. Here's a breakdown of how it works:\n","\n","- **Inputs**:\n","  - `inputs`: This parameter is expected to be a dictionary containing two keys:\n","    - `'input_1'`: This key corresponds to English text inputs.\n","    - `'input_2'`: This key corresponds to French text inputs.\n","\n","- **Output**:\n","  - `output`: This parameter represents the target French text outputs.\n","\n","- **Processing**:\n","  - `english_vectorize_layer(inputs['input_1'])`: This line uses the `english_vectorize_layer` (presumably a `TextVectorization` layer) to vectorize the English input text provided under the `'input_1'` key of the `inputs` dictionary.\n","  \n","  - `french_vectorize_layer(inputs['input_2'])`: Similarly, this line vectorizes the French input text provided under the `'input_2'` key using the `french_vectorize_layer`.\n","  \n","  - `french_vectorize_layer(output)`: This line vectorizes the target French text `output` using the `french_vectorize_layer`.\n","\n","- **Return Value**:\n","  - The function returns a tuple:\n","    - The first element is a dictionary with two entries:\n","      - `'input_1'`: Vectorized representation of the English input text.\n","      - `'input_2'`: Vectorized representation of the French input text.\n","    - The second element is the vectorized representation of the target French output text.\n","\n","Overall, the `vectorizer` function prepares input and output data for training a machine learning model. It uses specific vectorization layers (`english_vectorize_layer` and `french_vectorize_layer`) to convert raw text data into numerical formats suitable for model training. The output is structured to match the input requirements of a neural network model."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":331,"status":"ok","timestamp":1713963749258,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"rX-KQmOk_LaY","outputId":"0b13858d-df2e-4e0c-db48-ccf77d759719"},"outputs":[{"data":{"text/plain":["\u003c_MapDataset element_spec=({'input_1': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'input_2': TensorSpec(shape=(None,), dtype=tf.string, name=None)}, TensorSpec(shape=(None,), dtype=tf.string, name=None))\u003e"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["split_dataset"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":365,"status":"ok","timestamp":1713963751669,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"Chn6jYS8_Ldi"},"outputs":[],"source":["dataset=split_dataset.map(vectorizer)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1713963753501,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"2suQiDbG_Lgf","outputId":"f466e5f3-a7e8-4fff-b62d-767f6c74b642"},"outputs":[{"name":"stdout","output_type":"stream","text":["({'input_1': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)\u003e, 'input_2': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va !'], dtype=object)\u003e}, \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! endtoken'], dtype=object)\u003e)\n","({'input_1': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)\u003e, 'input_2': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Marche.'], dtype=object)\u003e}, \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'Marche. endtoken'], dtype=object)\u003e)\n","({'input_1': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)\u003e, 'input_2': \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken En route !'], dtype=object)\u003e}, \u003ctf.Tensor: shape=(1,), dtype=string, numpy=array([b'En route ! endtoken'], dtype=object)\u003e)\n"]}],"source":["for i in split_dataset.take(3):\n","  print(i)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713963755584,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"cIQj76FC_Ljm","outputId":"f8efab80-d281-49c2-9b5c-32a9a9157269"},"outputs":[{"name":"stdout","output_type":"stream","text":["({'input_1': \u003ctf.Tensor: shape=(1, 64), dtype=int64, numpy=\n","array([[45,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\u003e, 'input_2': \u003ctf.Tensor: shape=(1, 64), dtype=int64, numpy=\n","array([[  2, 104,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])\u003e}, \u003ctf.Tensor: shape=(1, 64), dtype=int64, numpy=\n","array([[104,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])\u003e)\n"]}],"source":["for i in dataset.take(1):\n","  print(i)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":382,"status":"ok","timestamp":1713963759349,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"B6QdoylI_Lmq","outputId":"ba405da4-77cd-4eb4-8a3a-c2c5aa82872b"},"outputs":[{"data":{"text/plain":["\u003c_MapDataset element_spec=({'input_1': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))\u003e"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":285,"status":"ok","timestamp":1713963761701,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"xl503pL-_Lph"},"outputs":[],"source":["dataset=dataset.shuffle(2048).unbatch().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":281,"status":"ok","timestamp":1713963764392,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"pVHkqocC_Lsh","outputId":"53ddb2c0-40fe-44a2-ec55-4e94e87ed091"},"outputs":[{"data":{"text/plain":["\u003c_PrefetchDataset element_spec=({'input_1': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))\u003e"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":279,"status":"ok","timestamp":1713963767020,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"UDkuzwqS_Lvl"},"outputs":[],"source":["NUM_BATCHES=int(200000/BATCH_SIZE)"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713963768848,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"kRN8BqQ7_LyW"},"outputs":[],"source":["train_dataset=dataset.take(int(0.9*NUM_BATCHES))\n","val_dataset=dataset.skip(int(0.9*NUM_BATCHES))"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713963770431,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"KAaUCEAO_L1O","outputId":"4a563fa6-8bed-459e-f5d1-4eb96e77a3f6"},"outputs":[{"data":{"text/plain":["\u003c_TakeDataset element_spec=({'input_1': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 64), dtype=tf.int64, name=None))\u003e"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"executionInfo":{"elapsed":2064,"status":"error","timestamp":1713963774362,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"y0AOv9CB_L4p","outputId":"539d561e-4acf-46c5-acf2-fd490c94fcf5"},"outputs":[{"ename":"TypeError","evalue":"The dataset length is unknown.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-30-8f70883cce9f\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    530\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The dataset is infinite.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mUNKNOWN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 532\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The dataset length is unknown.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: The dataset length is unknown."]}],"source":["len(dataset)"]},{"cell_type":"markdown","metadata":{"id":"FfDrukRISkXk"},"source":["#Modeling"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":297,"status":"ok","timestamp":1713963778915,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"mkfErAO0Sdhi"},"outputs":[],"source":["NUM_UNITS=256"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2929,"status":"ok","timestamp":1713963784575,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"66PHMb8pTTwy","outputId":"22eafd12-4246-4ced-aa03-3d1e93adf468"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 64)]                 0         []                            \n","                                                                                                  \n"," input_2 (InputLayer)        [(None, 64)]                 0         []                            \n","                                                                                                  \n"," embedding (Embedding)       (None, 64, 300)              6000000   ['input_1[0][0]']             \n","                                                                                                  \n"," embedding_1 (Embedding)     (None, 64, 300)              6000000   ['input_2[0][0]']             \n","                                                                                                  \n"," bidirectional (Bidirection  (None, 512)                  857088    ['embedding[0][0]']           \n"," al)                                                                                              \n","                                                                                                  \n"," gru_1 (GRU)                 (None, 64, 512)              1250304   ['embedding_1[0][0]',         \n","                                                                     'bidirectional[0][0]']       \n","                                                                                                  \n"," dropout (Dropout)           (None, 64, 512)              0         ['gru_1[0][0]']               \n","                                                                                                  \n"," dense (Dense)               (None, 64, 20000)            1026000   ['dropout[0][0]']             \n","                                                          0                                       \n","                                                                                                  \n","==================================================================================================\n","Total params: 24367392 (92.95 MB)\n","Trainable params: 24367392 (92.95 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["# Define the encoder\n","input = Input(shape=(ENGLISH_SEQUENCE_LENGTH,), dtype='int64', name='input_1')\n","x = Embedding(VOCAB_SIZE, EMBEDDING_DIM, )(input)\n","encoded_input = Bidirectional(GRU(NUM_UNITS), )(x)\n","\n","### DECODER\n","shifted_target=Input(shape=(FRENCH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_2\")\n","x=Embedding(VOCAB_SIZE,EMBEDDING_DIM,)(shifted_target)\n","x = GRU(NUM_UNITS*2, return_sequences=True)(x, initial_state=encoded_input)\n","\n","### OUTPUT\n","x = Dropout(0.5)(x)\n","target=Dense(VOCAB_SIZE,activation=\"softmax\")(x)\n","seq2seq_gru=Model([input,shifted_target],target)\n","seq2seq_gru.summary()"]},{"cell_type":"markdown","metadata":{"id":"iw_5fa1mXTZM"},"source":["This code snippet defines a sequence-to-sequence (seq2seq) model using a GRU-based encoder-decoder architecture.\n","### Encoder\n","```python\n","input = Input(shape=(ENGLISH_SEQUENCE_LENGTH,), dtype='int64', name='input_1')\n","x = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(input)\n","encoded_input = Bidirectional(GRU(NUM_UNITS))(x)\n","```\n","- **Input Layer (`input`):** Defines the input layer for the encoder, specifying the shape of the input sequences (`ENGLISH_SEQUENCE_LENGTH`) and data type (`int64`).\n","  \n","- **Embedding Layer (`Embedding`):** Maps input integer sequences to dense vectors (`EMBEDDING_DIM`) using an embedding matrix of size `VOCAB_SIZE`.\n","\n","- **Bidirectional GRU (`Bidirectional(GRU)`):** Applies a bidirectional GRU (Gated Recurrent Unit) layer to the embedded input sequences. This layer processes the input sequences in both forward and backward directions, capturing dependencies in both contexts.\n","\n","### Decoder\n","```python\n","shifted_target = Input(shape=(FRENCH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_2\")\n","x = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(shifted_target)\n","x = GRU(NUM_UNITS*2, return_sequences=True)(x, initial_state=encoded_input)\n","```\n","- **Input Layer (`shifted_target`):** Defines the input layer for the decoder, specifying the shape of the target sequences (`FRENCH_SEQUENCE_LENGTH`) and data type (`int64`).\n","\n","- **Embedding Layer (`Embedding`):** Similar to the encoder, this layer maps target integer sequences to dense vectors using the same embedding matrix (`VOCAB_SIZE`, `EMBEDDING_DIM`).\n","\n","- **GRU Decoder (`GRU`):** Applies a GRU layer to the embedded target sequences. The `initial_state` parameter is set to the `encoded_input` from the encoder, initializing the decoder's hidden state with the final state of the encoder.\n","\n","### Output\n","```python\n","x = Dropout(0.5)(x)\n","target = Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n","```\n","- **Dropout Layer (`Dropout`):** Applies dropout regularization to prevent overfitting by randomly setting a fraction of input units to zero during training (`0.5` in this case).\n","\n","- **Dense Layer (`Dense`):** Computes the output probabilities over the vocabulary (`VOCAB_SIZE`) using a softmax activation function, predicting the next token in the target sequence.\n","\n","### Model Compilation\n","```python\n","seq2seq_gru = Model([input, shifted_target], target)\n","seq2seq_gru.summary()\n","```\n","- **Model Definition (`Model`):** Combines the encoder and decoder layers into a functional Keras model, taking both the input and target sequences as inputs and outputting the predicted target sequences (`target`).\n","\n","- **Model Summary (`summary()`):** Displays the summary of the entire model architecture, showing the layer types, output shapes, and trainable parameters.\n","\n","This architecture represents a basic seq2seq model using GRU cells for sequence encoding and decoding. It's commonly used for tasks like machine translation, where the model learns to generate a target sequence (e.g., translated sentences) from a given input sequence (e.g., source sentences)."]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":397,"status":"ok","timestamp":1713963793825,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"me7DGrw5TTzr"},"outputs":[],"source":["class BLEU(tf.keras.metrics.Metric):\n","    def __init__(self,name='bleu_score'):\n","        super(BLEU,self).__init__()\n","        self.bleu_score=0\n","\n","    def update_state(self,y_true,y_pred,sample_weight=None):\n","      y_pred=tf.argmax(y_pred,-1)\n","      self.bleu_score=0\n","      for i,j in zip(y_pred,y_true):\n","        tf.autograph.experimental.set_loop_options()\n","\n","        total_words=tf.math.count_nonzero(i)\n","        total_matches=0\n","        for word in i:\n","          if word==0:\n","            break\n","          for q in range(len(j)):\n","            if j[q]==0:\n","              break\n","            if word==j[q]:\n","              total_matches+=1\n","              j=tf.boolean_mask(j,[False if y==q else True for y in range(len(j))])\n","              break\n","\n","        self.bleu_score+=total_matches/total_words\n","\n","    def result(self):\n","        return self.bleu_score/BATCH_SIZE"]},{"cell_type":"markdown","metadata":{"id":"BTLQhDpRZjPC"},"source":["This is a custom BLEU (Bilingual Evaluation Understudy) metric in TensorFlow/Keras. The BLEU score is a common metric used for evaluating the quality of machine-translated text against one or more reference translations. Let's explain the key components of this custom metric implementation:\n","\n","- **Class Definition (`BLEU`):** Defines a custom metric class named `BLEU` that inherits from `tf.keras.metrics.Metric`.\n","\n","- **Initialization (`__init__`):** Initializes the BLEU metric by calling the superclass constructor (`super(BLEU, self).__init__()`). Here, the default name for the metric is set to `'bleu_score'`.\n","\n","- **`update_state` Method:** This method is called to update the metric state based on the ground truth (`y_true`) and predicted (`y_pred`) values for each batch of data.\n","\n","  - **Input Arguments:**\n","    - `y_true`: Ground truth values (expected output).\n","    - `y_pred`: Predicted values (model output).\n","    - `sample_weight`: Optional sample weights.\n","\n","  - **Implementation Details:**\n","    1. **Argmax Prediction:** Converts the predicted values (`y_pred`) into class labels by taking the argmax along the last axis (`-1`). This is typical for sequence prediction tasks.\n","    \n","    2. **BLEU Calculation:** Iterates over each pair of predicted (`y_pred`) and true (`y_true`) sequences in the batch. For each sequence pair:\n","       - Calculates the total number of non-zero words (`total_words`) in the predicted sequence (`i`).\n","       - Counts the number of matching words (`total_matches`) between the predicted sequence and the true sequence (`j`). The matching process involves:\n","         - Iterating through each word in the predicted sequence.\n","         - Checking if the word exists in the true sequence (`j`).\n","         - Removing the matched word from the true sequence to prevent double counting.\n","       - Computes the BLEU score contribution for the sequence pair based on the ratio of `total_matches` to `total_words`.\n","       - Accumulates the BLEU score (`self.bleu_score`) by adding the contribution of each sequence pair in the batch.\n","\n","- **`result` Method:** Computes the final BLEU score by dividing the accumulated BLEU score (`self.bleu_score`) by the batch size (`BATCH_SIZE`) and returns the result.\n","\n","This custom BLEU metric implementation provides a way to compute BLEU scores directly within the TensorFlow/Keras framework for evaluating sequence prediction models. Note that the BLEU computation here is simplified and may not fully capture all aspects of the standard BLEU metric used in machine translation evaluation, such as handling n-grams and brevity penalty."]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":276,"status":"ok","timestamp":1713963799048,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"l8DRQbTbTT2j"},"outputs":[],"source":["seq2seq_gru.compile(\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","    optimizer=tf.keras.optimizers.Adam(5e-4),)\n","    #metrics=[BLEU()],\n","    #run_eagerly=True)"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":295,"status":"ok","timestamp":1713963801560,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"NR6fhrMWTT5a"},"outputs":[],"source":["checkpoint_filepath = '/content/drive/MyDrive/NLP Repository/Projects/Sentiments_analysis/lstm.h5'\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    monitor='val_loss',\n","    mode='min',\n","    save_best_only=True,)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"RuNtddOnTT8D"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","    478/Unknown - 5397s 11s/step - loss: 0.6783"]}],"source":["history=seq2seq_gru.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=20,\n","    callbacks=[model_checkpoint_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxUzkP3zTT-8"},"outputs":[],"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model_loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1713959295923,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"KT5hmIXvTUBv"},"outputs":[],"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","\n","plt.title('model_accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"TQtk1xRjcaAO"},"source":["#Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1713959295923,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"mIm5hHrQTUEp"},"outputs":[],"source":["seq2seq_gru.evaluate(val_dataset)"]},{"cell_type":"markdown","metadata":{"id":"iXgX-7vfcnsU"},"source":["#Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1713959295923,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"ZyU7ObTLccgZ"},"outputs":[],"source":["index_to_word={x:y for x, y in zip(range(len(french_vectorize_layer.get_vocabulary())),\n","                                   french_vectorize_layer.get_vocabulary())}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1713959295923,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"8Tei6IAZccjV"},"outputs":[],"source":["\n","def translator(english_sentence):\n","  tokenized_english_sentence=english_vectorize_layer([english_sentence])\n","  shifted_target='starttoken'\n","\n","  for i in range(FRENCH_SEQUENCE_LENGTH):\n","    tokenized_shifted_target=french_vectorize_layer([shifted_target])\n","    output=seq2seq_gru.predict([tokenized_english_sentence,tokenized_shifted_target])\n","    french_word_index=tf.argmax(output,axis=-1)[0][i].numpy()\n","    current_word=index_to_word[french_word_index]\n","    if current_word=='endtoken':\n","      break\n","    shifted_target+=' '+current_word\n","  return shifted_target[11:]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1713959295923,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"-H6tQBemccmM"},"outputs":[],"source":["translator('What makes you think that is not true?')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1713959295924,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"},"user_tz":-180},"id":"vuOB2rYzccpT"},"outputs":[],"source":["word_to_index={y:x for x, y in zip(range(len(french_vectorize_layer.get_vocabulary())),\n","                                   french_vectorize_layer.get_vocabulary())}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O1nncQ8Occr0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMbRzlUbccvS"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPcAO3u5q/TtL3TVNgPf+Uo","collapsed_sections":["kjFD7D9Y_pSe","ipKLBlfsABi4"],"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}