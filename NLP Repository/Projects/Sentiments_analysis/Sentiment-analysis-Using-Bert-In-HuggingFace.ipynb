{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["k9uEQDmgVAGl","3Tmg5bxsVLAs","lLvrB3VNVvzH","mhxSyiQCV1Qi","cOFqCbzOV-e5","BZf56hshWP6m","z9iWI0UaWiY3","AmqWZTzxWxIV"],"authorship_tag":"ABX9TyMrOQIEyEqBeiKkUrxyWB+i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Installation"],"metadata":{"id":"enZ1W2Xd5e0Y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKFmPylI5Sx4"},"outputs":[],"source":["!pip install transformers datasets"]},{"cell_type":"markdown","source":["#Import statements"],"metadata":{"id":"9GaOdYml5p0f"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","from sklearn.metrics import confusion_matrix, roc_curve\n","import seaborn as sns\n","import datetime\n","import pathlib\n","import io\n","import os\n","import re\n","import string\n","import time\n","from numpy import random\n","import gensim.downloader as api\n","from PIL import Image\n","import tensorflow_datasets as tfds\n","import tensorflow_probability as tfp\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras.layers import Dense,Flatten,InputLayer,BatchNormalization,Dropout,Input,LayerNormalization\n","from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n","from tensorflow.keras.optimizers import Adam\n","from google.colab import drive\n","from google.colab import files\n","from datasets import load_dataset\n","from transformers import (BertTokenizerFast,TFBertTokenizer,BertTokenizer,RobertaTokenizerFast,\n","                          DataCollatorWithPadding,TFRobertaForSequenceClassification,TFBertForSequenceClassification,\n","                          TFBertModel,create_optimizer)"],"metadata":{"id":"reADL6XJ5dBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE=8"],"metadata":{"id":"IaqbaIx75dEd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Data Preparation"],"metadata":{"id":"BbbfIofU7Wc2"}},{"cell_type":"code","source":["dataset_id='imdb'\n","dataset = load_dataset(dataset_id)"],"metadata":{"id":"xl8GhYbV5dHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"id":"EN0HfIQ95dM3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['train'][0]"],"metadata":{"id":"gC_tYS2o5dPt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Bert Model"],"metadata":{"id":"z3dLKehIEbyc"}},{"cell_type":"markdown","source":["The main difference between BERT models with \"uncased\" and \"cased\" variants lies in how they handle the casing of text during pre-training and inference. Here's a breakdown of the differences between `bert-uncased` and `bert-cased` models from Hugging Face:\n","\n","### BERT-Uncased (`bert-base-uncased`):\n","\n","- **Lowercase Text**:\n","  - The `bert-base-uncased` model is trained on uncased (lowercase) text during pre-training.\n","  - All text, including both input data and vocabulary, is converted to lowercase before being processed by the model.\n","  - Example: \"Hello, World!\" → \"hello, world!\"\n","\n","- **Vocabulary**:\n","  - The vocabulary of the `bert-base-uncased` model consists of lowercase words only.\n","  - This simplifies tokenization and reduces the number of unique tokens by treating uppercase and lowercase versions of the same word as identical.\n","\n","- **Usage**:\n","  - Use `bert-base-uncased` for tasks where the distinction between uppercase and lowercase letters is not crucial, such as general-purpose text processing or sentiment analysis.\n","\n","### BERT-Cased (`bert-base-cased`):\n","\n","- **Preserves Case**:\n","  - The `bert-base-cased` model preserves the case (uppercase and lowercase) of text during pre-training and inference.\n","  - Text is tokenized and processed without converting to lowercase, retaining the original casing of words.\n","  - Example: \"Hello, World!\" → \"Hello, World!\"\n","\n","- **Vocabulary**:\n","  - The vocabulary of the `bert-base-cased` model includes both lowercase and uppercase versions of words.\n","  - This allows the model to differentiate between words based on their casing, capturing nuances related to proper nouns and acronyms.\n","\n","- **Usage**:\n","  - Use `bert-base-cased` for tasks where the distinction between uppercase and lowercase letters is important, such as named entity recognition (NER) or tasks involving proper nouns and specific text formatting.\n","\n","### Choosing Between BERT-Uncased and BERT-Cased:\n","\n","- **Task Requirements**:\n","  - Consider the nature of your NLP task and whether it requires sensitivity to letter casing.\n","  - Tasks involving proper nouns, named entities, or specific text formatting may benefit from `bert-base-cased`.\n","  - For general-purpose tasks or scenarios where case sensitivity is less critical, `bert-base-uncased` can be a suitable choice.\n","\n","- **Model Performance**:\n","  - Evaluate both variants on your specific task to determine which model performs better based on accuracy, precision, and other metrics.\n","  - Sometimes, the choice between cased and uncased models can impact model performance depending on the characteristics of the dataset.\n","\n","- **Pre-trained Model Selection**:\n","  - Hugging Face offers both `bert-base-uncased` and `bert-base-cased` models pre-trained on large corpora.\n","  - Choose the pre-trained model that aligns with your task requirements and data characteristics.\n","\n","In summary, the choice between `bert-base-uncased` and `bert-base-cased` depends on whether your NLP task benefits from case sensitivity and how text casing impacts the performance of your models."],"metadata":{"id":"TpqI8m8hF4wB"}},{"cell_type":"code","source":["model_id=\"bert-base-uncased\"\n","tokenizer = BertTokenizerFast.from_pretrained(model_id)"],"metadata":{"id":"nCv-kcIy5dVM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.is_fast"],"metadata":{"id":"Di5wxDnZ5dWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_input_1='The Weather of Today is Gréat! zwp'\n","test_input_2='How are you doing?'\n","inputs=[test_input_1,test_input_2]\n","\n","tokenizer.tokenize(inputs,)"],"metadata":{"id":"iwCKx0pD5dX4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output=tokenizer(inputs,padding=True,truncation=True,max_length=128)\n","print(output)"],"metadata":{"id":"afMlniWR5dam"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.decode(output['input_ids'][0])"],"metadata":{"id":"3MokeII95ddt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.decode(output['input_ids'][1])"],"metadata":{"id":"I8zBIwhW5dga"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples):\n","  return tokenizer(examples[\"text\"],padding=True,truncation=True,)"],"metadata":{"id":"5vCladC55djT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `preprocess_function` is a Python function designed to preprocess text examples using a tokenizer, typically for input into a natural language processing (NLP) model such as BERT. This function applies tokenization, padding, and truncation to the text data. Let's break down the components and explain how this function works:\n","\n","### Explanation of `preprocess_function`:\n","\n","```python\n","def preprocess_function(examples):\n","    return tokenizer(\n","        examples[\"text\"],\n","        padding=True,\n","        truncation=True\n","    )\n","```\n","\n","- **Input**:\n","  - `examples`: This parameter represents a batch of input examples, where each example contains a `\"text\"` field that needs preprocessing.\n","\n","- **Tokenization**:\n","  - `tokenizer(examples[\"text\"])`: The function uses a tokenizer (assumed to be defined elsewhere) to tokenize the text data.\n","  - The tokenizer breaks down each input text into tokens according to its vocabulary and tokenization rules.\n","\n","- **Padding**:\n","  - `padding=True`: Enables padding of tokenized sequences to ensure uniform length across examples in a batch.\n","  - Padding is often necessary for batching sequences and is achieved by appending special tokens (like `[PAD]`) to shorter sequences.\n","\n","- **Truncation**:\n","  - `truncation=True`: Enables truncation of tokenized sequences to a specified maximum length.\n","  - Truncation is used to limit the length of tokenized sequences, ensuring they fit within the model's input size constraints.\n","\n","### Usage Scenario:\n","\n","- **Batch Processing**:\n","  - The `preprocess_function` is typically used in conjunction with a dataset or data pipeline (e.g., using Hugging Face Datasets library or TensorFlow `Dataset.map()` method).\n","  - It processes batches of input examples, where each example contains text data to be tokenized, padded, and possibly truncated.\n","\n","- **Integration with Tokenizer**:\n","  - The function assumes the existence of a `tokenizer` object initialized elsewhere.\n","  - The `tokenizer` performs the actual tokenization process, converting input text into tokens recognizable by the NLP model.\n","\n","- **Batch Output**:\n","  - The output of `preprocess_function` is a dictionary or structure suitable for input to an NLP model.\n","  - Each example in the batch is transformed into tokenized sequences with consistent length (due to padding) and possibly truncated to fit model input requirements."],"metadata":{"id":"sc8m1zGuHlT-"}},{"cell_type":"code","source":["tokenized_dataset = dataset.map(preprocess_function, batched=True)"],"metadata":{"id":"rggDdpoz5dmE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_dataset"],"metadata":{"id":"2qiXVeMK5dpE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_train_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n","    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","    shuffle=True,\n","    batch_size=BATCH_SIZE,\n","    #collate_fn=data_collator\n",")"],"metadata":{"id":"ro8wGfRv5dr_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Code Explanation:\n","\n","```python\n","tf_train_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n","    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","    shuffle=True,\n","    batch_size=BATCH_SIZE\n",")\n","```\n","\n","- **`to_tf_dataset()` Method**:\n","  - The `to_tf_dataset()` method is used to convert a Hugging Face `Dataset` object (`tokenized_dataset[\"train\"]`) into a TensorFlow dataset (`tf.data.Dataset`).\n","\n","- **Dataset Columns**:\n","  - `columns=['input_ids', 'token_type_ids', 'attention_mask', 'label']`: Specifies the dataset columns to include in the TensorFlow dataset.\n","    - `input_ids`: Contains the token IDs representing input text sequences.\n","    - `token_type_ids`: Represents the segment IDs (e.g., for sentence pair classification tasks).\n","    - `attention_mask`: Indicates which tokens should be attended to (1 for tokens, 0 for padding).\n","    - `label`: Contains the target labels associated with the input sequences (e.g., sentiment labels).\n","\n","- **Shuffling and Batch Size**:\n","  - `shuffle=True`: Enables shuffling of the dataset to randomize the order of examples during training.\n","  - `batch_size=BATCH_SIZE`: Sets the batch size for the TensorFlow dataset, determining the number of examples processed together in each training step.\n","\n","\n"],"metadata":{"id":"tTwmEAC5J40h"}},{"cell_type":"code","source":["tf_val_dataset = tokenized_dataset[\"test\"].to_tf_dataset(\n","    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","    shuffle=True,\n","    batch_size=BATCH_SIZE,\n","    #collate_fn=data_collator\n",")"],"metadata":{"id":"HWuJcHEl5dus"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def swap_positions(dataset):\n","  return {'input_ids':dataset['input_ids'],\n","          'token_type_ids':dataset['token_type_ids'],\n","          'attention_mask':dataset['attention_mask'],},dataset['label']"],"metadata":{"id":"MuJ5BtSL5dxY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `swap_positions` function is designed to transform a dataset dictionary into a format suitable for training or inference with a machine learning model. This function appears to modify the structure of the dataset by rearranging or extracting specific components.\n","### Code Explanation:\n","\n","```python\n","def swap_positions(dataset):\n","    return {\n","        'input_ids': dataset['input_ids'],\n","        'token_type_ids': dataset['token_type_ids'],\n","        'attention_mask': dataset['attention_mask'],\n","    }, dataset['label']\n","```\n","\n","- **Input**:\n","  - `dataset`: This parameter represents a single example from the dataset, typically in dictionary format containing input features (`'input_ids'`, `'token_type_ids'`, `'attention_mask'`) and corresponding labels (`'label'`).\n","\n","- **Output**:\n","  - The function returns a tuple containing two elements:\n","    1. A dictionary (`features_dict`) containing input features (`'input_ids'`, `'token_type_ids'`, `'attention_mask'`).\n","    2. The corresponding label (`'label'`).\n","\n","- **Dictionary Structure**:\n","  - The `features_dict` contains specific keys (`'input_ids'`, `'token_type_ids'`, `'attention_mask'`) extracted from the input `dataset`.\n","  - Each key corresponds to a specific input feature required for the machine learning model.\n","\n","- **Label Extraction**:\n","  - The label (`'label'`) is extracted directly from the input `dataset` and returned as the second element of the tuple.\n","\n","### Usage Scenario:\n","\n","- **Dataset Processing**:\n","  - Use `swap_positions` to process individual examples from a dataset, preparing them for model training or inference.\n","  - The function extracts necessary input features (`'input_ids'`, `'token_type_ids'`, `'attention_mask'`) and organizes them into a dictionary (`features_dict`).\n","\n","- **Model Input Format**:\n","  - This function is useful for adapting dataset structures to match the expected input format of machine learning models.\n","  - For example, the output tuple can directly serve as input to a TensorFlow model with the appropriate keys (`'input_ids'`, `'token_type_ids'`, `'attention_mask'`) and label (`'label'`).\n","\n","### Example Usage:\n","\n","```python\n","# Assuming `dataset` is a single example from a tokenized dataset\n","example = {\n","    'input_ids': [101, 2023, 2003, 1037, 2143, 1029],\n","    'token_type_ids': [0, 0, 0, 0, 0, 0],\n","    'attention_mask': [1, 1, 1, 1, 1, 1],\n","    'label': 1\n","}\n","\n","# Apply `swap_positions` function to process the example\n","processed_example = swap_positions(example)\n","\n","# Processed example output\n","print(processed_example)\n","# Output: ({'input_ids': [101, 2023, 2003, 1037, 2143, 1029], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}, 1)\n","```\n","\n","In this example, `swap_positions` processes a single example (`example`) by extracting input features (`'input_ids'`, `'token_type_ids'`, `'attention_mask'`) and the corresponding label (`'label'`). The processed example (`processed_example`) is then ready for use as input to a machine learning model."],"metadata":{"id":"2hsRb0MfLCvT"}},{"cell_type":"code","source":["tf_train_dataset=tf_train_dataset.map(swap_positions).prefetch(tf.data.AUTOTUNE)\n","tf_val_dataset=tf_val_dataset.map(swap_positions).prefetch(tf.data.AUTOTUNE)"],"metadata":{"id":"vjHQaYLj5d0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in tf_train_dataset.take(1):\n","  print(i)"],"metadata":{"id":"4EUp6xcg5d2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_val_dataset"],"metadata":{"id":"yGA9wlDELXkm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Data Preparation for Roberta Model"],"metadata":{"id":"jYYBV3UILjXc"}},{"cell_type":"code","source":["model_id=\"roberta-base\"\n","tokenizer=RobertaTokenizerFast.from_pretrained(model_id)"],"metadata":{"id":"2BrHtNDNLXnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples):\n","  return tokenizer(examples[\"text\"],padding=True,truncation=True,)"],"metadata":{"id":"r8rgY9WNLXqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_dataset = dataset.map(preprocess_function,)# batched=True)"],"metadata":{"id":"rlAWDelxLXtp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_dataset['train'][0]"],"metadata":{"id":"MDyiorgBLXwo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_dataset"],"metadata":{"id":"q6cyhvbFLXzo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_train_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n","    columns=['input_ids','attention_mask', 'label'],\n","    shuffle=True,\n","    batch_size=BATCH_SIZE,\n","    #collate_fn=data_collator\n",")"],"metadata":{"id":"jFssjJ9uLX2x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_val_dataset = tokenized_dataset[\"test\"].to_tf_dataset(\n","    columns=['input_ids','attention_mask', 'label'],\n","    shuffle=True,\n","    batch_size=BATCH_SIZE,\n","    #collate_fn=data_collator\n",")"],"metadata":{"id":"rqKTpYAPLX52"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def swap_positions(dataset):\n","  return {'input_ids':dataset['input_ids'],\n","          'attention_mask':dataset['attention_mask'],},dataset['label']"],"metadata":{"id":"jT7e43WfLX88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_train_dataset=tf_train_dataset.map(swap_positions).prefetch(tf.data.AUTOTUNE)\n","tf_val_dataset=tf_val_dataset.map(swap_positions).prefetch(tf.data.AUTOTUNE)"],"metadata":{"id":"Qbj2qu96LYAG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in tf_train_dataset.take(1):\n","  print(i)"],"metadata":{"id":"5ZtQbxRLLYC8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Data Preparation for XtremeDistill Model"],"metadata":{"id":"djcGDWjOOtST"}},{"cell_type":"code","source":["model_id=\"microsoft/xtremedistil-l6-h256-uncased\"\n","tokenizer = BertTokenizerFast.from_pretrained(model_id)"],"metadata":{"id":"fpexnQRnLYF_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.is_fast"],"metadata":{"id":"eXOOYwROLYJB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples):\n","  return tokenizer(examples[\"text\"],max_length=512,padding=True,truncation=True,)"],"metadata":{"id":"ZlbQ9fv3QXzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_dataset = dataset.map(preprocess_function, batched=True)"],"metadata":{"id":"F6ZEL7qjQX2D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_dataset"],"metadata":{"id":"bStAWY40QX5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_train_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n","    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","    shuffle=True,\n","    batch_size=BATCH_SIZE,\n","    #collate_fn=data_collator\n",")"],"metadata":{"id":"5YW_4quMQX7_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_val_dataset = tokenized_dataset[\"test\"].to_tf_dataset(\n","    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","    shuffle=True,\n","    batch_size=BATCH_SIZE,\n","    #collate_fn=data_collator\n",")"],"metadata":{"id":"IpLPdXS8QX_O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def swap_positions(dataset):\n","  return {'input_ids':dataset['input_ids'],\n","          'token_type_ids':dataset['token_type_ids'],\n","          'attention_mask':dataset['attention_mask'],},dataset['label']"],"metadata":{"id":"9JVHQs6sQYCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_train_dataset=tf_train_dataset.map(swap_positions).prefetch(tf.data.AUTOTUNE)\n","tf_val_dataset=tf_val_dataset.map(swap_positions).prefetch(tf.data.AUTOTUNE)"],"metadata":{"id":"LxxKUQz9QYFO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in tf_val_dataset.take(1):\n","  print(i)"],"metadata":{"id":"tllJsPmbQYIT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_val_dataset"],"metadata":{"id":"KJPA-pMFQYLf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Modeling\n"],"metadata":{"id":"DVd5lgqfQwWW"}},{"cell_type":"markdown","source":["##Model Building With TFBertForSequenceClassification"],"metadata":{"id":"98A-S0MzQ5T6"}},{"cell_type":"code","source":["model=TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=1)\n","model.summary()"],"metadata":{"id":"slSvyKQHQYOi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Model Building With XtremeDistillForSequenceClassification"],"metadata":{"id":"nBbYd0w4Sm7u"}},{"cell_type":"code","source":["model=TFBertForSequenceClassification.from_pretrained(model_id,num_labels=2)\n","model.summary()"],"metadata":{"id":"lmLjes3CQYRd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##TFBertModel"],"metadata":{"id":"7hiUJkJASyOa"}},{"cell_type":"code","source":["model=TFBertModel.from_pretrained(\"bert-base-uncased\")\n","model.summary()"],"metadata":{"id":"HAkugMl3QYUE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids=Input(shape = (512,),dtype=tf.int64,name='input_ids')\n","token_type_ids=Input(shape = (512,),dtype=tf.int64,name='token_type_ids')\n","attention_mask=Input(shape = (512,),dtype=tf.int64,name='attention_mask')\n","\n","x = model([input_ids,token_type_ids,attention_mask])\n","print(x)\n","x=Dense(128,activation='relu')(x[0][:,0,:])\n","output=Dense(1,activation='sigmoid',name='label')(x)\n","\n","custom_bert = tf.keras.Model(inputs=[input_ids,token_type_ids,attention_mask], outputs=output)"],"metadata":{"id":"I2dgeDv9QYW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["custom_bert.summary()"],"metadata":{"id":"rE7joY-iQYaB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Modling with TFRobertaForSequenceClassification"],"metadata":{"id":"DrfsN5N3UO_R"}},{"cell_type":"code","source":["model=TFRobertaForSequenceClassification.from_pretrained(model_id,num_labels=2)\n","model.summary()"],"metadata":{"id":"6GcWSDA1QYdH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Training"],"metadata":{"id":"Vucf0oG0UZvT"}},{"cell_type":"code","source":["num_epochs = 3\n","batches_per_epoch = len(tokenized_dataset[\"train\"]) // BATCH_SIZE\n","total_train_steps = int(batches_per_epoch * num_epochs)"],"metadata":{"id":"0ITY2OYHQYgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer, schedule = create_optimizer(init_lr=2e-5,num_warmup_steps=0, num_train_steps=total_train_steps)"],"metadata":{"id":"1jqyR0iRQYi_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","    optimizer=optimizer,\n","    metrics=['accuracy'],)\n","    #run_eagerly=True)"],"metadata":{"id":"GmaDKMejQYmC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history=model.fit(\n","    tf_train_dataset.take(1000),\n","    validation_data=tf_val_dataset,\n","    epochs=3,)"],"metadata":{"id":"mp3YGto9QYpK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model_loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"CpOr-NgYQYsC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","\n","plt.title('model_accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"zux7R3SzQYvB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Testing"],"metadata":{"id":"bE8rWJhQUxFa"}},{"cell_type":"code","source":["inputs = tokenizer([\"this movie looks very interesting, i love the fact that the actors do a great job in showing how people lived in the 18th century, which wasn't very good at all. But atleast this movie recreates this scenes! \",\n","                    \"very good start, but movie started becoming uninteresting at some point though initially i thought it would have been much more fun. There was too much background noise, but later on towards the middle of the movie, my favorite character got in and he did a great job, so over \"], padding=True,return_tensors=\"tf\")\n","\n","logits = model(**inputs).logits\n","print(logits)\n"],"metadata":{"id":"15HotecHQYyJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Conversion to Onnx Format"],"metadata":{"id":"k9uEQDmgVAGl"}},{"cell_type":"markdown","source":["##Installation"],"metadata":{"id":"4kCdxDipVEUB"}},{"cell_type":"code","source":["!pip install -U tf2onnx\n","!pip install onnxruntime"],"metadata":{"id":"Wz1nLB2eQY1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import onnxruntime as rt\n","import tf2onnx\n","rt.get_device()"],"metadata":{"id":"l0Pn9jdDQY4X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##From Keras Model"],"metadata":{"id":"3Tmg5bxsVLAs"}},{"cell_type":"code","source":["output_path = \"/content/drive/MyDrive/NLP Repository/Projects/Sentiments_analysis/xtremedistill.onnx\""],"metadata":{"id":"1kf_YfDtQY7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spec = [tf.TensorSpec((None,512),tf.int64, name=\"input_ids\"),\n","        tf.TensorSpec((None,512),tf.int64, name=\"token_type_ids\"),\n","        tf.TensorSpec((None,512),tf.int64, name=\"attention_mask\")]\n","\n","model_proto, _ = tf2onnx.convert.from_keras(\n","    model, input_signature=spec,\n","    opset=17, output_path=output_path,)\n","output_names = [n.name for n in model_proto.graph.output]"],"metadata":{"id":"If5V2fynQY-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(output_names)"],"metadata":{"id":"1Emi__eyQZBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Inference"],"metadata":{"id":"lLvrB3VNVvzH"}},{"cell_type":"markdown","source":["###Benchmarking Onnx"],"metadata":{"id":"mhxSyiQCV1Qi"}},{"cell_type":"code","source":["text=[\"this movie looks very interesting, i love the fact that the actors do a great job in showing how people lived in the 18th century, which wasn't very good at all. But atleast this movie recreates this scenes!\"]\n","\n","# text = [\"this movie looks very interesting, i love the fact that the actors do a great job in showing how people lived in the 18th century, which wasn't very good at all. But atleast this movie recreates this scenes! \",\n","#                     \"very good start, but movie started becoming uninteresting at some point though initially i thought it would have been much more fun. There was too much background noise, but later on towards the middle of the movie, my favorite character got in and he did a great job, so over \",\n","#                     \"very good start, but movie started becoming uninteresting at some point though initially i thought it would have been much more fun. There was too much background noise, but later on towards the middle of the movie, my favorite character got in and he did a great job, so overall i will give this movie a pass \"]\n","\n","\n","inputs = tokenizer(text,padding='max_length',max_length=512,truncation=True,return_tensors=\"np\")\n","\n","N_PREDICTIONS = 1\n","print(inputs)"],"metadata":{"id":"47oQCf65QZEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["providers=['CPUExecutionProvider']\n","m = rt.InferenceSession(output_path, providers=providers)\n","\n","t1 = time.time()\n","for _ in range(N_PREDICTIONS):\n","  onnx_pred = m.run([\"logits\"], {'input_ids':inputs['input_ids'],\n","                                'token_type_ids':inputs['token_type_ids'],\n","                                'attention_mask':inputs['attention_mask']})\n","print(\"Time for a single Prediction\", (time.time() - t1)/N_PREDICTIONS)"],"metadata":{"id":"PqAykX79QZHc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(onnx_pred)"],"metadata":{"id":"0_UqAGdfQZKd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Benchmarking TF"],"metadata":{"id":"cOFqCbzOV-e5"}},{"cell_type":"code","source":["t1 = time.time()\n","for _ in range(N_PREDICTIONS):\n","  logits = model(**inputs).logits\n","print(logits)\n","print(\"Time for a single Prediction\", (time.time() - t1)/N_PREDICTIONS)"],"metadata":{"id":"Yc4r1SqJQZNd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Quantization with Onnx"],"metadata":{"id":"BZf56hshWP6m"}},{"cell_type":"code","source":["import onnx\n","from onnxruntime.quantization import quantize_dynamic, QuantType"],"metadata":{"id":"7IacFq47QZQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_fp32 = '/content/drive/MyDrive/NLP Repository/Projects/Sentiments_analysis/xtremedistill.onnx'\n","model_quant = '/content/drive/MyDrive/NLP Repository/Projects/Sentiments_analysis/xtremedistill_quantized.onnx'"],"metadata":{"id":"jgsTVXV-QZUb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type = QuantType.QUInt8)"],"metadata":{"id":"a5NKnFgQLYMR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Accuracy Drop due to Quantization"],"metadata":{"id":"z9iWI0UaWiY3"}},{"cell_type":"code","source":["unbatched_val_dataset=tf_val_dataset.unbatch()"],"metadata":{"id":"qG0388xaLYPN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["N_SAMPLES=1024"],"metadata":{"id":"mRMjdqZ0LYSI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def accuracy(model):\n","  total=0\n","  for text,label in unbatched_val_dataset.take(N_SAMPLES):\n","\n","    onnx_pred = model.run([\"logits\"], {'input_ids':[text['input_ids'].numpy()],\n","                                'token_type_ids':[text['token_type_ids'].numpy()],\n","                                'attention_mask':[text['attention_mask'].numpy()]})\n","    if np.argmax(onnx_pred, axis = -1)[0][0] == label.numpy():\n","      total+=1\n","  return (total/N_SAMPLES)*100"],"metadata":{"id":"QB-xcbuCLYVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["providers=['CPUExecutionProvider']\n","m = rt.InferenceSession(model_fp32, providers=providers)\n","m_q = rt.InferenceSession(model_quant, providers=providers)\n","print(accuracy(m_q))\n","print(accuracy(m))"],"metadata":{"id":"KWo8ItcdLYYo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Temperature in Distillation"],"metadata":{"id":"AmqWZTzxWxIV"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"hZitKtqPWtNF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def softmax(logits,T):\n","  denominator=np.sum([np.exp(i/T) for i in logits])\n","  return [np.exp(i/T)/denominator for i in logits]"],"metadata":{"id":"VypPwdE2WtQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logits=[10,13,17,5]\n"],"metadata":{"id":"kS-Wn1TXWtTD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"For T=1 ------>\",softmax(logits,1))\n","print(\"For T=2 ------>\",softmax(logits,2))\n","print(\"For T=3 ------>\",softmax(logits,3))\n","print(\"For T=5 ------>\",softmax(logits,5))\n","print(\"For T=10 ----->\",softmax(logits,10))\n","print(\"For T=10000 -->\",softmax(logits,10000))"],"metadata":{"id":"Y81vcqFaWtWd"},"execution_count":null,"outputs":[]}]}