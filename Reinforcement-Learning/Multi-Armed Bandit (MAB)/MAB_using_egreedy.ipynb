{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOpFVahOKU5Ol9l+qBJUsqJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"BlVTmENiB4JY"}},{"cell_type":"code","source":["# Import statements\n","import numpy as np"],"metadata":{"id":"n1pSRAzMyCBL","executionInfo":{"status":"ok","timestamp":1732210397088,"user_tz":-180,"elapsed":238,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def epsilon_greedy(num_arms, num_plays, epsilon, true_reward_probs):\n","  Q = np.zeros(num_arms)\n","  N = np.zeros(num_arms)\n","  rewards = []\n","\n","  def pull_arm(arm):\n","    return np.random.rand() < true_reward_probs[arm]\n","\n","  for t in range(num_plays):\n","    if np.random.rand() < epsilon:\n","      arm = np.random.choice(num_arms)\n","    else:\n","      arm = np.argmax(Q)\n","\n","    reward = pull_arm(arm)\n","    rewards.append(reward)\n","\n","    N[arm] += 1\n","    Q[arm] += (reward - Q[arm]) / N[arm]\n","\n","  return Q, N, rewards\n","\n"],"metadata":{"id":"N_8O5Hy5yCEi","executionInfo":{"status":"ok","timestamp":1732211493296,"user_tz":-180,"elapsed":263,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Defining the parameters\n","num_arms = 3\n","num_plays = 1000\n","epsilon = 0.1\n","true_reward_probs = [0.2, 0.5, 0.7]\n","num_simulations = 1000"],"metadata":{"id":"IrauMqL5yCRe","executionInfo":{"status":"ok","timestamp":1732211496453,"user_tz":-180,"elapsed":316,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Running multiple simulations\n","all_Q = np.zeros((num_simulations, num_arms))\n","all_N = np.zeros((num_simulations, num_arms))\n","all_rewards = []\n","\n","for i in range(num_simulations):\n","  Q, N, rewards = epsilon_greedy(num_arms, num_plays, epsilon, true_reward_probs)\n","  all_Q[i] = all_N[i] = N\n","  all_rewards.append(sum(rewards))\n","\n","# Getting the average results\n","avg_Q = np.mean(all_Q, axis=0)\n","avg_N = np.mean(all_N, axis=0)\n","avg_cumulative_reward = np.mean(all_rewards)\n","best_arm_reward = max(true_reward_probs) * num_plays\n","avg_regret = best_arm_reward - avg_cumulative_reward\n","\n","# Printting the results\n","print(\"Average Estimated values:\", avg_Q)\n","print(\"Average Counts :\", avg_N)\n","print(\"Average Cumulative Reward:\", avg_cumulative_reward)\n","print(\"Average  Regret:\", avg_regret)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_F9wNs-nyCUM","executionInfo":{"status":"ok","timestamp":1732211503821,"user_tz":-180,"elapsed":6329,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}},"outputId":"63200958-7f91-49fc-9c60-156f637f7650"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Estimated values: [ 57.453  78.752 863.795]\n","Average Counts : [ 57.453  78.752 863.795]\n","Average Cumulative Reward: 655.546\n","Average  Regret: 44.45399999999995\n"]}]},{"cell_type":"markdown","source":["This code implements the **epsilon-greedy algorithm** for the multi-armed bandit (MAB) problem, simulating and analyzing its performance over multiple runs. Here's a detailed explanation of the code:\n","\n","---\n","\n","### **Step-by-Step Explanation**\n","\n","#### **1. Import Statements**\n","```python\n","import numpy as np\n","```\n","- **`numpy`** is used for numerical operations, including random number generation and array manipulations.\n","\n","---\n","\n","#### **2. Function: `epsilon_greedy`**\n","```python\n","def epsilon_greedy(num_arms, num_plays, epsilon, true_reward_probs):\n","    Q = np.zeros(num_arms)\n","    N = np.zeros(num_arms)\n","    rewards = []\n","```\n","- **Inputs:**\n","  - `num_arms`: Number of arms (actions or slot machines).\n","  - `num_plays`: Total number of times we interact with the arms.\n","  - `epsilon`: The probability of exploration (choosing a random arm).\n","  - `true_reward_probs`: The actual probabilities of getting a reward for each arm.\n","- **Variables:**\n","  - `Q`: Estimated values of each arm (initialized to 0).\n","  - `N`: Number of times each arm has been pulled (initialized to 0).\n","  - `rewards`: List to track all rewards obtained during the plays.\n","\n","```python\n","    def pull_arm(arm):\n","        return np.random.rand() < true_reward_probs[arm]\n","```\n","- **`pull_arm(arm)`**:\n","  - Simulates pulling an arm.\n","  - Generates a random number between 0 and 1 (`np.random.rand()`).\n","  - Returns `True` (reward) if the number is less than the true probability of the arm.\n","\n","---\n","\n","#### **3. Main Loop: Playing the Game**\n","```python\n","    for t in range(num_plays):\n","        if np.random.rand() < epsilon:\n","            arm = np.random.choice(num_arms)\n","        else:\n","            arm = np.argmax(Q)\n","```\n","- At each play `t`:\n","  - With probability `epsilon`, **explore** by randomly selecting an arm (`np.random.choice(num_arms)`).\n","  - Otherwise, **exploit** by selecting the arm with the highest estimated value (`np.argmax(Q)`).\n","\n","```python\n","        reward = pull_arm(arm)\n","        rewards.append(reward)\n","```\n","- Simulate pulling the selected arm and record the obtained reward.\n","\n","```python\n","        N[arm] += 1\n","        Q[arm] += (reward - Q[arm]) / N[arm]\n","```\n","- **Update the statistics**:\n","  - Increment the count of pulls for the chosen arm (`N[arm]`).\n","  - Update the estimated value `Q[arm]` using the formula for the running mean:\n","    \\[\n","    Q[arm] = Q[arm] + \\frac{(\\text{reward} - Q[arm])}{N[arm]}\n","    \\]\n","\n","---\n","\n","#### **4. Returning Results**\n","```python\n","    return Q, N, rewards\n","```\n","- Returns:\n","  - `Q`: Final estimated values of each arm.\n","  - `N`: Total number of times each arm was pulled.\n","  - `rewards`: List of all rewards obtained.\n","\n","---\n","\n","### **5. Simulating the Algorithm**\n","```python\n","num_arms = 3\n","num_plays = 1000\n","epsilon = 0.1\n","true_reward_probs = [0.2, 0.5, 0.7]\n","num_simulations = 1000\n","```\n","- Parameters:\n","  - 3 arms with reward probabilities `[0.2, 0.5, 0.7]`.\n","  - Play each simulation for 1000 rounds.\n","  - Use `epsilon = 0.1` (10% exploration).\n","  - Run the algorithm for `num_simulations = 1000` independent simulations.\n","\n","```python\n","all_Q = np.zeros((num_simulations, num_arms))\n","all_N = np.zeros((num_simulations, num_arms))\n","all_rewards = []\n","```\n","- Arrays to store results of all simulations:\n","  - `all_Q`: Stores estimated values (`Q`) for each simulation.\n","  - `all_N`: Stores counts of arm pulls for each simulation.\n","  - `all_rewards`: Stores total rewards for each simulation.\n","\n","```python\n","for i in range(num_simulations):\n","    Q, N, rewards = epsilon_greedy(num_arms, num_plays, epsilon, true_reward_probs)\n","    all_Q[i] = Q\n","    all_N[i] = N\n","    all_rewards.append(sum(rewards))\n","```\n","- Run `epsilon_greedy` for each simulation.\n","- Record:\n","  - Estimated values (`Q`).\n","  - Number of pulls (`N`).\n","  - Total rewards (`sum(rewards)`).\n","\n","---\n","\n","### **6. Analyzing Results**\n","```python\n","avg_Q = np.mean(all_Q, axis=0)\n","avg_N = np.mean(all_N, axis=0)\n","avg_cumulative_reward = np.mean(all_rewards)\n","```\n","- Compute averages across simulations:\n","  - `avg_Q`: Average estimated values of arms.\n","  - `avg_N`: Average number of times each arm was pulled.\n","  - `avg_cumulative_reward`: Average cumulative reward across all simulations.\n","\n","```python\n","best_arm_reward = max(true_reward_probs) * num_plays\n","avg_regret = best_arm_reward - avg_cumulative_reward\n","```\n","- **Best possible reward**: If the best arm (with the highest reward probability) was always selected:\n","  \\[\n","  \\text{best\\_arm\\_reward} = \\text{max(true\\_reward\\_probs)} \\times \\text{num\\_plays}\n","  \\]\n","- **Regret**: Difference between the best possible reward and the actual average cumulative reward.\n","\n","---\n","\n","### **7. Printing Results**\n","```python\n","print(\"Average Estimated values:\", avg_Q)\n","print(\"Average Counts :\", avg_N)\n","print(\"Average Cumulative Reward:\", avg_cumulative_reward)\n","print(\"Average  Regret:\", avg_regret)\n","```\n","- Outputs key metrics for analysis:\n","  - Average estimated values for each arm (`avg_Q`).\n","  - Average number of times each arm was pulled (`avg_N`).\n","  - Average total reward across all simulations.\n","  - Average regret (loss due to not always selecting the best arm).\n","\n","---\n","\n","### **Key Insights**\n","- The **epsilon-greedy algorithm** balances exploration and exploitation.\n","- Over time, it learns to focus on the best arm while occasionally exploring other arms.\n","- Regret decreases as the algorithm gains more information about the arms' true reward probabilities."],"metadata":{"id":"-FgU5vldGvlk"}},{"cell_type":"code","source":["# # Import statements\n","# import numpy as np\n","\n","# def epsilon_greedy(num_arms, num_plays, epsilon, true_reward_probs):\n","#   Q = np.zeros(num_arms)\n","#   N = np.zeros(num_arms)\n","#   rewards = []\n","\n","#   def pull_arm(arm):\n","#     return np.random.rand() < true_reward_probs[arm]\n","\n","#   for t in range(num_plays):\n","#     if np.random.rand() < epsilon:\n","#       arm = np.random.choice(num_arms)\n","#     else:\n","#       arm = np.argmax(Q)\n","\n","#     reward = pull_arm(arm)\n","#     rewards.append(reward)\n","\n","#     N[arm] += 1\n","#     Q[arm] += (reward - Q[arm]) / N[arm]\n","\n","#   return Q, N, rewards\n","\n","# # Defining the parameters\n","# num_arms = 3\n","# num_plays = 1000\n","# epsilon = 0.1\n","# true_reward_probs = [0.2, 0.5, 0.7]\n","# num_simulations = 1000\n","\n","# # Running multiple simulations\n","# all_Q = np.zeros((num_simulations, num_arms))\n","# all_N = np.zeros((num_simulations, num_arms))\n","# all_rewards = []\n","\n","# for i in range(num_simulations):\n","#   Q, N, rewards = epsilon_greedy(num_arms, num_plays, epsilon, true_reward_probs)\n","#   all_Q[i] = all_N[i] = N\n","#   all_rewards.append(sum(rewards))\n","\n","# # Getting the average results\n","# avg_Q = np.mean(all_Q, axis=0)\n","# avg_N = np.mean(all_N, axis=0)\n","# avg_cumulative_reward = np.mean(all_rewards)\n","# best_arm_reward = max(true_reward_probs) * num_plays\n","# avg_regret = best_arm_reward - avg_cumulative_reward\n","\n","# # Printting the results\n","# print(\"Average Estimated values:\", avg_Q)\n","# print(\"Average Counts :\", avg_N)\n","# print(\"Average Cumulative Reward:\", avg_cumulative_reward)\n","# print(\"Average  Regret:\", avg_regret)\n"],"metadata":{"id":"xE-aJSGT8ZWG","executionInfo":{"status":"ok","timestamp":1732215126441,"user_tz":-180,"elapsed":243,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Other way of implementing it :"],"metadata":{"id":"S7hwxqXWD69r"}},{"cell_type":"code","source":["np.zeros(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uPgO1zpEEd-E","executionInfo":{"status":"ok","timestamp":1732215178409,"user_tz":-180,"elapsed":259,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}},"outputId":"1ea3cbf1-0ad5-4d9b-843e-0e6265212439"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0.])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Define number of arms and number of trials\n","n_arms = 5\n","n_trials = 1000\n","epsilon = 0.1  # Exploration probability\n","\n","# Initialize estimates and action counts\n","Q = np.zeros(n_arms)  # Estimated rewards for each arm\n","N = np.zeros(n_arms)  # Number of times each arm is pulled\n","\n","# True reward probabilities for each arm (unknown to the agent)\n","true_probs = np.random.rand(n_arms)\n","\n","# Simulation of trials\n","for t in range(1, n_trials + 1):\n","    # Choose action using epsilon-greedy\n","    if np.random.rand() < epsilon:\n","        action = np.random.randint(0, n_arms)  # Explore\n","    else:\n","        action = np.argmax(Q)  # Exploit\n","\n","    # Simulate pulling the arm and receiving reward\n","    reward = np.random.rand() < true_probs[action]\n","\n","    # Update counts and action value estimates\n","    N[action] += 1\n","    Q[action] += (reward - Q[action]) / N[action]  # Incremental update\n","\n","# Results\n","print(\"Estimated action values (Q):\", Q)\n","print(\"True probabilities of arms:\", true_probs)\n","print(\"Number of times each arm was pulled:\", N)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ni9mmi-42G0u","executionInfo":{"status":"ok","timestamp":1732215040855,"user_tz":-180,"elapsed":253,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}},"outputId":"09c38dbc-0c13-4d1b-c3c5-4e6b71e75b6a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Estimated action values (Q): [0.77826564 0.5625     0.36363636 0.06666667 0.66666667]\n","True probabilities of arms: [0.77150968 0.54069643 0.38195034 0.15768095 0.42852121]\n","Number of times each arm was pulled: [911.  16.  22.  30.  21.]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gwhDfm6OD-kA"},"execution_count":null,"outputs":[]}]}