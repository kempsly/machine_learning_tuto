{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFVytqo3yZvRPHVM5miLyp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"b3Ne5DL4JIGo"},"outputs":[],"source":[]},{"cell_type":"code","source":["# import statements\n","import numpy as np"],"metadata":{"id":"a6zVNIwvJQfJ","executionInfo":{"status":"ok","timestamp":1732283598680,"user_tz":-180,"elapsed":253,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Defining thw MDP"],"metadata":{"id":"fDYIOZXQJQh3","executionInfo":{"status":"ok","timestamp":1732283618998,"user_tz":-180,"elapsed":248,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Example of states\n","states = [0, 1, 2, 3]\n","#Example of actions\n","actions = ['left', 'right']\n","\n","# Defining the rewards\n","rewards = np.array([\n","    [-1, 0], # Rewards for state 0\n","    [0, 1], # Rewards for state 1\n","    [1, -1], # Rewards for state 2\n","    [0, 0] # Rewards for state 3(terminal state)\n","])\n","\n","# Defining the transition matrix\n","transitions = np.array([\n","    [[0.7, 0.3, 0, 0], [0.1, 0.9, 0, 0]], # Transition probabilities for state 0\n","    [[0.8, 0, 0.2, 0], [0.4, 0.5, 0, 0.1]], #Transition probabilitie for state 1\n","    [[0.6, 0.2, 0, 0.2], [0.5, 0.5, 0, 0]], # Transition probabilities for state 2\n","    [[0, 0, 0, 1], [0, 0, 0, 1]] # Transition probabilities for state 3(Terminal state)\n","])\n","#Defining the discount factor\n","gamma = 0.9\n","#Threshold for convergence\n","theta = 1e-6"],"metadata":{"id":"eg-Xxn3nJQkv","executionInfo":{"status":"ok","timestamp":1732284457614,"user_tz":-180,"elapsed":366,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def value_iteration(states, actions, rewards, transitions, gamma, theta):\n","  #Initialize the value function to zeros\n","  V = np.zeros(len(states))\n","  #Initialize the policy\n","  policy = np.zeros(len(states), dtype=int)\n","\n","  while True:\n","    #change in value function\n","    delta = 0\n","    for s in states:\n","      action_values = []\n","      for a in range(len(actions)):\n","        action_value =  sum(transitions[s, a, s_prime] * (rewards[s, a] + gamma * V[s_prime]) for s_prime in states)\n","        action_values.append(action_value)\n","\n","      max_action_value = max(action_values)\n","      delta = max(delta, abs(max_action_value - V[s]))\n","      V[s] = max_action_value\n","      policy[s] = np.argmax(action_values)\n","\n","    if delta < theta:\n","      break\n","\n","  return policy, V"],"metadata":{"id":"a11MEd9bJQno","executionInfo":{"status":"ok","timestamp":1732285268487,"user_tz":-180,"elapsed":287,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#Solve the MDP using Value Iteration\n","optimal_policy, optimal_value = value_iteration(states, actions, rewards, transitions, gamma, theta)\n"],"metadata":{"id":"ENSvMA6wJQq8","executionInfo":{"status":"ok","timestamp":1732285356838,"user_tz":-180,"elapsed":244,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Display the results\n","print(\"Optimal Value Function:\")\n","print(optimal_value)\n","print(\"Optimal Policy (action index):\")\n","print(optimal_policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kTAhJqBlJQ1d","executionInfo":{"status":"ok","timestamp":1732285392599,"user_tz":-180,"elapsed":233,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}},"outputId":"8e0736fc-a492-42cc-ef51-8bf3b14dd9fd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Value Function:\n","[3.87744984 4.3561483  3.87792961 0.        ]\n","Optimal Policy (action index):\n","[1 1 0 0]\n"]}]},{"cell_type":"code","source":["# Map policy indices to action names\n","policy_actions = [actions[a] for a in optimal_policy]\n","print(\"Optimal Policy (actions):\")\n","print(policy_actions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gV_lM9XJQ4V","executionInfo":{"status":"ok","timestamp":1732285414169,"user_tz":-180,"elapsed":211,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}},"outputId":"35747d67-afc1-49ca-b891-c242f5807616"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Policy (actions):\n","['right', 'right', 'left', 'left']\n"]}]},{"cell_type":"markdown","source":["This script demonstrates the implementation of the **Value Iteration** algorithm to solve a simple **Markov Decision Process (MDP)**. Below is a detailed explanation of the code and its key concepts:\n","\n","---\n","\n","### **1. Defining the MDP Components**\n","\n","1. **States**:  \n","   - Represented as a list of integers `[0, 1, 2, 3]`. These are the states in the MDP.\n","\n","2. **Actions**:  \n","   - Two possible actions, `'left'` and `'right'`.\n","\n","3. **Rewards**:  \n","   - A 2D numpy array representing the reward for taking each action in each state.  \n","     Example: `rewards[0] = [-1, 0]` means:\n","     - Taking `'left'` in state 0 gives a reward of \\(-1\\).\n","     - Taking `'right'` in state 0 gives a reward of \\(0\\).\n","\n","4. **Transitions**:  \n","   - A 3D numpy array where `transitions[s, a, s_prime]` represents the probability of transitioning to state \\(s'\\) from state \\(s\\) after taking action \\(a\\).  \n","     Example: `transitions[0, 0] = [0.7, 0.3, 0, 0]` means:\n","     - From state 0, taking `'left'` transitions to:\n","       - State 0 with \\(70\\%\\) probability.\n","       - State 1 with \\(30\\%\\) probability.\n","\n","5. **Discount Factor (\\(\\gamma\\))**:  \n","   - Determines how much future rewards are valued compared to immediate rewards. Here, \\(\\gamma = 0.9\\), which values future rewards almost as much as immediate ones.\n","\n","6. **Threshold (\\(\\theta\\))**:  \n","   - A small value \\(1e-6\\) to determine convergence. The algorithm stops when the change in the value function is smaller than this threshold.\n","\n","---\n","\n","### **2. Value Iteration Algorithm**\n","\n","#### **Goal**  \n","To find the **optimal policy** (a mapping from states to actions) and the corresponding **value function** (maximum expected cumulative rewards for each state).\n","\n","#### **Steps**  \n","\n","1. **Initialize Values**:  \n","   - `V`: Value function for each state, initially set to \\(0\\).  \n","   - `policy`: The optimal action for each state, initialized to \\(0\\) (arbitrary action index).\n","\n","2. **Iterate Until Convergence**:\n","   - For each state \\(s\\):\n","     - Compute the **action-value** for each action \\(a\\):\n","       \\[\n","       Q(s, a) = \\sum_{s'} P(s'|s, a) \\cdot \\left( R(s, a) + \\gamma \\cdot V(s') \\right)\n","       \\]\n","       where:\n","       - \\(P(s'|s, a)\\) is the transition probability.\n","       - \\(R(s, a)\\) is the immediate reward.\n","       - \\(V(s')\\) is the value of the next state.\n","     - Find the **maximum action-value**:\n","       \\[\n","       V(s) = \\max_a Q(s, a)\n","       \\]\n","     - Update the **policy** to select the action with the highest action-value.\n","\n","   - Track the maximum change in the value function (\\(\\Delta\\)) during this iteration:\n","     \\[\n","     \\Delta = \\max |V_{\\text{new}}(s) - V_{\\text{old}}(s)|\n","     \\]\n","   - Stop when \\(\\Delta < \\theta\\).\n","\n","3. **Return Optimal Policy and Value Function**:\n","   - `policy`: The optimal action for each state.\n","   - `V`: The optimal value function.\n","\n","---\n","\n","### **3. Results**\n","\n","1. **Optimal Value Function**:  \n","   The maximum expected cumulative rewards for each state.\n","\n","2. **Optimal Policy (Indices)**:  \n","   The action indices (`0` for `'left'` and `1` for `'right'`) that achieve the optimal value.\n","\n","3. **Optimal Policy (Actions)**:  \n","   Mapped action names for clarity.\n","\n","---\n","\n","### **Example Output**\n","\n","Given the defined MDP:\n","\n","```plaintext\n","Optimal Value Function:\n","[...values for each state...]\n","Optimal Policy (action index):\n","[...indices...]\n","Optimal Policy (actions):\n","['right', 'right', 'left', 'left']\n","```\n","\n","This means, for example:\n","- In state 0, the optimal action is `'right'`.\n","- In state 2, the optimal action is `'left'`.\n","\n","---\n","\n","### **Key Takeaways**\n","- Value Iteration efficiently computes the optimal policy and value function for small to medium-sized MDPs.\n","- It requires explicit definitions of rewards and transition probabilities, making it suitable for problems with known dynamics.\n","- Larger state-action spaces may require more scalable methods like Approximate Dynamic Programming or Reinforcement Learning.\n"],"metadata":{"id":"vqWXMdABQ-dq"}},{"cell_type":"code","source":[],"metadata":{"id":"-S-etSJXJQ7N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The output of the Value Iteration algorithm provides the **Optimal Value Function** and **Optimal Policy** for the given Markov Decision Process (MDP).\n","\n","---\n","\n","### **1. Optimal Value Function**\n","\n","\\[\n","[3.87744984, 4.3561483, 3.87792961, 0.0]\n","\\]\n","\n","- Each value represents the maximum expected cumulative reward for being in a particular state and following the optimal policy thereafter.\n","- State-wise values:\n","  - **State 0**: \\(3.877\\) (expected reward when starting in state 0).\n","  - **State 1**: \\(4.356\\) (highest value, meaning this state is relatively more rewarding).\n","  - **State 2**: \\(3.878\\) (similar to state 0).\n","  - **State 3**: \\(0.0\\) (terminal state with no future rewards).\n","\n","---\n","\n","### **2. Optimal Policy (Action Index)**\n","\n","\\[\n","[1, 1, 0, 0]\n","\\]\n","\n","- The policy shows the optimal action for each state using action indices:\n","  - **Action 1** corresponds to `'right'`.\n","  - **Action 0** corresponds to `'left'`.\n","- State-wise optimal actions:\n","  - **State 0**: Action \\(1\\) (go `'right'`).\n","  - **State 1**: Action \\(1\\) (go `'right'`).\n","  - **State 2**: Action \\(0\\) (go `'left'`).\n","  - **State 3**: Action \\(0\\) (go `'left'`), though this state is terminal.\n","\n","---\n","\n","### **3. Optimal Policy (Actions)**\n","\n","\\[\n","['right', 'right', 'left', 'left']\n","\\]\n","\n","- Maps the action indices to their corresponding names:\n","  - **State 0**: `'right'` is optimal.\n","  - **State 1**: `'right'` is optimal.\n","  - **State 2**: `'left'` is optimal.\n","  - **State 3**: `'left'` is optimal, although this state is terminal.\n","\n","---\n","\n","### **Interpretation**\n","- The **Optimal Value Function** provides a numerical assessment of each state's desirability under the optimal policy.\n","- The **Optimal Policy** tells us which action to take in each state to maximize rewards.\n","\n","### **Insights**\n","1. **Terminal State (State 3)**:\n","   - Has a value of \\(0.0\\) since no future rewards are obtainable after reaching it.\n","\n","2. **Preference for Actions**:\n","   - States \\(0\\) and \\(1\\) favor moving `'right'`, suggesting that higher rewards or favorable transitions lie in that direction.\n","   - State \\(2\\) prefers `'left'`, indicating better outcomes by transitioning in that direction.\n","\n","---\n","\n"],"metadata":{"id":"xNdn0pcNRTx5"}},{"cell_type":"code","source":[],"metadata":{"id":"E1oWmYdkJQ-w"},"execution_count":null,"outputs":[]}]}