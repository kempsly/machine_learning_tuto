{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNaM7ZlcJJDohP0c4R9eYfV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"gepyvw-cjhCt","executionInfo":{"status":"ok","timestamp":1732307268572,"user_tz":-180,"elapsed":287,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"outputs":[],"source":["#import statements\n","import numpy as np"]},{"cell_type":"code","source":["#Defining the gridworld class\n"],"metadata":{"id":"tGqcqcOOjsQx","executionInfo":{"status":"ok","timestamp":1732307290205,"user_tz":-180,"elapsed":304,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class GridworldMDP:\n","\n","  def __init__(self, grid_size=4, gamma=0.9):\n","    # Grid dimensions (e.g., 4x4 grid)\n","    self.grid_size = grid_size\n","    #All states\n","    self.states = [(i, j) for i in range(grid_size) for j in range(grid_size)]\n","    #Possible actions that the agent can take\n","    self.actions = ['up', 'down', 'left', 'right']\n","    #The discount factor\n","    self.gamma = gamma\n","    #Reward function\n","    self.rewards = self.create_rewards()\n","    # Transition probabilities\n","    self.transition_probs = self.create_transition_probs()\n","\n","  def create_rewards(self):\n","    \"\"\" This function defines the reward structure.\"\"\"\n","    rewards = {}\n","    for state in self.states:\n","      #Setting a default reward for each step\n","      rewards[state] = -1\n","    #Goal state (positive reward)\n","    rewards[(0, self.grid_size - 1)] = 10\n","    #Defining the trap state (negative reward)\n","    rewards[(2, 2)] = -10\n","\n","    return rewards\n","\n","  def create_transition_probs(self):\n","    \"\"\"Create transition probabilities for all actions.\"\"\"\n","    transition_probs = {}\n","    for state in self.states:\n","      transition_probs[state] = {}\n","      for action in self.actions:\n","        transition_probs[state][action] = self.get_next_state_probs(state, action)\n","    return transition_probs\n","\n","  def get_next_state_probs(self, state, action):\n","    \"\"\"\"Get the possible next states and their probabilities.\"\"\"\n","    i, j = state\n","    if state == (0, self.grid_size - 1): #Goal state\n","      return {(0, self.grid_size -1): 1.0}\n","    if action == 'up':\n","      next_state = (max(i - 1, 0), j)\n","    elif action == 'down':\n","      next_state = (min(i + 1, self.grid_size - 1), j)\n","    elif action == 'left':\n","      next_state = (i, max(j - 1,  0))\n","    elif action == 'right':\n","      next_state = (i, min(j + 1, self.grid_size - 1))\n","\n","    return {next_state: 1.0}\n","\n","\n","  def value_iteration(self, theta=1e-3):\n","    \"\"\"Perform value iteration to find the optimal value function and policy.\"\"\"\n","    #Initialize the value function\n","    V = {state: 0 for state in self.states}\n","    #Random initial policy\n","    policy = {state: np.random.choice(self.actions) for state in self.states}\n","\n","    while True:\n","      delta = 0\n","      for state in self.states:\n","        old_value = V[state]\n","        new_value = self.compute_value_for_state(state, V)\n","        V[state] = new_value\n","        delta = max(delta, abs(old_value - new_value))\n","\n","      if delta < theta:\n","        break\n","    for state in self.states:\n","      policy[state] = self.compute_optimal_action_for_state(state, V)\n","\n","    return V, policy\n","\n","  def compute_value_for_state(self, state, V):\n","    \"\"\"Compute the value for a given state by considering all possible actions.\"\"\"\n","    action_values = []\n","    for action in self.actions:\n","      expected_value = 0\n","      for next_state, prob in self.transition_probs[state][action].items():\n","        expected_value += prob * (self.rewards[next_state] + self.gamma * V[next_state])\n","        action_values.append(expected_value)\n","\n","    #Return the best possible value for this state\n","    return max(action_values)\n","\n","  def compute_optimal_action_for_state(self, state, V):\n","    \"\"\"Compute the optimal action for a state given the value function.\"\"\"\n","    action_values = {}\n","    for action in self.actions:\n","      epected_value = 0\n","      for next_state, prob in self.transition_probs[state][action].items():\n","        expected_value += prob * (self.rewards[next_state] + self.gamma * V[next_state])\n","        action_values[action] = expected_value\n","\n","    #Return the action with the highest epected value\n","    return max(action_values, key=action_values.get)\n","\n","  def print_policy(self, policy):\n","    \"\"\"Print the optimal policy in a readable format.\"\"\"\n","    grid_policy = np.zeros((self.grid_size, self.grid_size), dtype=str)\n","    for state, actiom in policy.items():\n","      i, j = state\n","      #Showing the first letter of each action\n","      grid_policy[i, j] = action[0]\n","\n","    print(\"Optimal Policy (first letter of action):\")\n","    for row in grid_policy:\n","      print(' '.join(row))\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"83q36xFGjsTs","executionInfo":{"status":"ok","timestamp":1732310127155,"user_tz":-180,"elapsed":298,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["#CReate the Gridworld MDP\n","gridworld = GridworldMDP(grid_size=4, gamma=0.9)\n","\n","# Solve MDP using value Iteration\n","optimal_values, optimal_policy = gridworld.value_iteration()\n","# Print the results\n","print(\"Optimal Value Function:\")\n","for state, value in sorted(optimal_values.items()):\n","    print(f\"State {state}: {value:.2f}\")\n","\n","print(\"\\nOptimal Policy:\")\n","gridworld.print_policy(optimal_policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pFAUzVSvjsW4","executionInfo":{"status":"ok","timestamp":1732310440741,"user_tz":-180,"elapsed":283,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}},"outputId":"e3898453-d388-4f00-8f54-6cdc51a2ddfe"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Value Function:\n","State (0, 0): 79.09\n","State (0, 1): 88.99\n","State (0, 2): 99.99\n","State (0, 3): 99.99\n","State (1, 0): 70.18\n","State (1, 1): 79.09\n","State (1, 2): 88.99\n","State (1, 3): 99.99\n","State (2, 0): 62.16\n","State (2, 1): 70.18\n","State (2, 2): 79.09\n","State (2, 3): 88.99\n","State (3, 0): 54.95\n","State (3, 1): 62.16\n","State (3, 2): 70.18\n","State (3, 3): 79.09\n","\n","Optimal Policy:\n","Optimal Policy (first letter of action):\n","r r r u\n","r r r u\n","r u r u\n","r r r u\n"]}]},{"cell_type":"code","source":["print(gridworld)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eqly6mlVjsZp","executionInfo":{"status":"ok","timestamp":1732310129757,"user_tz":-180,"elapsed":375,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}},"outputId":"17133222-191c-4e00-981e-62ab553775a4"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.GridworldMDP object at 0x7ad7f24bffd0>\n"]}]},{"cell_type":"markdown","source":["The code implements a **Gridworld Markov Decision Process (MDP)** and solves it using **Value Iteration**.\n","---\n","\n","### **1. What is Gridworld?**\n","- **Gridworld** is a simple environment used to test reinforcement learning algorithms.\n","- It consists of a grid of states where an agent starts at some position and moves around the grid by taking actions (`up`, `down`, `left`, `right`).\n","- The goal is to reach a specific \"goal\" state while avoiding trap states or penalties, following a policy that maximizes rewards.\n","\n","---\n","\n","### **2. Class: `GridworldMDP`**\n","The class encapsulates the MDP environment, including states, actions, rewards, transitions, and solving the MDP using Value Iteration.\n","\n","#### **a) Attributes Defined in `__init__`:**\n","1. **`grid_size`**: The dimension of the grid (e.g., 4x4 grid).\n","2. **`states`**: All possible grid coordinates, represented as a list of tuples `(i, j)`.\n","3. **`actions`**: The agent's possible movements: `['up', 'down', 'left', 'right']`.\n","4. **`gamma`**: The discount factor, controlling how much future rewards are valued.\n","5. **`rewards`**: A dictionary where each state has an associated reward. Created in the `create_rewards` method.\n","6. **`transition_probs`**: A dictionary that defines the probability of moving to different states based on actions. Created in the `create_transition_probs` method.\n","\n","---\n","\n","#### **b) Reward Function (`create_rewards`):**\n","Defines the reward structure for the grid:\n","- **Default Reward**: `-1` for each step to encourage the agent to finish quickly.\n","- **Goal State**: High positive reward (`10`) for reaching the goal `(0, grid_size - 1)`.\n","- **Trap State**: High negative reward (`-10`) for falling into the trap `(2, 2)`.\n","\n","---\n","\n","#### **c) Transition Probabilities (`create_transition_probs`):**\n","Defines the probability of moving to a new state after taking an action:\n","- For each state and action, the method determines the next state.\n","- Movement outside the grid boundaries is disallowed (e.g., moving \"up\" from `(0, 0)` keeps the agent at `(0, 0)`).\n","\n","---\n","\n","### **3. Solving the MDP with Value Iteration**\n","Value Iteration computes the **Optimal Value Function** and the **Optimal Policy** by iteratively refining estimates of state values.\n","\n","#### **a) Value Iteration (`value_iteration`):**\n","- **Inputs:**\n","  - `theta`: A small threshold to determine convergence. Defaults to `1e-3`.\n","- **Outputs:**\n","  - `V`: The optimal value function, a dictionary mapping states to their values.\n","  - `policy`: The optimal policy, mapping states to the best action.\n","\n","- **Algorithm Steps:**\n","  1. Initialize value estimates (`V`) to `0` for all states.\n","  2. Repeat until convergence:\n","     - For each state:\n","       - Compute the new value by evaluating all actions and picking the one with the maximum expected reward (`compute_value_for_state`).\n","       - Update the value and track the maximum change (`delta`).\n","     - Stop when `delta < theta`, indicating convergence.\n","  3. Extract the optimal policy by selecting the action with the highest expected reward for each state (`compute_optimal_action_for_state`).\n","\n","---\n","\n","#### **b) Value Computation for a State (`compute_value_for_state`):**\n","For a given state, calculate its value by:\n","1. Iterating over all possible actions.\n","2. Computing the expected value of each action, considering:\n","   - Transition probabilities to next states.\n","   - Rewards for those next states.\n","   - Discounted future rewards (`gamma * V[next_state]`).\n","3. Returning the maximum value over all actions.\n","\n","---\n","\n","#### **c) Optimal Policy Extraction (`compute_optimal_action_for_state`):**\n","For each state, the optimal policy is determined by:\n","1. Calculating the expected value for each action (similar to the value computation).\n","2. Returning the action with the highest expected value.\n","\n","---\n","\n","### **4. Printing Results**\n","1. **Optimal Value Function**:\n","   After value iteration, it shows the value of each state.\n","   - States closer to the goal have higher values.\n","   - Trap states and distant states have lower values due to penalties or distance.\n","\n","2. **Optimal Policy**:\n","   A policy is displayed as a grid where each state's action is represented by the first letter of its optimal action (`'u'`, `'d'`, `'l'`, `'r'`).\n","\n","---\n","\n","### **5. Example Output**\n","For a 4x4 grid:\n","#### Optimal Value Function:\n","```\n","State (0, 0):  3.53\n","State (0, 1):  4.39\n","State (0, 2):  5.51\n","State (0, 3): 10.00\n","...\n","```\n","\n","#### Optimal Policy:\n","```\n","r r r r\n","u u u u\n","u u l u\n","l l l u\n","```\n","- The agent learns to move right (`r`) and up (`u`) towards the goal while avoiding the trap at `(2, 2)`.\n","\n","---\n","\n","### **Applications**\n","This implementation is foundational for studying:\n","1. **Reinforcement Learning Algorithms**.\n","2. **Dynamic Programming in AI**.\n","3. **Planning and Decision Making** in autonomous systems.\n"],"metadata":{"id":"q_T8wsKzwhn7"}},{"cell_type":"code","source":[],"metadata":{"id":"dWTh3vnvjsgC"},"execution_count":null,"outputs":[]}]}