{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTeya/QIFj8dC5QkimRHV7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"dQDYym7Bc3VY","executionInfo":{"status":"ok","timestamp":1732372579385,"user_tz":-180,"elapsed":275,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"outputs":[],"source":["def get_eps_greedy(actions, epsilon, best_action):\n","    prob = {}\n","    num_actions = len(actions)\n","    for action in actions:\n","        if action == best_action:\n","            prob[action] = 1 - epsilon + (epsilon / num_actions)\n","        else:\n","            prob[action] = epsilon / num_actions\n","    return prob\n"]},{"cell_type":"code","source":["def on_policy_mc_control(env, iterations, epsilon, gamma):\n","    states = env.state_space\n","    actions = env.action_space\n","    Q = {s: {a: 0 for a in actions} for s in states}\n","    returns_count = {s: {a: 0 for a in actions} for s in states}\n","    policy = {s: get_eps_greedy(actions, epsilon, max(actions)) for s in states}\n","\n","    for _ in range(iterations):\n","        episode = generate_episode(env, policy)\n","        G = 0\n","        for state, action, reward in reversed(episode):\n","            G = reward + gamma * G\n","            if (state, action) not in [(x[0], x[1]) for x in episode[:-1]]:\n","                returns_count[state][action] += 1\n","                Q[state][action] += (G - Q[state][action]) / returns_count[state][action]\n","                best_action = max(Q[state], key=Q[state].get)\n","                policy[state] = get_eps_greedy(actions, epsilon, best_action)\n","    return policy, Q\n"],"metadata":{"id":"n5Ei-9y2c8HE","executionInfo":{"status":"ok","timestamp":1732372586894,"user_tz":-180,"elapsed":245,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def off_policy_mc_control(env, iterations, epsilon, gamma):\n","    Q = {s: {a: 0 for a in env.action_space} for s in env.state_space}\n","    C = {s: {a: 0 for a in env.action_space} for s in env.state_space}\n","    target_policy = {}\n","    behavior_policy = {s: get_eps_greedy(env.action_space, epsilon, max(env.action_space)) for s in env.state_space}\n","\n","    for _ in range(iterations):\n","        episode = generate_episode(env, behavior_policy)\n","        G = 0\n","        W = 1\n","        for state, action, reward in reversed(episode):\n","            G = reward + gamma * G\n","            C[state][action] += W\n","            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n","            best_action = max(Q[state], key=Q[state].get)\n","            target_policy[state] = best_action\n","            if action != best_action:\n","                break\n","            W *= 1 / behavior_policy[state][action]\n","\n","    return target_policy, Q\n"],"metadata":{"id":"TiDSJ25Lc99I","executionInfo":{"status":"ok","timestamp":1732372597057,"user_tz":-180,"elapsed":231,"user":{"displayName":"Kempsly Silencieux","userId":"03106040422226631399"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"z7Zga9XFdAcR"},"execution_count":null,"outputs":[]}]}